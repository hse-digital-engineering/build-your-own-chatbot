{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sam4096/qwen2tools:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Create a Simple Chain for Summarization\n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Build a LangChain chain that can summarize a given text.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Create a LLMChain using with a Ollama model.\n",
    "- Define a prompt template for summarization.\n",
    "- Run the chain with a sample text and print the summary.\n",
    "- Add model output streaming.\n",
    "- Run the chain with a sample text and print the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text highlights the evolution of deep learning in processing and generating unstructured data like text, images, and video over the past decade and its impact on various industries, from large language models to transformational impacts across different sectors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load the Ollama model\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"Your task is to summarize a text in one sentence.\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "\n",
    "# Create the LLMChain\n",
    "chain = prompt_template | llm \n",
    "\n",
    "# Run the chain with a sample text\n",
    "text = \"\"\"Over the last decade, deep learning has evolved massively to process and generate unstructured data like text, images, and video. \n",
    "These advanced AI models have gained popularity in various industries, and include large language models (LLMs). \n",
    "There is currently a significant level of fanfare in both the media and the industry surrounding AI,\n",
    "and thereâ€™s a fair case to be made that Artificial Intelligence (AI), with these advancements,\n",
    "is about to have a wide-ranging and major impact on businesses, societies, and individuals alike.\n",
    "This is driven by numerous factors, including advancements in technology, high-profile applications, \n",
    "and the potential for transfor- mative impacts across multiple sectors.\"\"\"\n",
    "\n",
    "summary = chain.invoke(text)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses how artificial intelligence, especially large language models (LLMs), has significantly advanced over the past decade, processing and generating unstructured data like text, images, and video. This advancement is expected to have significant impacts on businesses, societies, and individuals, driven by advancements in technology, high-profile applications, and potential for transformative effects across various sectors."
     ]
    }
   ],
   "source": [
    "# Stream the chain output\n",
    "for chunk in chain.stream({\"text\": text}):\n",
    "    print(chunk.content, end=\"\", flush=True)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Chain with Tool Usage (Simple Math Tool)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Create a LangChain chain that uses a simple math tool to perform calculations.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Define a function as tool which multiplies two integer values and return the result.\n",
    "- Create a chain\n",
    "- Print the result of the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two integers together.\n",
      "{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Create custom tool\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args) # -> definition of tool arguments\n",
    "\n",
    "# Invoke custom tool\n",
    "multiply.invoke({\"first_int\": 4, \"second_int\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'first_int': 42, 'second_int': 5}, 'id': 'acb863f3-e7fc-4b51-8371-2aea32155474', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Load the Ollama model\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "# Use bind_tools to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# When the model invokes the tool, this will show up in the AIMessage.tool_calls attribute of the output -> extract tool parameters from input text\n",
    "msg = llm_with_tools.invoke(\"whats 5 times forty two\")\n",
    "print(msg.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the chain: pass the extracte tool parameters from the input text to the tool -> extract the arguments of the first tool_call\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | multiply\n",
    "\n",
    "# Run chain\n",
    "chain.invoke(\"whats 5 times forty two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Agent with Tool Usage (Two Tools)\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Create a LangChain agent that uses two tools to perform tasks.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Define prompt template.\n",
    "- Define tools.\n",
    "- Create an Agent using the Ollama model, prompt template and tools.\n",
    "- Run the agent with a prompt that requires one or both tools.\n",
    "- Observe how the agent uses the tools to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (agent.py, line 1057)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[12], line 1\u001b[0m\n    from langchain.agents import AgentExecutor, create_tool_calling_agent\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/langchain/agents/__init__.py:40\u001b[0;36m\n\u001b[0;31m    from langchain.agents.agent import (\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:1057\u001b[0;36m\u001b[0m\n\u001b[0;31m    : bool = False\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "# Load the Ollama model\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful assistant\"), \n",
    "    (\"human\", \"{user_input}\"), \n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# Custom math tools\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Add two integers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Exponentiate the base to the exponent power.\"\n",
    "    return base**exponent\n",
    "\n",
    "\n",
    "tools = [add, exponentiate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exponentiate` with `{'base': 3, 'exponent': 5}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m243\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'first_int': 12, 'second_int': 243}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m255\u001b[0m\u001b[32;1m\u001b[1;3mGreat job! Your calculation for 3 to the fifth power is correct. Now, adding that result with 12 gives us a total of 255.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'Take 3 to the fifth power then add that 12.',\n",
       " 'output': 'Great job! Your calculation for 3 to the fifth power is correct. Now, adding that result with 12 gives us a total of 255.'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"user_input\": \"Take 3 to the fifth power then add that 12.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Enhance Agent with Memory\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Eenhance the agent from Task 3 with memory to improve its context awareness and ability to maintain state.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Create a ConversationBufferMemory to store chat history.\n",
    "- Modify the agent to use the memory to inform its responses.\n",
    "- Run the agent with a series of prompts that require context or state to be maintained.\n",
    "- Observe how the agent's responses improve with the addition of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (agent.py, line 1057)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[8], line 5\u001b[0m\n    from langchain.agents import AgentExecutor, create_tool_calling_agent\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/langchain/agents/__init__.py:40\u001b[0;36m\n\u001b[0;31m    from langchain.agents.agent import (\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/DEV/build-your-own-chatbot/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:1057\u001b[0;36m\u001b[0m\n\u001b[0;31m    : bool = False\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "# Load the Ollama model\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "# Define memory object for conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Add history placeholder to prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful assistant\"), \n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\"), \n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent_executor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat was the previous question?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m ai_msg_1 \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat was the previous question?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history})\n\u001b[1;32m      7\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mextend([HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion), AIMessage(content\u001b[38;5;241m=\u001b[39mai_msg_1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])])\n\u001b[1;32m      9\u001b[0m second_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was the question?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent_executor' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"what was the previous question?\"\n",
    "ai_msg_1 = agent_executor.invoke({\"user_input\": \"what was the previous question?\", \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), AIMessage(content=ai_msg_1[\"answer\"])])\n",
    "\n",
    "second_question = \"What was the question?\"\n",
    "ai_msg_2 = agent_executor.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
