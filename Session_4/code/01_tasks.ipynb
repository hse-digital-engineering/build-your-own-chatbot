{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sam4096/qwen2tools:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple RAG Chain\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a RAG chain with ChatOllama, HuggingFaceEmbeddings and Chroma\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load llm model via ollama\n",
    "- load huggingface embedding model (model: \"sentence-transformers/all-mpnet-base-v2\", catch-path:\"/ssd/hf-cache\")\n",
    "- create chroma db client\n",
    "- create simple chain with following steps: retrieved documents, prompt, model, output parser\n",
    "- create query and perform similarity search with a query\n",
    "- invoke chain and pass retrieved documents to the chain\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/build-your-own-chatbot/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/ssd/build-your-own-chatbot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", cache_folder=\"/ssd/hf-cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "collection = client.get_or_create_collection(\"ai_model_book\")\n",
    "\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"ai_model_book\",\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 33, 'source': './AI_Book.pdf'}, page_content='Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape'), Document(metadata={'page': 40, 'source': './AI_Book.pdf'}, page_content='Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15'), Document(metadata={'page': 44, 'source': './AI_Book.pdf'}, page_content='Figure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these exam‐\\nples, then use that model to make predictions . This is called model-based learning\\n(Figure 1-16 ).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down‐\\nload the Better Life Index  data from the OECD’s website  as well as stats about GDP\\nper capita from the IMF’s website . Then you join the tables and sort by GDP per cap‐\\nita. Table 1-1  shows an excerpt of what you get.\\nTypes of Machine Learning Systems | 19'), Document(metadata={'page': 15, 'source': './AI_Book.pdf'}, page_content='•The most common learning algorithms: Linear and Polynomial Regression,\\nLogistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\\nTrees, Random Forests, and Ensemble methods.\\nxiv | Preface')]\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "docs = vector_db_from_client.similarity_search(search_query)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this document, the main themes are:\\n\\n1. Supervised Machine Learning: This involves training an algorithm with labeled data from a known set of classes or labels (e.g., spam filters that learn to recognize certain email types).\\n\\n2. Unsupervised Machine Learning: It's where an algorithm learns without being provided explicit information about what it should be finding, based on patterns in the available data.\\n\\n3. Semisupervised Learning: A combination of supervised and unsupervised learning, where a small amount of labeled training is used to guide the unsupervised component of the system.\\n\\n4. Reinforcement Learning (RL): It's an algorithmic approach that involves machine learning algorithms designed for decision-making under uncertainty by learning from trial outcomes rather than explicit rewards or punishments.\\n\\n5. Batch and Online Learning: There are different ways a Machine Learning algorithm can learn. A batch learning process takes all data as input and produces output, typically offline; while online learning processes take new data into consideration during training time.\\n\\n6. Instance-based learning: This type of machine learning involves storing frequently encountered cases in memory to speed up predictions rather than fitting a model from scratch every time.\\n\\n7. Model-based learning: It's the process of building a model based on historical examples or observations, then using that model for prediction rather than starting over each time new data is added.\\n\\n8. Ensemble methods: These are strategies where multiple models are trained together and then combined to achieve better performance in tasks such as classification or regression. \\n\\n9. Linear Regression and Logistic Regression, k-Nearest Neighbors, Support Vector Machines (SVM), Decision Trees, Random Forests, and Ensemble methods: These are some of the common learning algorithms used for Machine Learning.\\n\\n10. The Document provides an overview of these main topics with various examples and classification into different categories based on criteria such as supervised/unsupervised, batch vs. online learning, etc.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "**Task Description:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "\n",
    "\n",
    "**Task Description:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4:\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "**Instructions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
