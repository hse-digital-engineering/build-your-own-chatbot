{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load embedding model into ollama:\n",
    "    1. Attach ollama container shell\n",
    "    2. Run command in container `ollama pull pull bge-m3` or send an api call `requests.post(\"http://localhost:11434/api/pull\", json = {\"name\": \"bge-m3\",  \"stream\": False}` via the requests library to download embedding model.\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Ollama REST API](https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api)\n",
    "- [Langchain Ollama Embeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama/)\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embedding model into ollama container\n",
    "requests.post(\"http://localhost:11434/api/pull\", json = {\"name\": \"bge-m3\",  \"stream\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'bge-m3:latest',\n",
       "   'model': 'bge-m3:latest',\n",
       "   'size': 1189142528,\n",
       "   'digest': '7907646426070047a77226ac3e684fbbe8410524f7b4a74d02837e43f2146bab',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'bert',\n",
       "    'families': ['bert'],\n",
       "    'parameter_size': '566.70M',\n",
       "    'quantization_level': 'F16'},\n",
       "   'expires_at': '2025-02-10T13:42:02.900213833Z',\n",
       "   'size_vram': 0},\n",
       "  {'name': 'llama3.2:1B',\n",
       "   'model': 'llama3.2:1B',\n",
       "   'size': 2233526272,\n",
       "   'digest': 'baf6a787fdffd633537aa2eb51cfd54cb93ff08e28040095462bb63daf552878',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '1.2B',\n",
       "    'quantization_level': 'Q8_0'},\n",
       "   'expires_at': '2025-02-10T13:06:59.841731263Z',\n",
       "   'size_vram': 0}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all available models in the ollama container\n",
    "response = requests.get(\"http://localhost:11434/api/ps\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 1024\n",
      "[-0.050017744, 0.021129876, -0.057069883, -0.0055166543, -0.0062899087, -0.02841217, 0.045161195, 0.042706154, -0.00025403852, 0.013057328]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "query_vector = embedding_model.embed_query(text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: ``PyPDFLoader``\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien Géron\n",
      "Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "SECOND EDITION\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing' metadata={'source': './AI_Book.pdf', 'page': 2, 'page_label': '1'}\n",
      "page_content='Aurlien Gron\n",
      "Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "SECOND EDITION\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing' metadata={'source': './AI_Book.pdf', 'page': 2, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "loader = PyPDFLoader(pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "pages_chunked = loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter())\n",
    "\n",
    "print(pages_chunked[0])\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning while preserving the Document structure\n",
    "pages_chunked_cleaned = [\n",
    "    Document(page_content=clean_text(doc.page_content), metadata=doc.metadata)\n",
    "    for doc in pages_chunked\n",
    "]\n",
    "\n",
    "print(pages_chunked_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]\n",
      "Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurlien Gron\n",
      "Copyright  2019 Aurlien Gron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by OReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "OReilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Editor: Nicole Tache\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.\n",
      "The OReilly logo is a registered trademark of OReilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of OReilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked_cleaned[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "collection_name = \"ai_model_book\"\n",
    "\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35ee54f3-bb96-4901-80a2-ac56a06f1a4b',\n",
       " 'ff1eeec0-96cd-403a-8699-ba5d08e911cc',\n",
       " 'dbb5ab22-8082-4557-a85d-d23e3389fe8b',\n",
       " '06f0cdd6-947a-49e6-be4f-8f10bdccf987',\n",
       " 'e06ff6fa-05b6-4080-a730-b94c0963fe55',\n",
       " 'f581d261-af89-42d6-bf9b-aed572c7e772',\n",
       " 'f5e8a360-0ebb-41a7-ae87-a6bc97be825d',\n",
       " '4983d824-ed89-44e9-9dfb-8b9426bcfd39',\n",
       " '5bf9604c-f7d0-4e5c-9135-0059181b36e0',\n",
       " 'd24c4d56-c0be-4162-a203-f14766bccdaa',\n",
       " 'b62cf006-621a-4271-8acb-4840d7ab6f53',\n",
       " 'cb17c536-f854-46b4-a3e7-982c9c540b77',\n",
       " 'fed7579a-e989-4229-8798-21c5edc81043',\n",
       " '1b9f4f6f-4b32-43b4-bd52-f9895b9f26f4',\n",
       " 'fd6dea2d-9535-441e-84cc-4c241255ad5d',\n",
       " 'f72b6778-4a44-45f3-b417-f5e86dc862c3',\n",
       " 'a7f1d7a2-03ce-4eba-98d0-7cb8676909e1',\n",
       " '51b54f2b-3df5-4fcc-8e68-9cf6bfb04491',\n",
       " '432569f0-2ffd-41c3-91fa-e36c75ea13c0',\n",
       " '7efd826d-938c-40bc-afdb-266d51e2ab89',\n",
       " '70db087a-f1c4-48b4-a3bf-46e0d8a30257',\n",
       " '6f145883-fdb9-4ed1-9d65-018d9b515b31',\n",
       " '3c9f3ecc-7e1f-4020-8bcb-fb4d727eaddf',\n",
       " '5f3b4a0f-8700-4707-9dbe-5a7a2334882b',\n",
       " '3615b939-e385-47ce-8892-40cc1fe60acc',\n",
       " '815d9510-154e-492d-ac21-c5fa18c2ece7',\n",
       " '2e43ef8e-cc4e-4a79-8cb5-b5a090f6e52e',\n",
       " 'dec01bf7-b9de-49a0-b736-820c4085f9db',\n",
       " '81e41340-e58d-4aa1-b31d-9bce6da55e52',\n",
       " 'fe0a432f-e14a-4af5-a7a4-6c90ff86024f',\n",
       " '43156f8c-d4cc-44d9-b458-b4e1ad6fc514',\n",
       " 'c5706f68-528b-41bb-9669-9ff50b2496df',\n",
       " 'ad08dba9-d3d3-4725-8c74-3c90994caf32',\n",
       " '233b9af1-f4ca-40ea-98b6-6833aa03d257',\n",
       " 'de05a931-909b-4b45-bf33-8e8933160653',\n",
       " '9777ba3a-1329-4f74-9bf1-58a7ce240926',\n",
       " '61ce9cf7-a999-4b40-84f0-aa39d8953fdf',\n",
       " '05f333a3-4380-4e2c-8512-d0be84cfabf7',\n",
       " 'e77f271b-4f00-4c55-b2dd-438bfeed8940',\n",
       " '6b1a52f5-b4dd-4c53-a525-120c3e0ebf2d',\n",
       " 'c052d1c7-36cc-45a0-833b-deb79a92a662',\n",
       " 'd4280e62-4151-4e00-922d-bade0d5189f4',\n",
       " 'd4fa20ef-df91-4fe7-bc7f-d566ed0cef6a',\n",
       " 'cfdca591-42a5-458a-806b-bf85a4d29de0',\n",
       " '86d748b7-386b-46a1-927a-debe88295894',\n",
       " '6c9cba53-b9bb-4609-b1a8-89bff2cce7e2',\n",
       " '22d2fc38-deb8-4aa6-a1fe-cbf57b1f1624',\n",
       " 'e7ded6f7-2511-4619-a70f-545223873eb9',\n",
       " '184aaaff-838d-415a-a8f2-4a1c342b4527',\n",
       " 'faa09b25-5e12-44a4-93b8-793bdc74b91d',\n",
       " 'afc25c6d-4aa2-42d5-b51e-9bd82125db65',\n",
       " '321ab08a-9dfb-45c7-b041-a0df24202fd7',\n",
       " 'f89e52da-4ce2-4415-8f27-4c752b845fb1',\n",
       " '152cc445-489b-4197-9491-eb60de14cb94',\n",
       " '609aaa68-330f-4727-8f06-0fc51368f1ca',\n",
       " 'e864caba-a820-4638-8c7d-0d7c2e157ead',\n",
       " '3bfe74ad-fd0b-4208-a144-e44567c63c12',\n",
       " '53d50490-1c5a-47b0-8778-5a4353ceda6f',\n",
       " '45c83e3f-9b06-4a08-af27-03ffd2406ce1',\n",
       " '8e9dda7e-15fa-4bee-8944-eaff14100097',\n",
       " '8610a0d1-7497-4db4-9c96-b6f4ef3532a9',\n",
       " '4bd36c7b-1f0c-4860-8404-529b8a9d6822',\n",
       " '40b5431a-3536-4885-9225-7bef02b85c89',\n",
       " '580e7690-78ca-4a12-95c2-b7daa97b90fd',\n",
       " '600b61ba-0f84-4ede-af94-a9ea0fca300b',\n",
       " '1e4a76bc-5335-47e8-bc53-11eaf4bbb4ee',\n",
       " 'b2c24033-4725-431a-b41b-3b4ec5730136',\n",
       " 'ce0daf83-a66a-4e27-a735-bd35111e2559',\n",
       " '9948a89e-3f5f-422d-ad7a-1a1824647662',\n",
       " '3545b308-43c8-4f1f-bae3-4883fe958534',\n",
       " '011e1d2d-ab3e-4083-8db6-94324dd704de',\n",
       " 'a618a021-be05-45db-a4cd-bd5f04678d3a',\n",
       " '318c02de-98af-489c-8023-19c8997bde7e',\n",
       " '2c9acfbb-04d0-4d5a-b3b1-2d12ff17ba00',\n",
       " '769cbc56-910a-41c9-8cba-88b7c226d9f9',\n",
       " '4942a462-dbd7-450c-8642-532d1160ed2f',\n",
       " 'c92e6851-1a79-480d-8d44-59cf9f1ce322',\n",
       " '65926654-4169-4450-b5da-f172748f16f8',\n",
       " 'f5609413-a083-4ef8-8bc9-cf3257942876',\n",
       " '5b52ed92-10bf-44e3-aa1f-45e22996c49a',\n",
       " '8b7e1c58-7b02-4efa-b653-1d37074a26cb',\n",
       " '4d132987-9ee1-4e42-9ee5-a97ec38ab45f',\n",
       " '46ccc925-c2d8-4c1b-9634-96a9dd4520f1',\n",
       " '9608ac57-f9f7-4680-9b59-9d78d329a2b4',\n",
       " '7e3aefc3-278d-4c8f-8217-53cbd386309b',\n",
       " '6474a761-7cab-4cda-9db7-01edc625e946',\n",
       " 'af0c25fe-b06c-4092-ac32-ad09f5f41e45',\n",
       " 'a3b0acb2-92f2-407b-a17d-5951c2cde211',\n",
       " '099d7713-d18d-4a67-b113-c55ebee2681a',\n",
       " '63a73c59-228c-405a-a0e1-b812fa483a57',\n",
       " 'ebc70e9a-2dda-4c17-a99a-e811e3cc5c4d',\n",
       " 'fe37ff2e-5fcc-4bb7-8467-74af7a294f7d',\n",
       " 'b6bb30ad-de19-4618-94fc-baacf311841a',\n",
       " '58ad12ce-6d87-493d-b888-382f28e4dc69',\n",
       " '779fe991-ef7c-4315-8073-3d69afd2cdbb',\n",
       " '63ea7adf-39c5-4ce2-90d1-8746aee68ca8',\n",
       " '12246be2-be5e-49d7-b09f-7b55c30dce39',\n",
       " '8d469b3e-263d-46b4-b8a1-23ad98483600',\n",
       " 'efda9a43-bf70-4fce-8dba-b72bd57648c5',\n",
       " '5d7623af-1e4c-47d4-904d-d72b12421a96',\n",
       " 'c2b29453-ff8d-4d76-b512-ead41979db02',\n",
       " 'ebffb8f6-adf9-4015-8051-51e8fd7c2b10',\n",
       " 'd4051310-3d66-49ac-bb7f-204b88906eb1',\n",
       " 'afdd27cf-e8ec-4835-9eb6-47163f200a97',\n",
       " '42460768-e4c5-4094-b239-8c9007b1ceaa',\n",
       " 'e081bbef-2e35-448f-92ad-c5cae0412d39',\n",
       " '6830c828-0a62-4a7d-84dd-1caa1ba1cfcc',\n",
       " '2a8d5986-e5cc-47b2-b771-b39713ac0543',\n",
       " '93d1f180-a89d-48fe-bf97-32bdf226d300',\n",
       " '2aa0c581-7e24-4246-9336-1468e94a71ba',\n",
       " 'd48d5ba7-3a7f-46a0-b335-c6368527c879',\n",
       " 'f16255a5-d5d3-437c-80d0-4bf77c342bcb',\n",
       " '16bfbefe-b3bd-472c-b63f-b0f605786e12',\n",
       " '5b74f5bd-6be8-4725-a00d-3a8979ff7443',\n",
       " 'cf0f5cb3-b810-477e-91fd-4d7de5b61dca',\n",
       " '57dc3af4-2ed5-4ee8-b5c7-b1b0a9887d9f',\n",
       " 'f50cd7e2-7fab-4229-b5a1-c63afe98a8d7',\n",
       " 'c6695cbb-205a-46c3-b9bc-ecc7dc7f0c14',\n",
       " '3a531dca-33c5-4e7a-a720-ebf7198e657a',\n",
       " 'd5873aad-142d-4049-9b54-4fd61b8ff34e',\n",
       " '10fb4e6a-d1e2-45e0-8ac5-f6f6fa60ceeb',\n",
       " '060a4a70-fe76-4ecf-8c26-4ac6667460c4',\n",
       " '08737967-ff35-4f67-bf87-7d91db10d71a',\n",
       " '6b6c6a59-35ea-4a9a-b48b-c8c2963ddd7e',\n",
       " '962c41ff-912f-4add-a837-130227e022f4',\n",
       " 'c871f1c3-bce5-456f-b8f0-0a4b618cb041',\n",
       " '0f554522-2b5f-4c2f-ba9e-dd0741d5f3b3',\n",
       " '49c5612a-6584-476d-969a-90bab26cbcb4',\n",
       " 'fbc84772-6e69-45f4-bd56-61ab59125fec',\n",
       " '7e20dc17-6614-45e0-864d-76e0d1a1dbf2',\n",
       " '18e45866-84c7-4c9c-a498-ff44f13beb2c',\n",
       " '5a5359f9-a217-44ab-8666-6520d2917f05',\n",
       " '65e38edc-0b9d-4b42-a5d3-591c367d019f',\n",
       " 'ea5bf2d0-d55a-45f2-9b20-2e657f0685c3',\n",
       " '21d74205-a750-463a-b018-2a7121f3936b',\n",
       " '72f0f871-3514-47c1-a400-6f281ebce155',\n",
       " 'fcca2c94-c8ca-41d8-845c-b81fc136a5fa',\n",
       " '1f3be300-6a16-4048-872a-9fc6a3570780',\n",
       " 'd044a8d5-a7f1-4811-8924-c28f292cf522',\n",
       " 'ab077e65-1e7e-4a34-a656-c7f39b93a09a',\n",
       " '08906809-1774-41db-88a3-b344ccc21a54',\n",
       " '81603cc7-f15f-4a3a-9e83-7dd3d5f9ac9f',\n",
       " '948fda32-7758-4952-8742-91077eed9583',\n",
       " 'bf08c1c1-c215-4054-9557-cb8d105c9811',\n",
       " 'f853e0e6-757e-4235-a3bd-fcd02b4f90cf',\n",
       " 'bc6cbc51-92bf-41d1-bd05-bd4ad0b80056',\n",
       " '9fcde615-a93a-4775-a8b5-accf8d2e4a48',\n",
       " 'cb4f20ea-7ab6-476c-8725-3478fe2a9162',\n",
       " '920a85ca-8067-47a4-8efa-2cd2b9b5a474',\n",
       " '3cf02a93-3af9-4b77-852c-c1c1640f7bca',\n",
       " '4ff3a23c-2144-4ae4-a54b-a3fe1fadb765',\n",
       " '8379fc44-60fc-4337-9dbe-e9b538f7c6ac',\n",
       " '6e494510-2980-4bee-aa8a-fb26217e13be',\n",
       " 'aeaf6fe3-4074-4a20-83ed-83a416c1dfb8',\n",
       " 'fb55768c-400c-44e7-bade-72a0a18c744a',\n",
       " '94d190f5-a910-4047-9e3d-1e4c42755586',\n",
       " '3a38b7c1-4535-4b01-9aca-292100db2fec',\n",
       " '0f546f86-51f4-417d-a2cb-545f243a1eec',\n",
       " 'db266692-568e-451a-8de6-e53147e67938',\n",
       " '24496b4e-aab9-49fc-a4c8-2b2bb3055e8e',\n",
       " 'd0109695-a389-4ae9-9a9d-f9cfb26df7e0',\n",
       " 'e1e6e115-ad44-4e5e-998d-b21f669abe6f',\n",
       " '6e1861b9-5fb2-4919-b205-60881c7bb56c',\n",
       " '9eb06311-8d6a-4890-9281-eb20b14f6273',\n",
       " '8c672b6b-d146-428a-8237-553d82dcc233',\n",
       " '4d896edb-95f5-464e-b917-6e4e18fba798',\n",
       " '6873b4e8-1545-43fe-acb4-777c47cd2bf1',\n",
       " 'cbd79f07-a942-44cb-a327-de42c649b1ce',\n",
       " 'fafcb3ba-55e3-4c03-8262-36cf7a5f1f3e',\n",
       " 'eade5705-c806-406d-8f2b-4922af3352a9',\n",
       " '94a9c7b9-625f-4ddf-a954-dcdf34dd0d50',\n",
       " 'b09de973-6e76-48d4-b7a1-ed7a9528a419',\n",
       " '1bdc2259-c5af-4c8f-b1e9-a2ab99f3215f',\n",
       " '9cdf6505-f3dd-41f1-8b46-3c2aee1b37ef',\n",
       " '587dc239-6864-4387-8184-902bb5dc1c83',\n",
       " '9b2cb20f-ac1d-41e4-83df-4be0ee9155a3',\n",
       " '8cee8695-d47f-4ba8-a7f0-75b371c98128',\n",
       " '66959933-a412-4469-97ba-0e730715ee5b',\n",
       " '034ae9dd-51d6-4ae5-ad3c-ac399e56cb2e',\n",
       " '5b46f371-a6e8-42eb-9abf-ba058872fa2c',\n",
       " 'fcb299a8-290b-49ed-ad2f-a265db9faa28',\n",
       " '88d88e6e-c4f1-4493-90de-d3f3dbfd0f74',\n",
       " '254fdd3a-d130-449d-bebd-acc14d502c68',\n",
       " '8f8a0357-be0c-4d56-aa9f-6fae39aa4cef',\n",
       " '41803bb5-ae8d-4de9-9fe9-f7cc7b585a20',\n",
       " '3ed4f457-14b5-40dd-8517-b0f7fbe49af6',\n",
       " '0d8ecbbd-0a5b-4912-af94-2b2aa19973a6',\n",
       " '5589c865-57c2-4a48-a996-352178645be4',\n",
       " '40205dc9-c081-4c27-b8a1-32c166d212b8',\n",
       " 'a41715a4-92d4-4c31-bb12-05a4694fe9fb',\n",
       " 'ca3f54ec-dc09-45ba-93ff-1ceacf8bd031',\n",
       " '6844f259-e240-4cc7-81a3-c5c49a5fe1cc',\n",
       " 'fb9a188f-7e7c-42d9-8096-f74081c1bdeb',\n",
       " '9ecc3c5f-d6e6-4a5c-a2ab-fddec6d142ca',\n",
       " 'c4becff2-31b8-472b-be2d-aa5563572511',\n",
       " '7f3fac27-135d-4cac-8791-e1ea1a6f38fc',\n",
       " '955e1d79-4169-4979-b0de-7f2c7e8c37cf',\n",
       " '828eb3a4-24dc-43d5-9d6c-4d5985fd0fb1',\n",
       " 'a07445a1-31c9-42cf-a2f8-3b26e53f7b26',\n",
       " '8383792e-c32e-4129-9ae1-782e80e5a39a',\n",
       " 'd60bdf07-5425-4f34-ae6a-3f31d4fe2f01',\n",
       " '1764bd61-b877-4985-a29a-c847b0d64789',\n",
       " '352e3b71-7b36-4378-aa75-aa72af63f59c',\n",
       " '3a1483fb-d76f-409f-a024-8820661cca10',\n",
       " '83618d17-55ab-4141-b53d-061dd64036ab',\n",
       " '3ac757cd-f933-4d76-bb41-4309c5948dfb',\n",
       " 'eecaf8cf-b515-4b68-a157-321a981411d9',\n",
       " '4109df1f-7be2-4f31-bb24-1ad3264d59b7',\n",
       " '8314c5d3-9bb6-459e-a2f3-cab7fbb365c9',\n",
       " '2aa2282b-567d-4b56-8429-219bf3e9f65f',\n",
       " '410ae71d-fb93-4598-9b97-adcfedb2167b',\n",
       " '6b03726e-28e0-449e-b36e-ebf61a5d1327',\n",
       " '85767a45-eee7-4177-b382-326ce3168a08',\n",
       " 'ae717482-bd75-456f-8e76-46cc6a7f955a',\n",
       " '198f0434-9d8e-492a-a800-0f573c0871d9',\n",
       " '1fa7aa28-23c3-43ce-94fb-253278472e57',\n",
       " '20494395-81f4-43ab-b0d8-934c11800d72',\n",
       " '9a3dd747-2ca6-4a7b-9f4b-aba77fdb21e1',\n",
       " '12366f6d-7914-431d-9102-47c60c38f0b1',\n",
       " 'f07ccca9-617d-4a33-8a76-dcb3d1fce63b',\n",
       " 'ac20a290-04bf-4421-8789-cb54ac0ac363',\n",
       " '33f6f549-4372-400f-9dfa-cddd9ff2a3d7',\n",
       " '5dcc142c-bd5a-436d-a6f0-7690f33129aa',\n",
       " '52cfea6c-79b0-4bd4-88df-209bab982c23',\n",
       " '1ddd2685-9733-4db7-8825-ead81ed5573e',\n",
       " '6ff40ebc-8bb6-436d-9f3a-4004ad1c07de',\n",
       " '6381bed3-6b96-4afc-b09a-e1df8b58311f',\n",
       " '6cfba404-64e3-4afd-bf97-fa088104d42a',\n",
       " 'c48d97dd-42f1-4f6e-8f7b-1b3046d4bce5',\n",
       " 'a9a11c6c-b9b4-454f-875f-3607df3519a7',\n",
       " '315304b2-7ca1-4b12-8c11-a2e08190b78a',\n",
       " 'fe6b9d5c-c0a2-416a-a01e-270d6c0155da',\n",
       " 'f69e9de5-1cc5-4aee-aac6-56e25006273f',\n",
       " 'd360f641-a416-4eca-a29a-b93d2fae4b8c',\n",
       " '173d8395-4846-4a06-980d-0b91e40d58d6',\n",
       " '4ad3d524-924a-4d99-88a5-d26bfe21b408',\n",
       " '10962d48-d8c3-4605-9340-30781406c1e6',\n",
       " 'aa8fdfd3-b824-493b-966d-4474e76840ef',\n",
       " '2d68f3de-a61e-4ffb-8b91-de442446dc14',\n",
       " '6a8bbd83-033e-4b9c-b4c8-45715203c05a',\n",
       " '85503fe7-9bd5-49c5-b249-73f03d8ef814',\n",
       " '05d295a5-d122-4f20-b7d4-1d404030a02b',\n",
       " '402ca8c4-2038-4a4e-b79c-5bf780c929d7',\n",
       " 'bcc38db3-90d1-4eaf-99e6-70d700979377',\n",
       " 'f7a9c5a0-06b3-451c-89c9-df40c1cb7168',\n",
       " 'eb0a53ff-ef75-4693-a713-9259799093b3',\n",
       " '214a0ec3-2c10-4eed-af3f-be9188518353',\n",
       " 'ca08e618-77d6-42e3-8b95-92c9a64fafd3',\n",
       " 'b09f61bb-4923-4146-a17d-d909bb6af475',\n",
       " 'a6529d40-cf19-40c0-842d-69538423a11f',\n",
       " '3adbadd0-5e53-40fa-a524-4f64e47bf3bd',\n",
       " 'c3828922-4120-4410-bfca-7b9c5c59d621',\n",
       " 'b2e225a8-d21b-4735-be57-0e49e0d9939e',\n",
       " '9e2da259-c92f-4656-98f0-bae8e626237f',\n",
       " 'f79601c5-f5db-4924-847c-94593d4f14fe',\n",
       " 'fbb61fc8-f60c-40cb-93ea-669d65f964ff',\n",
       " '486e2169-46f4-422d-9a82-a27d2ca9b21f',\n",
       " 'e583e214-d8fb-4e17-a8b0-b9f66d7ecb96',\n",
       " '21f43dea-13b7-4524-bdc7-2715e0e4781e',\n",
       " '5e0fb780-98f8-4557-8652-f6be30d3e787',\n",
       " 'b97191c0-cb8f-4309-9306-56ff98b6bd41',\n",
       " 'f4303d60-cd1f-4253-be55-418b99a53438',\n",
       " '565e6d7d-82dc-4ead-a06e-40f3e392d016',\n",
       " 'e1b78009-9931-4771-8dff-ebb2908d5c79',\n",
       " '3e5be6ee-8b5b-4b1e-9135-1da1ea16ca42',\n",
       " 'e277783a-2126-4875-9d3b-cf60b7123c58',\n",
       " '5aee8c94-8ed4-4725-8c7d-674a1b115753',\n",
       " '69df8620-c2ca-439c-91b2-045e16da373e',\n",
       " 'df2978ce-712c-42aa-be65-d9adfec64ca0',\n",
       " '4b0e0019-e21c-4d07-8357-81ef9c92f572',\n",
       " '3a1e6912-107c-400c-a3ef-0b3e3c2a67db',\n",
       " '5a7bcaf3-36ae-48d6-b4ea-230b10055874',\n",
       " 'de36e409-bc92-49ff-b72b-6e29657790e4',\n",
       " '6f640807-8246-4fe5-8a79-8d74d5063290',\n",
       " '5d01410b-0127-45ea-9f9d-e0e4eed2c750',\n",
       " 'd4fd8759-8975-4c9e-b4c0-2d57a82ec8ec',\n",
       " '2541cf5f-b1c1-4214-a904-d8f7b08aad56',\n",
       " '657c67e2-1ebd-4d3e-b3a1-b54f9343e2d4',\n",
       " '706e03f4-3c6f-47b5-960b-5a2b732943d2',\n",
       " '31f84833-75d7-48b0-95b8-4ad50716a104',\n",
       " '3cdc8cc1-f221-4ce0-89b2-b5e729304286',\n",
       " '77174b69-6339-4389-97c8-caee59ed335c',\n",
       " '57d0e387-072e-4ed2-9437-ec4dc16e0156',\n",
       " '38d273ca-b138-4cd2-b1e8-e274511657ef',\n",
       " '61fd9921-6997-4fdb-8023-b9272c605f67',\n",
       " '015eef98-b26f-437a-aee0-f90217f1c7ce',\n",
       " 'd1179471-a9e6-46f8-9d03-991d7e812d81',\n",
       " '9d98547b-6e53-44ed-a64e-84432b5e0f5f',\n",
       " 'c70ccc71-312d-4355-838a-e21acfd08dda',\n",
       " '9219573a-e5b8-4586-ad0b-0b280de42ed8',\n",
       " '26e78036-5f58-448e-8dae-809bb5086932',\n",
       " '77c91b8d-0008-4366-8be1-67457168d412',\n",
       " 'e0d375c2-db0d-47db-b4f8-f31fc781f62f',\n",
       " '7c61e0e6-4ae3-430f-b82d-3253a4278150',\n",
       " '5b9b5ef2-092d-4ff6-bd66-e56e03e60167',\n",
       " 'c951d69f-266c-4fa2-bbd8-f9d9db760259',\n",
       " '93c14ccd-fae7-4afe-b7d7-9edbf657e1cc',\n",
       " 'fa6edf98-9dba-441b-a428-8016acfc7361',\n",
       " '83c5cc16-887f-4a0f-9d50-ee68ec9dc6e3',\n",
       " 'b356543d-acc0-43f7-854e-5e6aacd0894e',\n",
       " 'c4eec4ad-376e-4788-add5-fb76bec1deb2',\n",
       " 'fd8d1490-fd56-4994-bf5d-0123f7fe50ad',\n",
       " '8825d3b9-c809-4fd6-9b3d-155a2f3b4892',\n",
       " 'ca3159d4-cdeb-423f-9799-db9fde35063c',\n",
       " 'e0d4ae5c-5423-482d-aa48-a0bb1ed243f6',\n",
       " 'c54d82e5-b72f-4ca7-a656-edd06f60e49a',\n",
       " 'fbcf9d7c-eab0-4e86-bf3c-f099ddd8f0fd',\n",
       " '3cca5e32-55f7-4cdd-adb4-b816e537aeb0',\n",
       " '3a75c9de-1de9-49b2-8b0d-953f64f60300',\n",
       " '24db36de-2d0a-4b61-8b59-8a81820d2812',\n",
       " '5bf930ea-aeb2-4df0-a300-b352b533421b',\n",
       " 'e2d780f8-8655-4c47-9074-ff30229b3dc3',\n",
       " '6c24402b-c583-48f0-90b4-cd5ce8c83a84',\n",
       " 'd7f2b19e-9f66-4660-a02e-49fa26ed6f21',\n",
       " '5fdf86dc-aaed-405b-9b5a-d42774b26056',\n",
       " 'd0afcea4-8efb-4c1f-9932-81ad7135eb43',\n",
       " '68cb731a-323b-467c-b3f2-bc4efdae0d51',\n",
       " '6026cae9-90f0-4958-bc19-72e0ad396c57',\n",
       " 'fccc9c06-17a3-43b8-b245-3071194afaaa',\n",
       " '8257056e-e997-4003-9260-74b4a7fc2d95',\n",
       " '2749c26a-dcf3-4fc2-a588-acfa9fe82f00',\n",
       " 'bea2ba51-1f3b-4ef9-83dc-9372026dded6',\n",
       " 'c183da6c-8e76-4b30-b161-62f9f8797d9e',\n",
       " '584a7351-b380-488e-a034-79f213c27991',\n",
       " '2e9270f5-a8e5-4f22-adff-ac650305edc4',\n",
       " '6c149df2-fdab-4b0d-a70a-dbc6afbac8e2',\n",
       " '80ed0296-13ab-49e3-b251-f0ef1422d752',\n",
       " '015e68e7-8788-4c4d-99c2-045cf2106699',\n",
       " '5e058322-141e-4d94-827a-ddd410acc63f',\n",
       " '9a0168c7-7f6c-4140-a11c-8aa59d95f96f',\n",
       " '69963429-45c2-4ff5-91d4-a91b79eb9d7d',\n",
       " 'b18fe0d6-54e2-4660-b045-70e5da05bc44',\n",
       " '7a118a5c-c1ff-4318-88b0-0640dcd1d392',\n",
       " '58535e1e-9c92-4a0f-b949-d22da0cf84b5',\n",
       " '3de54f53-f4e0-41ec-bd8c-37460999a244',\n",
       " '4aa37b18-96a3-4a02-add3-d07e420b3ce1',\n",
       " '43e8893d-bf13-4536-8b05-54282749e7c7',\n",
       " '0a98eec8-7b09-4fa0-9b6f-1e4a47491e77',\n",
       " '3218496f-21b5-4ce7-b6bf-3bb22ad00cf0',\n",
       " 'bb0afeb3-0c48-4b37-9e14-48a3fb215fd1',\n",
       " 'cb486321-5ed7-411d-9360-d771d5ab9735',\n",
       " '6870ae98-bc8a-40e0-bf9f-8d441afe531a',\n",
       " '79b2a71e-7234-4e67-bd37-d9c4091bb8e5',\n",
       " '926cbd45-b88c-4628-a536-ff86f4f511d8',\n",
       " '9d25fb23-63c7-486d-89b7-cfe4d1a58753',\n",
       " '79206e2f-7c64-4aab-bbb4-2464d3cd59be',\n",
       " '98c7637c-bace-4980-afb3-c64f52087f96',\n",
       " '3af138e3-991e-4819-82f9-aabfea5f105d',\n",
       " '8c7bc2e1-1c35-461b-a87f-3a7bf2197607',\n",
       " '7e6adf4d-713e-414a-bdf0-3fd74eb64949',\n",
       " 'd08dc8f8-b1af-4f90-94e9-3bc56972838e',\n",
       " '871afc30-962f-411e-ac14-99d6cc25d88a',\n",
       " 'd4c5f131-fa7a-45f7-ae34-d9b6120fedd6',\n",
       " '611c6ea6-5b44-4256-a74b-5416616d311f',\n",
       " '977ddba4-5d64-4113-8472-eb6d9f89f127',\n",
       " '38f5c992-7eb8-410b-aca8-b3430bce1c0b',\n",
       " '97f7fad3-7a51-46cf-99d3-23cd45ded17f',\n",
       " '41c22bb1-b3d3-4bcf-81de-626ca53aeb8b',\n",
       " 'bc4393fa-e6c7-4b83-899a-84c167f5a8a7',\n",
       " '471dc767-82e5-4a0d-815c-e9734cd8e9cd',\n",
       " 'f45f1569-9f3e-45ce-9e63-9a7aa27b9f06',\n",
       " '78c2c3c4-22f0-4e65-a38d-6fbbb84e6525',\n",
       " 'b1e8d906-39c5-4eb7-86ff-dbbb14579632',\n",
       " 'aac89481-d116-42fd-b8be-86b1419ef758',\n",
       " '021ebb02-2d60-4193-baf0-7e72a3dbfa92',\n",
       " 'f1db92be-7403-4d07-b677-8250cd5f9bb3',\n",
       " 'f8c859e1-2b76-4194-abbb-5083fc37136a',\n",
       " 'e9131449-a83d-4a4d-aaec-02edcfba62c7',\n",
       " '01f446ae-55b3-4ce7-b9e9-6bbee5a7026a',\n",
       " '25512b15-aec4-4178-8874-28ff51bbc534',\n",
       " '265355d0-6136-4d9d-b10e-279d2113072b',\n",
       " 'a9028ca1-cc0d-457c-b917-96055386e9a6',\n",
       " '1bde3c6f-fda6-4106-a4bd-265bda19e64a',\n",
       " 'b557f572-b22c-4d1d-93ee-33e246bea99e',\n",
       " 'a658868c-1f12-45a7-b036-8491f1ed0d76',\n",
       " '222ff724-cae1-443a-b942-1c4a09a5df3e',\n",
       " 'c829c96a-663d-418b-b514-e74f3135f306',\n",
       " '7ec29da4-a5e9-47ad-adc7-c4f1536a2658',\n",
       " 'f3d51c53-f2e3-432f-8b1b-b42c4072cee2',\n",
       " '98b975f5-e205-4462-94d8-2bbec15a8d96',\n",
       " '725ccc9c-7abc-4ef5-b027-65b490b9da1a',\n",
       " '4940873a-f7ed-4dde-98f0-cda0593d48a1',\n",
       " 'f8508a27-7443-4786-9843-53ba4a9ebfb3',\n",
       " 'cee8332e-078b-47be-abc3-bfb8877afe63',\n",
       " '0bb9c433-d6ec-40f0-9aaf-0ebef5298a6c',\n",
       " '1f02554a-a514-4341-90de-6a0f6672cc24',\n",
       " '7583daa8-9f28-4d1b-848b-e85de1cbaf03',\n",
       " 'a848402b-3e5c-448f-b4d1-f0a56065920e',\n",
       " '8aca29ef-9963-4c19-b19b-88eeeaa0ec4e',\n",
       " '4fcb2079-7a41-43c9-93e3-603b6f18dcea',\n",
       " '7c635ee4-047e-4ca6-b383-d22f37a6e6ef',\n",
       " '69f19859-7e9b-4c13-b36c-accaafbb1ebb',\n",
       " '9b753823-730f-4f47-9d23-357da012dbc1',\n",
       " '3d417673-38ac-4ec2-8b54-339dd5230184',\n",
       " 'ff7ff2de-8348-4d6f-aed1-4305a12f5a41',\n",
       " 'ec334649-94b8-42b4-8783-b9c6b3deaef3',\n",
       " '67c13533-5fc5-4d74-a6b0-f0e0fd8d5ae9',\n",
       " 'ed1ff57d-6f5c-4bd6-9623-7ecdce379577',\n",
       " '89fd5389-09a9-4bf6-96e2-54ec3fc8f14c',\n",
       " '8fe7d646-e5dc-4227-b702-a512007f9b47',\n",
       " '9fed2a4e-fba8-47e0-90c1-73240b56b260',\n",
       " '2ea943d8-682c-4412-b310-ced9bd11e90e',\n",
       " '1fb76d5c-779a-4cfe-885f-73fed6b850cc',\n",
       " '8c44e129-f2d1-42b8-ba45-efe7f4271923',\n",
       " '812c48d5-8975-4e28-8e81-879424b6d8b1',\n",
       " 'b11645b0-733e-4837-8242-29e3b1302d19',\n",
       " '77ced7fd-b20c-429b-9a6b-e636bd1eb56a',\n",
       " '9d739b01-93d5-4b7a-8037-c930e4cf8efe',\n",
       " 'e098b9d3-b275-4876-a5c0-a0716e7aaf7f',\n",
       " 'b55546d6-abab-402b-aeb9-a6b59213e945',\n",
       " 'ef5136d5-49c5-4757-9294-4f4f3cdc07d3',\n",
       " 'ff18ff97-5663-4bea-8338-15ba4581f3fd',\n",
       " '9efc6cf9-58bc-4bc4-adb2-daa33053da38',\n",
       " 'f60aed07-2e3e-47cb-a9c8-f757840c338e',\n",
       " '91f3d7d8-3c21-4f28-b5a9-ef64b4485343',\n",
       " '37ea6a4e-5e19-435c-a3a5-281ad3f19d66',\n",
       " '284538a7-4737-492f-9469-b10dc375122e',\n",
       " '2b641ec5-a6be-4e83-9adf-731aacbd7f6d',\n",
       " '4a85c202-8159-495e-a98f-c3205acfacb7',\n",
       " 'c9352420-c0bb-467f-b88d-f6804ca27145',\n",
       " 'aefc61c7-b367-491a-87f4-ea51d533d914',\n",
       " 'bfa12145-a8f5-42ca-92a3-7150d5a5f5ea',\n",
       " '497ad2c3-61c2-494d-a0c7-9a364fba8cb2',\n",
       " 'd2b8be52-f9e5-439c-be06-18bf599f4c5f',\n",
       " '99df0337-61f7-4622-92f7-4c82bcb67114',\n",
       " 'f530850d-5355-49f9-96d3-52af986ada0b',\n",
       " '71411d0f-916b-4ae9-9919-42d2198c6745',\n",
       " '0e39a3e0-b19c-4307-8185-69414d188bfb',\n",
       " 'f3a10dd8-d3c1-4ae3-a6fa-f8bd7a2b58e1',\n",
       " 'a166a277-2255-4509-aef5-18c420fd16b3',\n",
       " '306edd00-3916-483b-aa81-564345b4312f',\n",
       " 'b4aac03a-7bcd-4a71-90df-c94eccc61ae9',\n",
       " '3bc61c94-22f1-462c-a55e-58f6221d5c1e',\n",
       " '71b3404a-0ae4-4e85-9089-44f422de282b',\n",
       " 'e0edd1fb-3b91-49ea-aa67-dca0641407d6',\n",
       " '9fa7efe5-0a0c-47b6-9411-1f7ecbed802a',\n",
       " 'd76d135d-04be-4931-8517-35af1dd20f78',\n",
       " '7b61b5c1-a560-425b-8bf0-c54ee9372c7f',\n",
       " 'ee2f9d28-ff5d-4471-a41e-ffd331c3fd88',\n",
       " 'f9c0e49c-29be-4261-b59c-1190d0be8078',\n",
       " '4bceeb05-43bd-4e67-ae32-da8e30bccd4c',\n",
       " '2959b84a-1116-4041-9434-40f266fc4225',\n",
       " 'd8c3353f-a051-4d42-93ed-4fe9166bd415',\n",
       " '3560fd20-0bc7-4c4d-b0be-a2b8d494d9c9',\n",
       " '37fae784-1b26-486e-9f1e-d1b01de36095',\n",
       " '658aa29d-51b3-4056-840b-5e1ff4829adb',\n",
       " 'fdf91389-6b40-4990-bb75-775ed405f106',\n",
       " '11e4727a-33eb-4190-91ec-308882c51daf',\n",
       " '1b0bf34e-1263-4d99-bcdd-ddffe90aff98',\n",
       " '51a2e4fb-cb80-482a-9c51-83e5929f7deb',\n",
       " 'b82c4875-3c91-4c04-a766-7d201d63d3af',\n",
       " 'bb799481-d1ed-49bd-a1b7-ccbcec9231f1',\n",
       " '5089c8d5-61c3-42be-afad-77366f7417d2',\n",
       " '7b094577-686e-463c-af92-8d030846f813',\n",
       " '5e3feb25-c921-42f6-9e47-b2e9fcc360ad',\n",
       " '7062d10b-1673-43ac-a571-6b73a64fb708',\n",
       " 'b2ed2a7f-6c26-4e88-81e0-038714ac7e84',\n",
       " 'bec53686-5c38-4e64-9ed6-c742280a87b3',\n",
       " 'de028c39-f23c-4050-ad0c-a19a316f3c92',\n",
       " '1cc3cc91-1b5c-4f22-8d83-3e8b2e71d4f5',\n",
       " 'fb78d4db-12c1-488a-b15a-11e6764c0495',\n",
       " '88d91349-8858-457d-8be8-d65362d80855',\n",
       " 'f7c5b11b-9f70-44f0-bc2b-c547c2c66064',\n",
       " 'bb107467-deb2-4407-bf6f-5308f51190da',\n",
       " 'bca916fd-b093-4d1c-9826-17858c3b941f',\n",
       " 'c17c66ea-8a27-4cd4-abce-6233543915cc',\n",
       " '72b3fb21-80bb-4f96-9b2b-cd2a6b88d2fc',\n",
       " '3ffd03a8-ae5b-4df6-9613-b8dee3164278',\n",
       " '43d42578-32f4-4e3d-8ab9-5bbc3c2bf422',\n",
       " '1e05bcbc-c3e8-42f4-9252-5a8361be9b3e',\n",
       " '7fcb880a-b573-4d6e-9588-30a6ab2ab3a2',\n",
       " '41c81903-b737-4cc7-9be9-14c4d99cbd94',\n",
       " '6c67642b-5024-4360-bdcd-aacd2fa797b6',\n",
       " '4e66cbbe-3694-4972-80a7-553b4ddcb39f',\n",
       " '2db8cae3-475c-4794-a7e6-eab95dd4a5f7',\n",
       " '2687fa47-a3fa-4f57-ab5d-47598b73e08e',\n",
       " 'bec8348e-d862-4d39-85e5-af21eab0cd93',\n",
       " '7b75c6cc-5a41-4c18-ae47-ce248eccbf47',\n",
       " '445caaa8-d86b-499f-a097-bf08076b0660',\n",
       " '1090e271-69fb-484f-8fa6-d565b9f868d0',\n",
       " 'ca6abd97-0414-4f67-8774-9f07e9babcad',\n",
       " 'ff88633e-13c8-474a-8578-d09cd8ca0f22',\n",
       " '7b634711-31fb-48ac-8679-6e9e2ddeca16',\n",
       " '305336d2-561e-4de1-9937-007a968f7b29',\n",
       " '8469f4a7-5938-4b9c-b2b4-9a6a54393cc8',\n",
       " '4fdbc334-174a-424a-8ede-15d7b68cabd7',\n",
       " 'e35e2fa0-55f1-416a-b364-933c3190074c',\n",
       " '425bb8a1-afdd-47df-b560-3a6095d34eb8',\n",
       " '084624c6-36d7-4984-8485-0d5e0565210d',\n",
       " '053045b4-1223-4559-a599-6290b897cf97',\n",
       " 'aa3f8cf0-fc0a-4c77-8e4b-63b48be9f04b',\n",
       " 'bc9f9f94-a13b-42c8-8c60-7421bd2d5086',\n",
       " '9f3b89ef-ff5b-4989-871a-e13deb74da85',\n",
       " 'e2da09d4-c398-4514-8b44-0fd14a78934a',\n",
       " 'd8f318d6-af06-4b3a-abe1-f6b8946fa07c',\n",
       " '236261fc-b5c7-49ae-bbbf-5e1b4211ddcc',\n",
       " 'a876ed0f-34bf-4802-a6d0-b90289a69c64',\n",
       " '4d1eb338-cd1b-46f1-8f7e-324fc28af2a1',\n",
       " 'e641efb5-dbaf-470d-b87a-4a654f0c4815',\n",
       " 'ffd5a408-dc73-41f2-af89-17874c0afe18',\n",
       " 'a181ad6f-82e7-479e-a842-36384302976c',\n",
       " 'b8bdc0d8-a83a-4e4c-8020-77639c11df11',\n",
       " '877a2581-00f3-4c65-b975-8423240979d3',\n",
       " '11375913-4e7f-459b-9fe7-1879b6458124',\n",
       " '238134ad-5e58-45ef-8af9-9388488df0bd',\n",
       " 'b5fd1e1b-2b01-4aa5-9d47-40d4a4016179',\n",
       " 'bda8f8cb-4345-4013-ae7c-4e6150296c78']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned))]\n",
    "\n",
    "vector_db_from_client.add_documents(documents=pages_chunked_cleaned, ids=uuids) # that can take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of collections\n",
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of a collection\n",
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['35ee54f3-bb96-4901-80a2-ac56a06f1a4b',\n",
       "  'ff1eeec0-96cd-403a-8699-ba5d08e911cc',\n",
       "  'dbb5ab22-8082-4557-a85d-d23e3389fe8b',\n",
       "  '06f0cdd6-947a-49e6-be4f-8f10bdccf987',\n",
       "  'e06ff6fa-05b6-4080-a730-b94c0963fe55',\n",
       "  'f581d261-af89-42d6-bf9b-aed572c7e772',\n",
       "  'f5e8a360-0ebb-41a7-ae87-a6bc97be825d',\n",
       "  '4983d824-ed89-44e9-9dfb-8b9426bcfd39',\n",
       "  '5bf9604c-f7d0-4e5c-9135-0059181b36e0',\n",
       "  'd24c4d56-c0be-4162-a203-f14766bccdaa',\n",
       "  'b62cf006-621a-4271-8acb-4840d7ab6f53',\n",
       "  'cb17c536-f854-46b4-a3e7-982c9c540b77',\n",
       "  'fed7579a-e989-4229-8798-21c5edc81043',\n",
       "  '1b9f4f6f-4b32-43b4-bd52-f9895b9f26f4',\n",
       "  'fd6dea2d-9535-441e-84cc-4c241255ad5d',\n",
       "  'f72b6778-4a44-45f3-b417-f5e86dc862c3',\n",
       "  'a7f1d7a2-03ce-4eba-98d0-7cb8676909e1',\n",
       "  '51b54f2b-3df5-4fcc-8e68-9cf6bfb04491',\n",
       "  '432569f0-2ffd-41c3-91fa-e36c75ea13c0',\n",
       "  '7efd826d-938c-40bc-afdb-266d51e2ab89',\n",
       "  '70db087a-f1c4-48b4-a3bf-46e0d8a30257',\n",
       "  '6f145883-fdb9-4ed1-9d65-018d9b515b31',\n",
       "  '3c9f3ecc-7e1f-4020-8bcb-fb4d727eaddf',\n",
       "  '5f3b4a0f-8700-4707-9dbe-5a7a2334882b',\n",
       "  '3615b939-e385-47ce-8892-40cc1fe60acc',\n",
       "  '815d9510-154e-492d-ac21-c5fa18c2ece7',\n",
       "  '2e43ef8e-cc4e-4a79-8cb5-b5a090f6e52e',\n",
       "  'dec01bf7-b9de-49a0-b736-820c4085f9db',\n",
       "  '81e41340-e58d-4aa1-b31d-9bce6da55e52',\n",
       "  'fe0a432f-e14a-4af5-a7a4-6c90ff86024f',\n",
       "  '43156f8c-d4cc-44d9-b458-b4e1ad6fc514',\n",
       "  'c5706f68-528b-41bb-9669-9ff50b2496df',\n",
       "  'ad08dba9-d3d3-4725-8c74-3c90994caf32',\n",
       "  '233b9af1-f4ca-40ea-98b6-6833aa03d257',\n",
       "  'de05a931-909b-4b45-bf33-8e8933160653',\n",
       "  '9777ba3a-1329-4f74-9bf1-58a7ce240926',\n",
       "  '61ce9cf7-a999-4b40-84f0-aa39d8953fdf',\n",
       "  '05f333a3-4380-4e2c-8512-d0be84cfabf7',\n",
       "  'e77f271b-4f00-4c55-b2dd-438bfeed8940',\n",
       "  '6b1a52f5-b4dd-4c53-a525-120c3e0ebf2d',\n",
       "  'c052d1c7-36cc-45a0-833b-deb79a92a662',\n",
       "  'd4280e62-4151-4e00-922d-bade0d5189f4',\n",
       "  'd4fa20ef-df91-4fe7-bc7f-d566ed0cef6a',\n",
       "  'cfdca591-42a5-458a-806b-bf85a4d29de0',\n",
       "  '86d748b7-386b-46a1-927a-debe88295894',\n",
       "  '6c9cba53-b9bb-4609-b1a8-89bff2cce7e2',\n",
       "  '22d2fc38-deb8-4aa6-a1fe-cbf57b1f1624',\n",
       "  'e7ded6f7-2511-4619-a70f-545223873eb9',\n",
       "  '184aaaff-838d-415a-a8f2-4a1c342b4527',\n",
       "  'faa09b25-5e12-44a4-93b8-793bdc74b91d',\n",
       "  'afc25c6d-4aa2-42d5-b51e-9bd82125db65',\n",
       "  '321ab08a-9dfb-45c7-b041-a0df24202fd7',\n",
       "  'f89e52da-4ce2-4415-8f27-4c752b845fb1',\n",
       "  '152cc445-489b-4197-9491-eb60de14cb94',\n",
       "  '609aaa68-330f-4727-8f06-0fc51368f1ca',\n",
       "  'e864caba-a820-4638-8c7d-0d7c2e157ead',\n",
       "  '3bfe74ad-fd0b-4208-a144-e44567c63c12',\n",
       "  '53d50490-1c5a-47b0-8778-5a4353ceda6f',\n",
       "  '45c83e3f-9b06-4a08-af27-03ffd2406ce1',\n",
       "  '8e9dda7e-15fa-4bee-8944-eaff14100097',\n",
       "  '8610a0d1-7497-4db4-9c96-b6f4ef3532a9',\n",
       "  '4bd36c7b-1f0c-4860-8404-529b8a9d6822',\n",
       "  '40b5431a-3536-4885-9225-7bef02b85c89',\n",
       "  '580e7690-78ca-4a12-95c2-b7daa97b90fd',\n",
       "  '600b61ba-0f84-4ede-af94-a9ea0fca300b',\n",
       "  '1e4a76bc-5335-47e8-bc53-11eaf4bbb4ee',\n",
       "  'b2c24033-4725-431a-b41b-3b4ec5730136',\n",
       "  'ce0daf83-a66a-4e27-a735-bd35111e2559',\n",
       "  '9948a89e-3f5f-422d-ad7a-1a1824647662',\n",
       "  '3545b308-43c8-4f1f-bae3-4883fe958534',\n",
       "  '011e1d2d-ab3e-4083-8db6-94324dd704de',\n",
       "  'a618a021-be05-45db-a4cd-bd5f04678d3a',\n",
       "  '318c02de-98af-489c-8023-19c8997bde7e',\n",
       "  '2c9acfbb-04d0-4d5a-b3b1-2d12ff17ba00',\n",
       "  '769cbc56-910a-41c9-8cba-88b7c226d9f9',\n",
       "  '4942a462-dbd7-450c-8642-532d1160ed2f',\n",
       "  'c92e6851-1a79-480d-8d44-59cf9f1ce322',\n",
       "  '65926654-4169-4450-b5da-f172748f16f8',\n",
       "  'f5609413-a083-4ef8-8bc9-cf3257942876',\n",
       "  '5b52ed92-10bf-44e3-aa1f-45e22996c49a',\n",
       "  '8b7e1c58-7b02-4efa-b653-1d37074a26cb',\n",
       "  '4d132987-9ee1-4e42-9ee5-a97ec38ab45f',\n",
       "  '46ccc925-c2d8-4c1b-9634-96a9dd4520f1',\n",
       "  '9608ac57-f9f7-4680-9b59-9d78d329a2b4',\n",
       "  '7e3aefc3-278d-4c8f-8217-53cbd386309b',\n",
       "  '6474a761-7cab-4cda-9db7-01edc625e946',\n",
       "  'af0c25fe-b06c-4092-ac32-ad09f5f41e45',\n",
       "  'a3b0acb2-92f2-407b-a17d-5951c2cde211',\n",
       "  '099d7713-d18d-4a67-b113-c55ebee2681a',\n",
       "  '63a73c59-228c-405a-a0e1-b812fa483a57',\n",
       "  'ebc70e9a-2dda-4c17-a99a-e811e3cc5c4d',\n",
       "  'fe37ff2e-5fcc-4bb7-8467-74af7a294f7d',\n",
       "  'b6bb30ad-de19-4618-94fc-baacf311841a',\n",
       "  '58ad12ce-6d87-493d-b888-382f28e4dc69',\n",
       "  '779fe991-ef7c-4315-8073-3d69afd2cdbb',\n",
       "  '63ea7adf-39c5-4ce2-90d1-8746aee68ca8',\n",
       "  '12246be2-be5e-49d7-b09f-7b55c30dce39',\n",
       "  '8d469b3e-263d-46b4-b8a1-23ad98483600',\n",
       "  'efda9a43-bf70-4fce-8dba-b72bd57648c5',\n",
       "  '5d7623af-1e4c-47d4-904d-d72b12421a96',\n",
       "  'c2b29453-ff8d-4d76-b512-ead41979db02',\n",
       "  'ebffb8f6-adf9-4015-8051-51e8fd7c2b10',\n",
       "  'd4051310-3d66-49ac-bb7f-204b88906eb1',\n",
       "  'afdd27cf-e8ec-4835-9eb6-47163f200a97',\n",
       "  '42460768-e4c5-4094-b239-8c9007b1ceaa',\n",
       "  'e081bbef-2e35-448f-92ad-c5cae0412d39',\n",
       "  '6830c828-0a62-4a7d-84dd-1caa1ba1cfcc',\n",
       "  '2a8d5986-e5cc-47b2-b771-b39713ac0543',\n",
       "  '93d1f180-a89d-48fe-bf97-32bdf226d300',\n",
       "  '2aa0c581-7e24-4246-9336-1468e94a71ba',\n",
       "  'd48d5ba7-3a7f-46a0-b335-c6368527c879',\n",
       "  'f16255a5-d5d3-437c-80d0-4bf77c342bcb',\n",
       "  '16bfbefe-b3bd-472c-b63f-b0f605786e12',\n",
       "  '5b74f5bd-6be8-4725-a00d-3a8979ff7443',\n",
       "  'cf0f5cb3-b810-477e-91fd-4d7de5b61dca',\n",
       "  '57dc3af4-2ed5-4ee8-b5c7-b1b0a9887d9f',\n",
       "  'f50cd7e2-7fab-4229-b5a1-c63afe98a8d7',\n",
       "  'c6695cbb-205a-46c3-b9bc-ecc7dc7f0c14',\n",
       "  '3a531dca-33c5-4e7a-a720-ebf7198e657a',\n",
       "  'd5873aad-142d-4049-9b54-4fd61b8ff34e',\n",
       "  '10fb4e6a-d1e2-45e0-8ac5-f6f6fa60ceeb',\n",
       "  '060a4a70-fe76-4ecf-8c26-4ac6667460c4',\n",
       "  '08737967-ff35-4f67-bf87-7d91db10d71a',\n",
       "  '6b6c6a59-35ea-4a9a-b48b-c8c2963ddd7e',\n",
       "  '962c41ff-912f-4add-a837-130227e022f4',\n",
       "  'c871f1c3-bce5-456f-b8f0-0a4b618cb041',\n",
       "  '0f554522-2b5f-4c2f-ba9e-dd0741d5f3b3',\n",
       "  '49c5612a-6584-476d-969a-90bab26cbcb4',\n",
       "  'fbc84772-6e69-45f4-bd56-61ab59125fec',\n",
       "  '7e20dc17-6614-45e0-864d-76e0d1a1dbf2',\n",
       "  '18e45866-84c7-4c9c-a498-ff44f13beb2c',\n",
       "  '5a5359f9-a217-44ab-8666-6520d2917f05',\n",
       "  '65e38edc-0b9d-4b42-a5d3-591c367d019f',\n",
       "  'ea5bf2d0-d55a-45f2-9b20-2e657f0685c3',\n",
       "  '21d74205-a750-463a-b018-2a7121f3936b',\n",
       "  '72f0f871-3514-47c1-a400-6f281ebce155',\n",
       "  'fcca2c94-c8ca-41d8-845c-b81fc136a5fa',\n",
       "  '1f3be300-6a16-4048-872a-9fc6a3570780',\n",
       "  'd044a8d5-a7f1-4811-8924-c28f292cf522',\n",
       "  'ab077e65-1e7e-4a34-a656-c7f39b93a09a',\n",
       "  '08906809-1774-41db-88a3-b344ccc21a54',\n",
       "  '81603cc7-f15f-4a3a-9e83-7dd3d5f9ac9f',\n",
       "  '948fda32-7758-4952-8742-91077eed9583',\n",
       "  'bf08c1c1-c215-4054-9557-cb8d105c9811',\n",
       "  'f853e0e6-757e-4235-a3bd-fcd02b4f90cf',\n",
       "  'bc6cbc51-92bf-41d1-bd05-bd4ad0b80056',\n",
       "  '9fcde615-a93a-4775-a8b5-accf8d2e4a48',\n",
       "  'cb4f20ea-7ab6-476c-8725-3478fe2a9162',\n",
       "  '920a85ca-8067-47a4-8efa-2cd2b9b5a474',\n",
       "  '3cf02a93-3af9-4b77-852c-c1c1640f7bca',\n",
       "  '4ff3a23c-2144-4ae4-a54b-a3fe1fadb765',\n",
       "  '8379fc44-60fc-4337-9dbe-e9b538f7c6ac',\n",
       "  '6e494510-2980-4bee-aa8a-fb26217e13be',\n",
       "  'aeaf6fe3-4074-4a20-83ed-83a416c1dfb8',\n",
       "  'fb55768c-400c-44e7-bade-72a0a18c744a',\n",
       "  '94d190f5-a910-4047-9e3d-1e4c42755586',\n",
       "  '3a38b7c1-4535-4b01-9aca-292100db2fec',\n",
       "  '0f546f86-51f4-417d-a2cb-545f243a1eec',\n",
       "  'db266692-568e-451a-8de6-e53147e67938',\n",
       "  '24496b4e-aab9-49fc-a4c8-2b2bb3055e8e',\n",
       "  'd0109695-a389-4ae9-9a9d-f9cfb26df7e0',\n",
       "  'e1e6e115-ad44-4e5e-998d-b21f669abe6f',\n",
       "  '6e1861b9-5fb2-4919-b205-60881c7bb56c',\n",
       "  '9eb06311-8d6a-4890-9281-eb20b14f6273',\n",
       "  '8c672b6b-d146-428a-8237-553d82dcc233',\n",
       "  '4d896edb-95f5-464e-b917-6e4e18fba798',\n",
       "  '6873b4e8-1545-43fe-acb4-777c47cd2bf1',\n",
       "  'cbd79f07-a942-44cb-a327-de42c649b1ce',\n",
       "  'fafcb3ba-55e3-4c03-8262-36cf7a5f1f3e',\n",
       "  'eade5705-c806-406d-8f2b-4922af3352a9',\n",
       "  '94a9c7b9-625f-4ddf-a954-dcdf34dd0d50',\n",
       "  'b09de973-6e76-48d4-b7a1-ed7a9528a419',\n",
       "  '1bdc2259-c5af-4c8f-b1e9-a2ab99f3215f',\n",
       "  '9cdf6505-f3dd-41f1-8b46-3c2aee1b37ef',\n",
       "  '587dc239-6864-4387-8184-902bb5dc1c83',\n",
       "  '9b2cb20f-ac1d-41e4-83df-4be0ee9155a3',\n",
       "  '8cee8695-d47f-4ba8-a7f0-75b371c98128',\n",
       "  '66959933-a412-4469-97ba-0e730715ee5b',\n",
       "  '034ae9dd-51d6-4ae5-ad3c-ac399e56cb2e',\n",
       "  '5b46f371-a6e8-42eb-9abf-ba058872fa2c',\n",
       "  'fcb299a8-290b-49ed-ad2f-a265db9faa28',\n",
       "  '88d88e6e-c4f1-4493-90de-d3f3dbfd0f74',\n",
       "  '254fdd3a-d130-449d-bebd-acc14d502c68',\n",
       "  '8f8a0357-be0c-4d56-aa9f-6fae39aa4cef',\n",
       "  '41803bb5-ae8d-4de9-9fe9-f7cc7b585a20',\n",
       "  '3ed4f457-14b5-40dd-8517-b0f7fbe49af6',\n",
       "  '0d8ecbbd-0a5b-4912-af94-2b2aa19973a6',\n",
       "  '5589c865-57c2-4a48-a996-352178645be4',\n",
       "  '40205dc9-c081-4c27-b8a1-32c166d212b8',\n",
       "  'a41715a4-92d4-4c31-bb12-05a4694fe9fb',\n",
       "  'ca3f54ec-dc09-45ba-93ff-1ceacf8bd031',\n",
       "  '6844f259-e240-4cc7-81a3-c5c49a5fe1cc',\n",
       "  'fb9a188f-7e7c-42d9-8096-f74081c1bdeb',\n",
       "  '9ecc3c5f-d6e6-4a5c-a2ab-fddec6d142ca',\n",
       "  'c4becff2-31b8-472b-be2d-aa5563572511',\n",
       "  '7f3fac27-135d-4cac-8791-e1ea1a6f38fc',\n",
       "  '955e1d79-4169-4979-b0de-7f2c7e8c37cf',\n",
       "  '828eb3a4-24dc-43d5-9d6c-4d5985fd0fb1',\n",
       "  'a07445a1-31c9-42cf-a2f8-3b26e53f7b26',\n",
       "  '8383792e-c32e-4129-9ae1-782e80e5a39a',\n",
       "  'd60bdf07-5425-4f34-ae6a-3f31d4fe2f01',\n",
       "  '1764bd61-b877-4985-a29a-c847b0d64789',\n",
       "  '352e3b71-7b36-4378-aa75-aa72af63f59c',\n",
       "  '3a1483fb-d76f-409f-a024-8820661cca10',\n",
       "  '83618d17-55ab-4141-b53d-061dd64036ab',\n",
       "  '3ac757cd-f933-4d76-bb41-4309c5948dfb',\n",
       "  'eecaf8cf-b515-4b68-a157-321a981411d9',\n",
       "  '4109df1f-7be2-4f31-bb24-1ad3264d59b7',\n",
       "  '8314c5d3-9bb6-459e-a2f3-cab7fbb365c9',\n",
       "  '2aa2282b-567d-4b56-8429-219bf3e9f65f',\n",
       "  '410ae71d-fb93-4598-9b97-adcfedb2167b',\n",
       "  '6b03726e-28e0-449e-b36e-ebf61a5d1327',\n",
       "  '85767a45-eee7-4177-b382-326ce3168a08',\n",
       "  'ae717482-bd75-456f-8e76-46cc6a7f955a',\n",
       "  '198f0434-9d8e-492a-a800-0f573c0871d9',\n",
       "  '1fa7aa28-23c3-43ce-94fb-253278472e57',\n",
       "  '20494395-81f4-43ab-b0d8-934c11800d72',\n",
       "  '9a3dd747-2ca6-4a7b-9f4b-aba77fdb21e1',\n",
       "  '12366f6d-7914-431d-9102-47c60c38f0b1',\n",
       "  'f07ccca9-617d-4a33-8a76-dcb3d1fce63b',\n",
       "  'ac20a290-04bf-4421-8789-cb54ac0ac363',\n",
       "  '33f6f549-4372-400f-9dfa-cddd9ff2a3d7',\n",
       "  '5dcc142c-bd5a-436d-a6f0-7690f33129aa',\n",
       "  '52cfea6c-79b0-4bd4-88df-209bab982c23',\n",
       "  '1ddd2685-9733-4db7-8825-ead81ed5573e',\n",
       "  '6ff40ebc-8bb6-436d-9f3a-4004ad1c07de',\n",
       "  '6381bed3-6b96-4afc-b09a-e1df8b58311f',\n",
       "  '6cfba404-64e3-4afd-bf97-fa088104d42a',\n",
       "  'c48d97dd-42f1-4f6e-8f7b-1b3046d4bce5',\n",
       "  'a9a11c6c-b9b4-454f-875f-3607df3519a7',\n",
       "  '315304b2-7ca1-4b12-8c11-a2e08190b78a',\n",
       "  'fe6b9d5c-c0a2-416a-a01e-270d6c0155da',\n",
       "  'f69e9de5-1cc5-4aee-aac6-56e25006273f',\n",
       "  'd360f641-a416-4eca-a29a-b93d2fae4b8c',\n",
       "  '173d8395-4846-4a06-980d-0b91e40d58d6',\n",
       "  '4ad3d524-924a-4d99-88a5-d26bfe21b408',\n",
       "  '10962d48-d8c3-4605-9340-30781406c1e6',\n",
       "  'aa8fdfd3-b824-493b-966d-4474e76840ef',\n",
       "  '2d68f3de-a61e-4ffb-8b91-de442446dc14',\n",
       "  '6a8bbd83-033e-4b9c-b4c8-45715203c05a',\n",
       "  '85503fe7-9bd5-49c5-b249-73f03d8ef814',\n",
       "  '05d295a5-d122-4f20-b7d4-1d404030a02b',\n",
       "  '402ca8c4-2038-4a4e-b79c-5bf780c929d7',\n",
       "  'bcc38db3-90d1-4eaf-99e6-70d700979377',\n",
       "  'f7a9c5a0-06b3-451c-89c9-df40c1cb7168',\n",
       "  'eb0a53ff-ef75-4693-a713-9259799093b3',\n",
       "  '214a0ec3-2c10-4eed-af3f-be9188518353',\n",
       "  'ca08e618-77d6-42e3-8b95-92c9a64fafd3',\n",
       "  'b09f61bb-4923-4146-a17d-d909bb6af475',\n",
       "  'a6529d40-cf19-40c0-842d-69538423a11f',\n",
       "  '3adbadd0-5e53-40fa-a524-4f64e47bf3bd',\n",
       "  'c3828922-4120-4410-bfca-7b9c5c59d621',\n",
       "  'b2e225a8-d21b-4735-be57-0e49e0d9939e',\n",
       "  '9e2da259-c92f-4656-98f0-bae8e626237f',\n",
       "  'f79601c5-f5db-4924-847c-94593d4f14fe',\n",
       "  'fbb61fc8-f60c-40cb-93ea-669d65f964ff',\n",
       "  '486e2169-46f4-422d-9a82-a27d2ca9b21f',\n",
       "  'e583e214-d8fb-4e17-a8b0-b9f66d7ecb96',\n",
       "  '21f43dea-13b7-4524-bdc7-2715e0e4781e',\n",
       "  '5e0fb780-98f8-4557-8652-f6be30d3e787',\n",
       "  'b97191c0-cb8f-4309-9306-56ff98b6bd41',\n",
       "  'f4303d60-cd1f-4253-be55-418b99a53438',\n",
       "  '565e6d7d-82dc-4ead-a06e-40f3e392d016',\n",
       "  'e1b78009-9931-4771-8dff-ebb2908d5c79',\n",
       "  '3e5be6ee-8b5b-4b1e-9135-1da1ea16ca42',\n",
       "  'e277783a-2126-4875-9d3b-cf60b7123c58',\n",
       "  '5aee8c94-8ed4-4725-8c7d-674a1b115753',\n",
       "  '69df8620-c2ca-439c-91b2-045e16da373e',\n",
       "  'df2978ce-712c-42aa-be65-d9adfec64ca0',\n",
       "  '4b0e0019-e21c-4d07-8357-81ef9c92f572',\n",
       "  '3a1e6912-107c-400c-a3ef-0b3e3c2a67db',\n",
       "  '5a7bcaf3-36ae-48d6-b4ea-230b10055874',\n",
       "  'de36e409-bc92-49ff-b72b-6e29657790e4',\n",
       "  '6f640807-8246-4fe5-8a79-8d74d5063290',\n",
       "  '5d01410b-0127-45ea-9f9d-e0e4eed2c750',\n",
       "  'd4fd8759-8975-4c9e-b4c0-2d57a82ec8ec',\n",
       "  '2541cf5f-b1c1-4214-a904-d8f7b08aad56',\n",
       "  '657c67e2-1ebd-4d3e-b3a1-b54f9343e2d4',\n",
       "  '706e03f4-3c6f-47b5-960b-5a2b732943d2',\n",
       "  '31f84833-75d7-48b0-95b8-4ad50716a104',\n",
       "  '3cdc8cc1-f221-4ce0-89b2-b5e729304286',\n",
       "  '77174b69-6339-4389-97c8-caee59ed335c',\n",
       "  '57d0e387-072e-4ed2-9437-ec4dc16e0156',\n",
       "  '38d273ca-b138-4cd2-b1e8-e274511657ef',\n",
       "  '61fd9921-6997-4fdb-8023-b9272c605f67',\n",
       "  '015eef98-b26f-437a-aee0-f90217f1c7ce',\n",
       "  'd1179471-a9e6-46f8-9d03-991d7e812d81',\n",
       "  '9d98547b-6e53-44ed-a64e-84432b5e0f5f',\n",
       "  'c70ccc71-312d-4355-838a-e21acfd08dda',\n",
       "  '9219573a-e5b8-4586-ad0b-0b280de42ed8',\n",
       "  '26e78036-5f58-448e-8dae-809bb5086932',\n",
       "  '77c91b8d-0008-4366-8be1-67457168d412',\n",
       "  'e0d375c2-db0d-47db-b4f8-f31fc781f62f',\n",
       "  '7c61e0e6-4ae3-430f-b82d-3253a4278150',\n",
       "  '5b9b5ef2-092d-4ff6-bd66-e56e03e60167',\n",
       "  'c951d69f-266c-4fa2-bbd8-f9d9db760259',\n",
       "  '93c14ccd-fae7-4afe-b7d7-9edbf657e1cc',\n",
       "  'fa6edf98-9dba-441b-a428-8016acfc7361',\n",
       "  '83c5cc16-887f-4a0f-9d50-ee68ec9dc6e3',\n",
       "  'b356543d-acc0-43f7-854e-5e6aacd0894e',\n",
       "  'c4eec4ad-376e-4788-add5-fb76bec1deb2',\n",
       "  'fd8d1490-fd56-4994-bf5d-0123f7fe50ad',\n",
       "  '8825d3b9-c809-4fd6-9b3d-155a2f3b4892',\n",
       "  'ca3159d4-cdeb-423f-9799-db9fde35063c',\n",
       "  'e0d4ae5c-5423-482d-aa48-a0bb1ed243f6',\n",
       "  'c54d82e5-b72f-4ca7-a656-edd06f60e49a',\n",
       "  'fbcf9d7c-eab0-4e86-bf3c-f099ddd8f0fd',\n",
       "  '3cca5e32-55f7-4cdd-adb4-b816e537aeb0',\n",
       "  '3a75c9de-1de9-49b2-8b0d-953f64f60300',\n",
       "  '24db36de-2d0a-4b61-8b59-8a81820d2812',\n",
       "  '5bf930ea-aeb2-4df0-a300-b352b533421b',\n",
       "  'e2d780f8-8655-4c47-9074-ff30229b3dc3',\n",
       "  '6c24402b-c583-48f0-90b4-cd5ce8c83a84',\n",
       "  'd7f2b19e-9f66-4660-a02e-49fa26ed6f21',\n",
       "  '5fdf86dc-aaed-405b-9b5a-d42774b26056',\n",
       "  'd0afcea4-8efb-4c1f-9932-81ad7135eb43',\n",
       "  '68cb731a-323b-467c-b3f2-bc4efdae0d51',\n",
       "  '6026cae9-90f0-4958-bc19-72e0ad396c57',\n",
       "  'fccc9c06-17a3-43b8-b245-3071194afaaa',\n",
       "  '8257056e-e997-4003-9260-74b4a7fc2d95',\n",
       "  '2749c26a-dcf3-4fc2-a588-acfa9fe82f00',\n",
       "  'bea2ba51-1f3b-4ef9-83dc-9372026dded6',\n",
       "  'c183da6c-8e76-4b30-b161-62f9f8797d9e',\n",
       "  '584a7351-b380-488e-a034-79f213c27991',\n",
       "  '2e9270f5-a8e5-4f22-adff-ac650305edc4',\n",
       "  '6c149df2-fdab-4b0d-a70a-dbc6afbac8e2',\n",
       "  '80ed0296-13ab-49e3-b251-f0ef1422d752',\n",
       "  '015e68e7-8788-4c4d-99c2-045cf2106699',\n",
       "  '5e058322-141e-4d94-827a-ddd410acc63f',\n",
       "  '9a0168c7-7f6c-4140-a11c-8aa59d95f96f',\n",
       "  '69963429-45c2-4ff5-91d4-a91b79eb9d7d',\n",
       "  'b18fe0d6-54e2-4660-b045-70e5da05bc44',\n",
       "  '7a118a5c-c1ff-4318-88b0-0640dcd1d392',\n",
       "  '58535e1e-9c92-4a0f-b949-d22da0cf84b5',\n",
       "  '3de54f53-f4e0-41ec-bd8c-37460999a244',\n",
       "  '4aa37b18-96a3-4a02-add3-d07e420b3ce1',\n",
       "  '43e8893d-bf13-4536-8b05-54282749e7c7',\n",
       "  '0a98eec8-7b09-4fa0-9b6f-1e4a47491e77',\n",
       "  '3218496f-21b5-4ce7-b6bf-3bb22ad00cf0',\n",
       "  'bb0afeb3-0c48-4b37-9e14-48a3fb215fd1',\n",
       "  'cb486321-5ed7-411d-9360-d771d5ab9735',\n",
       "  '6870ae98-bc8a-40e0-bf9f-8d441afe531a',\n",
       "  '79b2a71e-7234-4e67-bd37-d9c4091bb8e5',\n",
       "  '926cbd45-b88c-4628-a536-ff86f4f511d8',\n",
       "  '9d25fb23-63c7-486d-89b7-cfe4d1a58753',\n",
       "  '79206e2f-7c64-4aab-bbb4-2464d3cd59be',\n",
       "  '98c7637c-bace-4980-afb3-c64f52087f96',\n",
       "  '3af138e3-991e-4819-82f9-aabfea5f105d',\n",
       "  '8c7bc2e1-1c35-461b-a87f-3a7bf2197607',\n",
       "  '7e6adf4d-713e-414a-bdf0-3fd74eb64949',\n",
       "  'd08dc8f8-b1af-4f90-94e9-3bc56972838e',\n",
       "  '871afc30-962f-411e-ac14-99d6cc25d88a',\n",
       "  'd4c5f131-fa7a-45f7-ae34-d9b6120fedd6',\n",
       "  '611c6ea6-5b44-4256-a74b-5416616d311f',\n",
       "  '977ddba4-5d64-4113-8472-eb6d9f89f127',\n",
       "  '38f5c992-7eb8-410b-aca8-b3430bce1c0b',\n",
       "  '97f7fad3-7a51-46cf-99d3-23cd45ded17f',\n",
       "  '41c22bb1-b3d3-4bcf-81de-626ca53aeb8b',\n",
       "  'bc4393fa-e6c7-4b83-899a-84c167f5a8a7',\n",
       "  '471dc767-82e5-4a0d-815c-e9734cd8e9cd',\n",
       "  'f45f1569-9f3e-45ce-9e63-9a7aa27b9f06',\n",
       "  '78c2c3c4-22f0-4e65-a38d-6fbbb84e6525',\n",
       "  'b1e8d906-39c5-4eb7-86ff-dbbb14579632',\n",
       "  'aac89481-d116-42fd-b8be-86b1419ef758',\n",
       "  '021ebb02-2d60-4193-baf0-7e72a3dbfa92',\n",
       "  'f1db92be-7403-4d07-b677-8250cd5f9bb3',\n",
       "  'f8c859e1-2b76-4194-abbb-5083fc37136a',\n",
       "  'e9131449-a83d-4a4d-aaec-02edcfba62c7',\n",
       "  '01f446ae-55b3-4ce7-b9e9-6bbee5a7026a',\n",
       "  '25512b15-aec4-4178-8874-28ff51bbc534',\n",
       "  '265355d0-6136-4d9d-b10e-279d2113072b',\n",
       "  'a9028ca1-cc0d-457c-b917-96055386e9a6',\n",
       "  '1bde3c6f-fda6-4106-a4bd-265bda19e64a',\n",
       "  'b557f572-b22c-4d1d-93ee-33e246bea99e',\n",
       "  'a658868c-1f12-45a7-b036-8491f1ed0d76',\n",
       "  '222ff724-cae1-443a-b942-1c4a09a5df3e',\n",
       "  'c829c96a-663d-418b-b514-e74f3135f306',\n",
       "  '7ec29da4-a5e9-47ad-adc7-c4f1536a2658',\n",
       "  'f3d51c53-f2e3-432f-8b1b-b42c4072cee2',\n",
       "  '98b975f5-e205-4462-94d8-2bbec15a8d96',\n",
       "  '725ccc9c-7abc-4ef5-b027-65b490b9da1a',\n",
       "  '4940873a-f7ed-4dde-98f0-cda0593d48a1',\n",
       "  'f8508a27-7443-4786-9843-53ba4a9ebfb3',\n",
       "  'cee8332e-078b-47be-abc3-bfb8877afe63',\n",
       "  '0bb9c433-d6ec-40f0-9aaf-0ebef5298a6c',\n",
       "  '1f02554a-a514-4341-90de-6a0f6672cc24',\n",
       "  '7583daa8-9f28-4d1b-848b-e85de1cbaf03',\n",
       "  'a848402b-3e5c-448f-b4d1-f0a56065920e',\n",
       "  '8aca29ef-9963-4c19-b19b-88eeeaa0ec4e',\n",
       "  '4fcb2079-7a41-43c9-93e3-603b6f18dcea',\n",
       "  '7c635ee4-047e-4ca6-b383-d22f37a6e6ef',\n",
       "  '69f19859-7e9b-4c13-b36c-accaafbb1ebb',\n",
       "  '9b753823-730f-4f47-9d23-357da012dbc1',\n",
       "  '3d417673-38ac-4ec2-8b54-339dd5230184',\n",
       "  'ff7ff2de-8348-4d6f-aed1-4305a12f5a41',\n",
       "  'ec334649-94b8-42b4-8783-b9c6b3deaef3',\n",
       "  '67c13533-5fc5-4d74-a6b0-f0e0fd8d5ae9',\n",
       "  'ed1ff57d-6f5c-4bd6-9623-7ecdce379577',\n",
       "  '89fd5389-09a9-4bf6-96e2-54ec3fc8f14c',\n",
       "  '8fe7d646-e5dc-4227-b702-a512007f9b47',\n",
       "  '9fed2a4e-fba8-47e0-90c1-73240b56b260',\n",
       "  '2ea943d8-682c-4412-b310-ced9bd11e90e',\n",
       "  '1fb76d5c-779a-4cfe-885f-73fed6b850cc',\n",
       "  '8c44e129-f2d1-42b8-ba45-efe7f4271923',\n",
       "  '812c48d5-8975-4e28-8e81-879424b6d8b1',\n",
       "  'b11645b0-733e-4837-8242-29e3b1302d19',\n",
       "  '77ced7fd-b20c-429b-9a6b-e636bd1eb56a',\n",
       "  '9d739b01-93d5-4b7a-8037-c930e4cf8efe',\n",
       "  'e098b9d3-b275-4876-a5c0-a0716e7aaf7f',\n",
       "  'b55546d6-abab-402b-aeb9-a6b59213e945',\n",
       "  'ef5136d5-49c5-4757-9294-4f4f3cdc07d3',\n",
       "  'ff18ff97-5663-4bea-8338-15ba4581f3fd',\n",
       "  '9efc6cf9-58bc-4bc4-adb2-daa33053da38',\n",
       "  'f60aed07-2e3e-47cb-a9c8-f757840c338e',\n",
       "  '91f3d7d8-3c21-4f28-b5a9-ef64b4485343',\n",
       "  '37ea6a4e-5e19-435c-a3a5-281ad3f19d66',\n",
       "  '284538a7-4737-492f-9469-b10dc375122e',\n",
       "  '2b641ec5-a6be-4e83-9adf-731aacbd7f6d',\n",
       "  '4a85c202-8159-495e-a98f-c3205acfacb7',\n",
       "  'c9352420-c0bb-467f-b88d-f6804ca27145',\n",
       "  'aefc61c7-b367-491a-87f4-ea51d533d914',\n",
       "  'bfa12145-a8f5-42ca-92a3-7150d5a5f5ea',\n",
       "  '497ad2c3-61c2-494d-a0c7-9a364fba8cb2',\n",
       "  'd2b8be52-f9e5-439c-be06-18bf599f4c5f',\n",
       "  '99df0337-61f7-4622-92f7-4c82bcb67114',\n",
       "  'f530850d-5355-49f9-96d3-52af986ada0b',\n",
       "  '71411d0f-916b-4ae9-9919-42d2198c6745',\n",
       "  '0e39a3e0-b19c-4307-8185-69414d188bfb',\n",
       "  'f3a10dd8-d3c1-4ae3-a6fa-f8bd7a2b58e1',\n",
       "  'a166a277-2255-4509-aef5-18c420fd16b3',\n",
       "  '306edd00-3916-483b-aa81-564345b4312f',\n",
       "  'b4aac03a-7bcd-4a71-90df-c94eccc61ae9',\n",
       "  '3bc61c94-22f1-462c-a55e-58f6221d5c1e',\n",
       "  '71b3404a-0ae4-4e85-9089-44f422de282b',\n",
       "  'e0edd1fb-3b91-49ea-aa67-dca0641407d6',\n",
       "  '9fa7efe5-0a0c-47b6-9411-1f7ecbed802a',\n",
       "  'd76d135d-04be-4931-8517-35af1dd20f78',\n",
       "  '7b61b5c1-a560-425b-8bf0-c54ee9372c7f',\n",
       "  'ee2f9d28-ff5d-4471-a41e-ffd331c3fd88',\n",
       "  'f9c0e49c-29be-4261-b59c-1190d0be8078',\n",
       "  '4bceeb05-43bd-4e67-ae32-da8e30bccd4c',\n",
       "  '2959b84a-1116-4041-9434-40f266fc4225',\n",
       "  'd8c3353f-a051-4d42-93ed-4fe9166bd415',\n",
       "  '3560fd20-0bc7-4c4d-b0be-a2b8d494d9c9',\n",
       "  '37fae784-1b26-486e-9f1e-d1b01de36095',\n",
       "  '658aa29d-51b3-4056-840b-5e1ff4829adb',\n",
       "  'fdf91389-6b40-4990-bb75-775ed405f106',\n",
       "  '11e4727a-33eb-4190-91ec-308882c51daf',\n",
       "  '1b0bf34e-1263-4d99-bcdd-ddffe90aff98',\n",
       "  '51a2e4fb-cb80-482a-9c51-83e5929f7deb',\n",
       "  'b82c4875-3c91-4c04-a766-7d201d63d3af',\n",
       "  'bb799481-d1ed-49bd-a1b7-ccbcec9231f1',\n",
       "  '5089c8d5-61c3-42be-afad-77366f7417d2',\n",
       "  '7b094577-686e-463c-af92-8d030846f813',\n",
       "  '5e3feb25-c921-42f6-9e47-b2e9fcc360ad',\n",
       "  '7062d10b-1673-43ac-a571-6b73a64fb708',\n",
       "  'b2ed2a7f-6c26-4e88-81e0-038714ac7e84',\n",
       "  'bec53686-5c38-4e64-9ed6-c742280a87b3',\n",
       "  'de028c39-f23c-4050-ad0c-a19a316f3c92',\n",
       "  '1cc3cc91-1b5c-4f22-8d83-3e8b2e71d4f5',\n",
       "  'fb78d4db-12c1-488a-b15a-11e6764c0495',\n",
       "  '88d91349-8858-457d-8be8-d65362d80855',\n",
       "  'f7c5b11b-9f70-44f0-bc2b-c547c2c66064',\n",
       "  'bb107467-deb2-4407-bf6f-5308f51190da',\n",
       "  'bca916fd-b093-4d1c-9826-17858c3b941f',\n",
       "  'c17c66ea-8a27-4cd4-abce-6233543915cc',\n",
       "  '72b3fb21-80bb-4f96-9b2b-cd2a6b88d2fc',\n",
       "  '3ffd03a8-ae5b-4df6-9613-b8dee3164278',\n",
       "  '43d42578-32f4-4e3d-8ab9-5bbc3c2bf422',\n",
       "  '1e05bcbc-c3e8-42f4-9252-5a8361be9b3e',\n",
       "  '7fcb880a-b573-4d6e-9588-30a6ab2ab3a2',\n",
       "  '41c81903-b737-4cc7-9be9-14c4d99cbd94',\n",
       "  '6c67642b-5024-4360-bdcd-aacd2fa797b6',\n",
       "  '4e66cbbe-3694-4972-80a7-553b4ddcb39f',\n",
       "  '2db8cae3-475c-4794-a7e6-eab95dd4a5f7',\n",
       "  '2687fa47-a3fa-4f57-ab5d-47598b73e08e',\n",
       "  'bec8348e-d862-4d39-85e5-af21eab0cd93',\n",
       "  '7b75c6cc-5a41-4c18-ae47-ce248eccbf47',\n",
       "  '445caaa8-d86b-499f-a097-bf08076b0660',\n",
       "  '1090e271-69fb-484f-8fa6-d565b9f868d0',\n",
       "  'ca6abd97-0414-4f67-8774-9f07e9babcad',\n",
       "  'ff88633e-13c8-474a-8578-d09cd8ca0f22',\n",
       "  '7b634711-31fb-48ac-8679-6e9e2ddeca16',\n",
       "  '305336d2-561e-4de1-9937-007a968f7b29',\n",
       "  '8469f4a7-5938-4b9c-b2b4-9a6a54393cc8',\n",
       "  '4fdbc334-174a-424a-8ede-15d7b68cabd7',\n",
       "  'e35e2fa0-55f1-416a-b364-933c3190074c',\n",
       "  '425bb8a1-afdd-47df-b560-3a6095d34eb8',\n",
       "  '084624c6-36d7-4984-8485-0d5e0565210d',\n",
       "  '053045b4-1223-4559-a599-6290b897cf97',\n",
       "  'aa3f8cf0-fc0a-4c77-8e4b-63b48be9f04b',\n",
       "  'bc9f9f94-a13b-42c8-8c60-7421bd2d5086',\n",
       "  '9f3b89ef-ff5b-4989-871a-e13deb74da85',\n",
       "  'e2da09d4-c398-4514-8b44-0fd14a78934a',\n",
       "  'd8f318d6-af06-4b3a-abe1-f6b8946fa07c',\n",
       "  '236261fc-b5c7-49ae-bbbf-5e1b4211ddcc',\n",
       "  'a876ed0f-34bf-4802-a6d0-b90289a69c64',\n",
       "  '4d1eb338-cd1b-46f1-8f7e-324fc28af2a1',\n",
       "  'e641efb5-dbaf-470d-b87a-4a654f0c4815',\n",
       "  'ffd5a408-dc73-41f2-af89-17874c0afe18',\n",
       "  'a181ad6f-82e7-479e-a842-36384302976c',\n",
       "  'b8bdc0d8-a83a-4e4c-8020-77639c11df11',\n",
       "  '877a2581-00f3-4c65-b975-8423240979d3',\n",
       "  '11375913-4e7f-459b-9fe7-1879b6458124',\n",
       "  '238134ad-5e58-45ef-8af9-9388488df0bd',\n",
       "  'b5fd1e1b-2b01-4aa5-9d47-40d4a4016179',\n",
       "  'bda8f8cb-4345-4013-ae7c-4e6150296c78'],\n",
       " 'embeddings': array([[-0.02890195,  0.01371561,  0.01319193, ..., -0.00628033,\n",
       "         -0.00912366, -0.00225354],\n",
       "        [-0.06687635,  0.01050784, -0.01053225, ..., -0.00042948,\n",
       "         -0.01780386,  0.01176747],\n",
       "        [-0.01221734, -0.012147  , -0.05715948, ...,  0.02648854,\n",
       "          0.03951912, -0.01088541],\n",
       "        ...,\n",
       "        [ 0.02651327, -0.006016  ,  0.00671523, ..., -0.03031903,\n",
       "          0.02162761,  0.03869981],\n",
       "        [-0.07041045, -0.01015912, -0.01442365, ..., -0.06875956,\n",
       "          0.03289804,  0.03707669],\n",
       "        [-0.01784221,  0.03013815, -0.00866475, ..., -0.02040559,\n",
       "         -0.02582635, -0.00374245]]),\n",
       " 'metadatas': [{'page': 2, 'page_label': '1', 'source': './AI_Book.pdf'},\n",
       "  {'page': 3, 'page_label': '2', 'source': './AI_Book.pdf'},\n",
       "  {'page': 4, 'page_label': '3', 'source': './AI_Book.pdf'},\n",
       "  {'page': 5, 'page_label': '4', 'source': './AI_Book.pdf'},\n",
       "  {'page': 5, 'page_label': '4', 'source': './AI_Book.pdf'},\n",
       "  {'page': 6, 'page_label': '5', 'source': './AI_Book.pdf'},\n",
       "  {'page': 6, 'page_label': '5', 'source': './AI_Book.pdf'},\n",
       "  {'page': 7, 'page_label': '6', 'source': './AI_Book.pdf'},\n",
       "  {'page': 7, 'page_label': '6', 'source': './AI_Book.pdf'},\n",
       "  {'page': 8, 'page_label': '7', 'source': './AI_Book.pdf'},\n",
       "  {'page': 8, 'page_label': '7', 'source': './AI_Book.pdf'},\n",
       "  {'page': 9, 'page_label': '8', 'source': './AI_Book.pdf'},\n",
       "  {'page': 9, 'page_label': '8', 'source': './AI_Book.pdf'},\n",
       "  {'page': 10, 'page_label': '9', 'source': './AI_Book.pdf'},\n",
       "  {'page': 10, 'page_label': '9', 'source': './AI_Book.pdf'},\n",
       "  {'page': 11, 'page_label': '10', 'source': './AI_Book.pdf'},\n",
       "  {'page': 12, 'page_label': '11', 'source': './AI_Book.pdf'},\n",
       "  {'page': 13, 'page_label': '12', 'source': './AI_Book.pdf'},\n",
       "  {'page': 14, 'page_label': '13', 'source': './AI_Book.pdf'},\n",
       "  {'page': 15, 'page_label': '14', 'source': './AI_Book.pdf'},\n",
       "  {'page': 16, 'page_label': '15', 'source': './AI_Book.pdf'},\n",
       "  {'page': 17, 'page_label': '16', 'source': './AI_Book.pdf'},\n",
       "  {'page': 18, 'page_label': '17', 'source': './AI_Book.pdf'},\n",
       "  {'page': 19, 'page_label': '18', 'source': './AI_Book.pdf'},\n",
       "  {'page': 20, 'page_label': '19', 'source': './AI_Book.pdf'},\n",
       "  {'page': 21, 'page_label': '20', 'source': './AI_Book.pdf'},\n",
       "  {'page': 22, 'page_label': '21', 'source': './AI_Book.pdf'},\n",
       "  {'page': 23, 'page_label': '22', 'source': './AI_Book.pdf'},\n",
       "  {'page': 24, 'page_label': '23', 'source': './AI_Book.pdf'},\n",
       "  {'page': 25, 'page_label': '24', 'source': './AI_Book.pdf'},\n",
       "  {'page': 26, 'page_label': '1', 'source': './AI_Book.pdf'},\n",
       "  {'page': 28, 'page_label': '3', 'source': './AI_Book.pdf'},\n",
       "  {'page': 29, 'page_label': '4', 'source': './AI_Book.pdf'},\n",
       "  {'page': 30, 'page_label': '5', 'source': './AI_Book.pdf'},\n",
       "  {'page': 31, 'page_label': '6', 'source': './AI_Book.pdf'},\n",
       "  {'page': 32, 'page_label': '7', 'source': './AI_Book.pdf'},\n",
       "  {'page': 33, 'page_label': '8', 'source': './AI_Book.pdf'},\n",
       "  {'page': 34, 'page_label': '9', 'source': './AI_Book.pdf'},\n",
       "  {'page': 35, 'page_label': '10', 'source': './AI_Book.pdf'},\n",
       "  {'page': 36, 'page_label': '11', 'source': './AI_Book.pdf'},\n",
       "  {'page': 37, 'page_label': '12', 'source': './AI_Book.pdf'},\n",
       "  {'page': 38, 'page_label': '13', 'source': './AI_Book.pdf'},\n",
       "  {'page': 39, 'page_label': '14', 'source': './AI_Book.pdf'},\n",
       "  {'page': 40, 'page_label': '15', 'source': './AI_Book.pdf'},\n",
       "  {'page': 41, 'page_label': '16', 'source': './AI_Book.pdf'},\n",
       "  {'page': 42, 'page_label': '17', 'source': './AI_Book.pdf'},\n",
       "  {'page': 43, 'page_label': '18', 'source': './AI_Book.pdf'},\n",
       "  {'page': 44, 'page_label': '19', 'source': './AI_Book.pdf'},\n",
       "  {'page': 45, 'page_label': '20', 'source': './AI_Book.pdf'},\n",
       "  {'page': 46, 'page_label': '21', 'source': './AI_Book.pdf'},\n",
       "  {'page': 47, 'page_label': '22', 'source': './AI_Book.pdf'},\n",
       "  {'page': 48, 'page_label': '23', 'source': './AI_Book.pdf'},\n",
       "  {'page': 49, 'page_label': '24', 'source': './AI_Book.pdf'},\n",
       "  {'page': 50, 'page_label': '25', 'source': './AI_Book.pdf'},\n",
       "  {'page': 51, 'page_label': '26', 'source': './AI_Book.pdf'},\n",
       "  {'page': 52, 'page_label': '27', 'source': './AI_Book.pdf'},\n",
       "  {'page': 53, 'page_label': '28', 'source': './AI_Book.pdf'},\n",
       "  {'page': 54, 'page_label': '29', 'source': './AI_Book.pdf'},\n",
       "  {'page': 55, 'page_label': '30', 'source': './AI_Book.pdf'},\n",
       "  {'page': 56, 'page_label': '31', 'source': './AI_Book.pdf'},\n",
       "  {'page': 57, 'page_label': '32', 'source': './AI_Book.pdf'},\n",
       "  {'page': 58, 'page_label': '33', 'source': './AI_Book.pdf'},\n",
       "  {'page': 59, 'page_label': '34', 'source': './AI_Book.pdf'},\n",
       "  {'page': 60, 'page_label': '35', 'source': './AI_Book.pdf'},\n",
       "  {'page': 62, 'page_label': '37', 'source': './AI_Book.pdf'},\n",
       "  {'page': 63, 'page_label': '38', 'source': './AI_Book.pdf'},\n",
       "  {'page': 64, 'page_label': '39', 'source': './AI_Book.pdf'},\n",
       "  {'page': 65, 'page_label': '40', 'source': './AI_Book.pdf'},\n",
       "  {'page': 66, 'page_label': '41', 'source': './AI_Book.pdf'},\n",
       "  {'page': 67, 'page_label': '42', 'source': './AI_Book.pdf'},\n",
       "  {'page': 68, 'page_label': '43', 'source': './AI_Book.pdf'},\n",
       "  {'page': 69, 'page_label': '44', 'source': './AI_Book.pdf'},\n",
       "  {'page': 70, 'page_label': '45', 'source': './AI_Book.pdf'},\n",
       "  {'page': 71, 'page_label': '46', 'source': './AI_Book.pdf'},\n",
       "  {'page': 72, 'page_label': '47', 'source': './AI_Book.pdf'},\n",
       "  {'page': 73, 'page_label': '48', 'source': './AI_Book.pdf'},\n",
       "  {'page': 74, 'page_label': '49', 'source': './AI_Book.pdf'},\n",
       "  {'page': 75, 'page_label': '50', 'source': './AI_Book.pdf'},\n",
       "  {'page': 76, 'page_label': '51', 'source': './AI_Book.pdf'},\n",
       "  {'page': 77, 'page_label': '52', 'source': './AI_Book.pdf'},\n",
       "  {'page': 78, 'page_label': '53', 'source': './AI_Book.pdf'},\n",
       "  {'page': 79, 'page_label': '54', 'source': './AI_Book.pdf'},\n",
       "  {'page': 80, 'page_label': '55', 'source': './AI_Book.pdf'},\n",
       "  {'page': 81, 'page_label': '56', 'source': './AI_Book.pdf'},\n",
       "  {'page': 82, 'page_label': '57', 'source': './AI_Book.pdf'},\n",
       "  {'page': 83, 'page_label': '58', 'source': './AI_Book.pdf'},\n",
       "  {'page': 84, 'page_label': '59', 'source': './AI_Book.pdf'},\n",
       "  {'page': 85, 'page_label': '60', 'source': './AI_Book.pdf'},\n",
       "  {'page': 86, 'page_label': '61', 'source': './AI_Book.pdf'},\n",
       "  {'page': 87, 'page_label': '62', 'source': './AI_Book.pdf'},\n",
       "  {'page': 88, 'page_label': '63', 'source': './AI_Book.pdf'},\n",
       "  {'page': 89, 'page_label': '64', 'source': './AI_Book.pdf'},\n",
       "  {'page': 90, 'page_label': '65', 'source': './AI_Book.pdf'},\n",
       "  {'page': 91, 'page_label': '66', 'source': './AI_Book.pdf'},\n",
       "  {'page': 92, 'page_label': '67', 'source': './AI_Book.pdf'},\n",
       "  {'page': 93, 'page_label': '68', 'source': './AI_Book.pdf'},\n",
       "  {'page': 94, 'page_label': '69', 'source': './AI_Book.pdf'},\n",
       "  {'page': 95, 'page_label': '70', 'source': './AI_Book.pdf'},\n",
       "  {'page': 96, 'page_label': '71', 'source': './AI_Book.pdf'},\n",
       "  {'page': 97, 'page_label': '72', 'source': './AI_Book.pdf'},\n",
       "  {'page': 98, 'page_label': '73', 'source': './AI_Book.pdf'},\n",
       "  {'page': 99, 'page_label': '74', 'source': './AI_Book.pdf'},\n",
       "  {'page': 100, 'page_label': '75', 'source': './AI_Book.pdf'},\n",
       "  {'page': 101, 'page_label': '76', 'source': './AI_Book.pdf'},\n",
       "  {'page': 102, 'page_label': '77', 'source': './AI_Book.pdf'},\n",
       "  {'page': 103, 'page_label': '78', 'source': './AI_Book.pdf'},\n",
       "  {'page': 104, 'page_label': '79', 'source': './AI_Book.pdf'},\n",
       "  {'page': 105, 'page_label': '80', 'source': './AI_Book.pdf'},\n",
       "  {'page': 106, 'page_label': '81', 'source': './AI_Book.pdf'},\n",
       "  {'page': 107, 'page_label': '82', 'source': './AI_Book.pdf'},\n",
       "  {'page': 108, 'page_label': '83', 'source': './AI_Book.pdf'},\n",
       "  {'page': 109, 'page_label': '84', 'source': './AI_Book.pdf'},\n",
       "  {'page': 110, 'page_label': '85', 'source': './AI_Book.pdf'},\n",
       "  {'page': 112, 'page_label': '87', 'source': './AI_Book.pdf'},\n",
       "  {'page': 113, 'page_label': '88', 'source': './AI_Book.pdf'},\n",
       "  {'page': 114, 'page_label': '89', 'source': './AI_Book.pdf'},\n",
       "  {'page': 115, 'page_label': '90', 'source': './AI_Book.pdf'},\n",
       "  {'page': 116, 'page_label': '91', 'source': './AI_Book.pdf'},\n",
       "  {'page': 117, 'page_label': '92', 'source': './AI_Book.pdf'},\n",
       "  {'page': 118, 'page_label': '93', 'source': './AI_Book.pdf'},\n",
       "  {'page': 119, 'page_label': '94', 'source': './AI_Book.pdf'},\n",
       "  {'page': 120, 'page_label': '95', 'source': './AI_Book.pdf'},\n",
       "  {'page': 121, 'page_label': '96', 'source': './AI_Book.pdf'},\n",
       "  {'page': 122, 'page_label': '97', 'source': './AI_Book.pdf'},\n",
       "  {'page': 123, 'page_label': '98', 'source': './AI_Book.pdf'},\n",
       "  {'page': 124, 'page_label': '99', 'source': './AI_Book.pdf'},\n",
       "  {'page': 125, 'page_label': '100', 'source': './AI_Book.pdf'},\n",
       "  {'page': 126, 'page_label': '101', 'source': './AI_Book.pdf'},\n",
       "  {'page': 127, 'page_label': '102', 'source': './AI_Book.pdf'},\n",
       "  {'page': 128, 'page_label': '103', 'source': './AI_Book.pdf'},\n",
       "  {'page': 129, 'page_label': '104', 'source': './AI_Book.pdf'},\n",
       "  {'page': 130, 'page_label': '105', 'source': './AI_Book.pdf'},\n",
       "  {'page': 131, 'page_label': '106', 'source': './AI_Book.pdf'},\n",
       "  {'page': 132, 'page_label': '107', 'source': './AI_Book.pdf'},\n",
       "  {'page': 133, 'page_label': '108', 'source': './AI_Book.pdf'},\n",
       "  {'page': 134, 'page_label': '109', 'source': './AI_Book.pdf'},\n",
       "  {'page': 135, 'page_label': '110', 'source': './AI_Book.pdf'},\n",
       "  {'page': 136, 'page_label': '111', 'source': './AI_Book.pdf'},\n",
       "  {'page': 138, 'page_label': '113', 'source': './AI_Book.pdf'},\n",
       "  {'page': 139, 'page_label': '114', 'source': './AI_Book.pdf'},\n",
       "  {'page': 140, 'page_label': '115', 'source': './AI_Book.pdf'},\n",
       "  {'page': 141, 'page_label': '116', 'source': './AI_Book.pdf'},\n",
       "  {'page': 142, 'page_label': '117', 'source': './AI_Book.pdf'},\n",
       "  {'page': 143, 'page_label': '118', 'source': './AI_Book.pdf'},\n",
       "  {'page': 144, 'page_label': '119', 'source': './AI_Book.pdf'},\n",
       "  {'page': 145, 'page_label': '120', 'source': './AI_Book.pdf'},\n",
       "  {'page': 146, 'page_label': '121', 'source': './AI_Book.pdf'},\n",
       "  {'page': 147, 'page_label': '122', 'source': './AI_Book.pdf'},\n",
       "  {'page': 148, 'page_label': '123', 'source': './AI_Book.pdf'},\n",
       "  {'page': 149, 'page_label': '124', 'source': './AI_Book.pdf'},\n",
       "  {'page': 150, 'page_label': '125', 'source': './AI_Book.pdf'},\n",
       "  {'page': 151, 'page_label': '126', 'source': './AI_Book.pdf'},\n",
       "  {'page': 152, 'page_label': '127', 'source': './AI_Book.pdf'},\n",
       "  {'page': 153, 'page_label': '128', 'source': './AI_Book.pdf'},\n",
       "  {'page': 154, 'page_label': '129', 'source': './AI_Book.pdf'},\n",
       "  {'page': 155, 'page_label': '130', 'source': './AI_Book.pdf'},\n",
       "  {'page': 156, 'page_label': '131', 'source': './AI_Book.pdf'},\n",
       "  {'page': 157, 'page_label': '132', 'source': './AI_Book.pdf'},\n",
       "  {'page': 158, 'page_label': '133', 'source': './AI_Book.pdf'},\n",
       "  {'page': 159, 'page_label': '134', 'source': './AI_Book.pdf'},\n",
       "  {'page': 160, 'page_label': '135', 'source': './AI_Book.pdf'},\n",
       "  {'page': 161, 'page_label': '136', 'source': './AI_Book.pdf'},\n",
       "  {'page': 162, 'page_label': '137', 'source': './AI_Book.pdf'},\n",
       "  {'page': 163, 'page_label': '138', 'source': './AI_Book.pdf'},\n",
       "  {'page': 164, 'page_label': '139', 'source': './AI_Book.pdf'},\n",
       "  {'page': 165, 'page_label': '140', 'source': './AI_Book.pdf'},\n",
       "  {'page': 166, 'page_label': '141', 'source': './AI_Book.pdf'},\n",
       "  {'page': 167, 'page_label': '142', 'source': './AI_Book.pdf'},\n",
       "  {'page': 168, 'page_label': '143', 'source': './AI_Book.pdf'},\n",
       "  {'page': 169, 'page_label': '144', 'source': './AI_Book.pdf'},\n",
       "  {'page': 170, 'page_label': '145', 'source': './AI_Book.pdf'},\n",
       "  {'page': 171, 'page_label': '146', 'source': './AI_Book.pdf'},\n",
       "  {'page': 172, 'page_label': '147', 'source': './AI_Book.pdf'},\n",
       "  {'page': 173, 'page_label': '148', 'source': './AI_Book.pdf'},\n",
       "  {'page': 174, 'page_label': '149', 'source': './AI_Book.pdf'},\n",
       "  {'page': 175, 'page_label': '150', 'source': './AI_Book.pdf'},\n",
       "  {'page': 176, 'page_label': '151', 'source': './AI_Book.pdf'},\n",
       "  {'page': 177, 'page_label': '152', 'source': './AI_Book.pdf'},\n",
       "  {'page': 178, 'page_label': '153', 'source': './AI_Book.pdf'},\n",
       "  {'page': 180, 'page_label': '155', 'source': './AI_Book.pdf'},\n",
       "  {'page': 181, 'page_label': '156', 'source': './AI_Book.pdf'},\n",
       "  {'page': 182, 'page_label': '157', 'source': './AI_Book.pdf'},\n",
       "  {'page': 183, 'page_label': '158', 'source': './AI_Book.pdf'},\n",
       "  {'page': 184, 'page_label': '159', 'source': './AI_Book.pdf'},\n",
       "  {'page': 185, 'page_label': '160', 'source': './AI_Book.pdf'},\n",
       "  {'page': 186, 'page_label': '161', 'source': './AI_Book.pdf'},\n",
       "  {'page': 187, 'page_label': '162', 'source': './AI_Book.pdf'},\n",
       "  {'page': 188, 'page_label': '163', 'source': './AI_Book.pdf'},\n",
       "  {'page': 189, 'page_label': '164', 'source': './AI_Book.pdf'},\n",
       "  {'page': 190, 'page_label': '165', 'source': './AI_Book.pdf'},\n",
       "  {'page': 191, 'page_label': '166', 'source': './AI_Book.pdf'},\n",
       "  {'page': 192, 'page_label': '167', 'source': './AI_Book.pdf'},\n",
       "  {'page': 193, 'page_label': '168', 'source': './AI_Book.pdf'},\n",
       "  {'page': 194, 'page_label': '169', 'source': './AI_Book.pdf'},\n",
       "  {'page': 195, 'page_label': '170', 'source': './AI_Book.pdf'},\n",
       "  {'page': 196, 'page_label': '171', 'source': './AI_Book.pdf'},\n",
       "  {'page': 197, 'page_label': '172', 'source': './AI_Book.pdf'},\n",
       "  {'page': 198, 'page_label': '173', 'source': './AI_Book.pdf'},\n",
       "  {'page': 199, 'page_label': '174', 'source': './AI_Book.pdf'},\n",
       "  {'page': 200, 'page_label': '175', 'source': './AI_Book.pdf'},\n",
       "  {'page': 201, 'page_label': '176', 'source': './AI_Book.pdf'},\n",
       "  {'page': 202, 'page_label': '177', 'source': './AI_Book.pdf'},\n",
       "  {'page': 203, 'page_label': '178', 'source': './AI_Book.pdf'},\n",
       "  {'page': 204, 'page_label': '179', 'source': './AI_Book.pdf'},\n",
       "  {'page': 205, 'page_label': '180', 'source': './AI_Book.pdf'},\n",
       "  {'page': 206, 'page_label': '181', 'source': './AI_Book.pdf'},\n",
       "  {'page': 207, 'page_label': '182', 'source': './AI_Book.pdf'},\n",
       "  {'page': 208, 'page_label': '183', 'source': './AI_Book.pdf'},\n",
       "  {'page': 209, 'page_label': '184', 'source': './AI_Book.pdf'},\n",
       "  {'page': 210, 'page_label': '185', 'source': './AI_Book.pdf'},\n",
       "  {'page': 211, 'page_label': '186', 'source': './AI_Book.pdf'},\n",
       "  {'page': 212, 'page_label': '187', 'source': './AI_Book.pdf'},\n",
       "  {'page': 213, 'page_label': '188', 'source': './AI_Book.pdf'},\n",
       "  {'page': 214, 'page_label': '189', 'source': './AI_Book.pdf'},\n",
       "  {'page': 215, 'page_label': '190', 'source': './AI_Book.pdf'},\n",
       "  {'page': 216, 'page_label': '191', 'source': './AI_Book.pdf'},\n",
       "  {'page': 217, 'page_label': '192', 'source': './AI_Book.pdf'},\n",
       "  {'page': 218, 'page_label': '193', 'source': './AI_Book.pdf'},\n",
       "  {'page': 219, 'page_label': '194', 'source': './AI_Book.pdf'},\n",
       "  {'page': 220, 'page_label': '195', 'source': './AI_Book.pdf'},\n",
       "  {'page': 221, 'page_label': '196', 'source': './AI_Book.pdf'},\n",
       "  {'page': 222, 'page_label': '197', 'source': './AI_Book.pdf'},\n",
       "  {'page': 223, 'page_label': '198', 'source': './AI_Book.pdf'},\n",
       "  {'page': 224, 'page_label': '199', 'source': './AI_Book.pdf'},\n",
       "  {'page': 225, 'page_label': '200', 'source': './AI_Book.pdf'},\n",
       "  {'page': 226, 'page_label': '201', 'source': './AI_Book.pdf'},\n",
       "  {'page': 227, 'page_label': '202', 'source': './AI_Book.pdf'},\n",
       "  {'page': 228, 'page_label': '203', 'source': './AI_Book.pdf'},\n",
       "  {'page': 229, 'page_label': '204', 'source': './AI_Book.pdf'},\n",
       "  {'page': 230, 'page_label': '205', 'source': './AI_Book.pdf'},\n",
       "  {'page': 231, 'page_label': '206', 'source': './AI_Book.pdf'},\n",
       "  {'page': 232, 'page_label': '207', 'source': './AI_Book.pdf'},\n",
       "  {'page': 233, 'page_label': '208', 'source': './AI_Book.pdf'},\n",
       "  {'page': 234, 'page_label': '209', 'source': './AI_Book.pdf'},\n",
       "  {'page': 235, 'page_label': '210', 'source': './AI_Book.pdf'},\n",
       "  {'page': 236, 'page_label': '211', 'source': './AI_Book.pdf'},\n",
       "  {'page': 237, 'page_label': '212', 'source': './AI_Book.pdf'},\n",
       "  {'page': 238, 'page_label': '213', 'source': './AI_Book.pdf'},\n",
       "  {'page': 239, 'page_label': '214', 'source': './AI_Book.pdf'},\n",
       "  {'page': 240, 'page_label': '215', 'source': './AI_Book.pdf'},\n",
       "  {'page': 241, 'page_label': '216', 'source': './AI_Book.pdf'},\n",
       "  {'page': 242, 'page_label': '217', 'source': './AI_Book.pdf'},\n",
       "  {'page': 243, 'page_label': '218', 'source': './AI_Book.pdf'},\n",
       "  {'page': 244, 'page_label': '219', 'source': './AI_Book.pdf'},\n",
       "  {'page': 245, 'page_label': '220', 'source': './AI_Book.pdf'},\n",
       "  {'page': 246, 'page_label': '221', 'source': './AI_Book.pdf'},\n",
       "  {'page': 247, 'page_label': '222', 'source': './AI_Book.pdf'},\n",
       "  {'page': 248, 'page_label': '223', 'source': './AI_Book.pdf'},\n",
       "  {'page': 249, 'page_label': '224', 'source': './AI_Book.pdf'},\n",
       "  {'page': 250, 'page_label': '225', 'source': './AI_Book.pdf'},\n",
       "  {'page': 251, 'page_label': '226', 'source': './AI_Book.pdf'},\n",
       "  {'page': 252, 'page_label': '227', 'source': './AI_Book.pdf'},\n",
       "  {'page': 253, 'page_label': '228', 'source': './AI_Book.pdf'},\n",
       "  {'page': 254, 'page_label': '229', 'source': './AI_Book.pdf'},\n",
       "  {'page': 255, 'page_label': '230', 'source': './AI_Book.pdf'},\n",
       "  {'page': 256, 'page_label': '231', 'source': './AI_Book.pdf'},\n",
       "  {'page': 257, 'page_label': '232', 'source': './AI_Book.pdf'},\n",
       "  {'page': 258, 'page_label': '233', 'source': './AI_Book.pdf'},\n",
       "  {'page': 259, 'page_label': '234', 'source': './AI_Book.pdf'},\n",
       "  {'page': 260, 'page_label': '235', 'source': './AI_Book.pdf'},\n",
       "  {'page': 261, 'page_label': '236', 'source': './AI_Book.pdf'},\n",
       "  {'page': 262, 'page_label': '237', 'source': './AI_Book.pdf'},\n",
       "  {'page': 263, 'page_label': '238', 'source': './AI_Book.pdf'},\n",
       "  {'page': 264, 'page_label': '239', 'source': './AI_Book.pdf'},\n",
       "  {'page': 265, 'page_label': '240', 'source': './AI_Book.pdf'},\n",
       "  {'page': 266, 'page_label': '241', 'source': './AI_Book.pdf'},\n",
       "  {'page': 267, 'page_label': '242', 'source': './AI_Book.pdf'},\n",
       "  {'page': 268, 'page_label': '243', 'source': './AI_Book.pdf'},\n",
       "  {'page': 269, 'page_label': '244', 'source': './AI_Book.pdf'},\n",
       "  {'page': 270, 'page_label': '245', 'source': './AI_Book.pdf'},\n",
       "  {'page': 271, 'page_label': '246', 'source': './AI_Book.pdf'},\n",
       "  {'page': 272, 'page_label': '247', 'source': './AI_Book.pdf'},\n",
       "  {'page': 273, 'page_label': '248', 'source': './AI_Book.pdf'},\n",
       "  {'page': 274, 'page_label': '249', 'source': './AI_Book.pdf'},\n",
       "  {'page': 275, 'page_label': '250', 'source': './AI_Book.pdf'},\n",
       "  {'page': 276, 'page_label': '251', 'source': './AI_Book.pdf'},\n",
       "  {'page': 277, 'page_label': '252', 'source': './AI_Book.pdf'},\n",
       "  {'page': 278, 'page_label': '253', 'source': './AI_Book.pdf'},\n",
       "  {'page': 279, 'page_label': '254', 'source': './AI_Book.pdf'},\n",
       "  {'page': 280, 'page_label': '255', 'source': './AI_Book.pdf'},\n",
       "  {'page': 281, 'page_label': '256', 'source': './AI_Book.pdf'},\n",
       "  {'page': 282, 'page_label': '257', 'source': './AI_Book.pdf'},\n",
       "  {'page': 283, 'page_label': '258', 'source': './AI_Book.pdf'},\n",
       "  {'page': 284, 'page_label': '259', 'source': './AI_Book.pdf'},\n",
       "  {'page': 285, 'page_label': '260', 'source': './AI_Book.pdf'},\n",
       "  {'page': 286, 'page_label': '261', 'source': './AI_Book.pdf'},\n",
       "  {'page': 287, 'page_label': '262', 'source': './AI_Book.pdf'},\n",
       "  {'page': 288, 'page_label': '263', 'source': './AI_Book.pdf'},\n",
       "  {'page': 289, 'page_label': '264', 'source': './AI_Book.pdf'},\n",
       "  {'page': 290, 'page_label': '265', 'source': './AI_Book.pdf'},\n",
       "  {'page': 291, 'page_label': '266', 'source': './AI_Book.pdf'},\n",
       "  {'page': 292, 'page_label': '267', 'source': './AI_Book.pdf'},\n",
       "  {'page': 293, 'page_label': '268', 'source': './AI_Book.pdf'},\n",
       "  {'page': 294, 'page_label': '269', 'source': './AI_Book.pdf'},\n",
       "  {'page': 295, 'page_label': '270', 'source': './AI_Book.pdf'},\n",
       "  {'page': 296, 'page_label': '271', 'source': './AI_Book.pdf'},\n",
       "  {'page': 297, 'page_label': '272', 'source': './AI_Book.pdf'},\n",
       "  {'page': 298, 'page_label': '273', 'source': './AI_Book.pdf'},\n",
       "  {'page': 299, 'page_label': '274', 'source': './AI_Book.pdf'},\n",
       "  {'page': 300, 'page_label': '275', 'source': './AI_Book.pdf'},\n",
       "  {'page': 302, 'page_label': '277', 'source': './AI_Book.pdf'},\n",
       "  {'page': 303, 'page_label': '278', 'source': './AI_Book.pdf'},\n",
       "  {'page': 304, 'page_label': '279', 'source': './AI_Book.pdf'},\n",
       "  {'page': 305, 'page_label': '280', 'source': './AI_Book.pdf'},\n",
       "  {'page': 306, 'page_label': '281', 'source': './AI_Book.pdf'},\n",
       "  {'page': 307, 'page_label': '282', 'source': './AI_Book.pdf'},\n",
       "  {'page': 308, 'page_label': '283', 'source': './AI_Book.pdf'},\n",
       "  {'page': 309, 'page_label': '284', 'source': './AI_Book.pdf'},\n",
       "  {'page': 310, 'page_label': '285', 'source': './AI_Book.pdf'},\n",
       "  {'page': 311, 'page_label': '286', 'source': './AI_Book.pdf'},\n",
       "  {'page': 312, 'page_label': '287', 'source': './AI_Book.pdf'},\n",
       "  {'page': 313, 'page_label': '288', 'source': './AI_Book.pdf'},\n",
       "  {'page': 314, 'page_label': '289', 'source': './AI_Book.pdf'},\n",
       "  {'page': 315, 'page_label': '290', 'source': './AI_Book.pdf'},\n",
       "  {'page': 316, 'page_label': '291', 'source': './AI_Book.pdf'},\n",
       "  {'page': 317, 'page_label': '292', 'source': './AI_Book.pdf'},\n",
       "  {'page': 318, 'page_label': '293', 'source': './AI_Book.pdf'},\n",
       "  {'page': 319, 'page_label': '294', 'source': './AI_Book.pdf'},\n",
       "  {'page': 320, 'page_label': '295', 'source': './AI_Book.pdf'},\n",
       "  {'page': 321, 'page_label': '296', 'source': './AI_Book.pdf'},\n",
       "  {'page': 322, 'page_label': '297', 'source': './AI_Book.pdf'},\n",
       "  {'page': 323, 'page_label': '298', 'source': './AI_Book.pdf'},\n",
       "  {'page': 324, 'page_label': '299', 'source': './AI_Book.pdf'},\n",
       "  {'page': 325, 'page_label': '300', 'source': './AI_Book.pdf'},\n",
       "  {'page': 326, 'page_label': '301', 'source': './AI_Book.pdf'},\n",
       "  {'page': 327, 'page_label': '302', 'source': './AI_Book.pdf'},\n",
       "  {'page': 328, 'page_label': '303', 'source': './AI_Book.pdf'},\n",
       "  {'page': 329, 'page_label': '304', 'source': './AI_Book.pdf'},\n",
       "  {'page': 330, 'page_label': '305', 'source': './AI_Book.pdf'},\n",
       "  {'page': 331, 'page_label': '306', 'source': './AI_Book.pdf'},\n",
       "  {'page': 332, 'page_label': '307', 'source': './AI_Book.pdf'},\n",
       "  {'page': 333, 'page_label': '308', 'source': './AI_Book.pdf'},\n",
       "  {'page': 334, 'page_label': '309', 'source': './AI_Book.pdf'},\n",
       "  {'page': 335, 'page_label': '310', 'source': './AI_Book.pdf'},\n",
       "  {'page': 336, 'page_label': '311', 'source': './AI_Book.pdf'},\n",
       "  {'page': 337, 'page_label': '312', 'source': './AI_Book.pdf'},\n",
       "  {'page': 338, 'page_label': '313', 'source': './AI_Book.pdf'},\n",
       "  {'page': 339, 'page_label': '314', 'source': './AI_Book.pdf'},\n",
       "  {'page': 340, 'page_label': '315', 'source': './AI_Book.pdf'},\n",
       "  {'page': 341, 'page_label': '316', 'source': './AI_Book.pdf'},\n",
       "  {'page': 342, 'page_label': '317', 'source': './AI_Book.pdf'},\n",
       "  {'page': 343, 'page_label': '318', 'source': './AI_Book.pdf'},\n",
       "  {'page': 344, 'page_label': '319', 'source': './AI_Book.pdf'},\n",
       "  {'page': 345, 'page_label': '320', 'source': './AI_Book.pdf'},\n",
       "  {'page': 346, 'page_label': '321', 'source': './AI_Book.pdf'},\n",
       "  {'page': 347, 'page_label': '322', 'source': './AI_Book.pdf'},\n",
       "  {'page': 348, 'page_label': '323', 'source': './AI_Book.pdf'},\n",
       "  {'page': 349, 'page_label': '324', 'source': './AI_Book.pdf'},\n",
       "  {'page': 350, 'page_label': '325', 'source': './AI_Book.pdf'},\n",
       "  {'page': 351, 'page_label': '326', 'source': './AI_Book.pdf'},\n",
       "  {'page': 352, 'page_label': '327', 'source': './AI_Book.pdf'},\n",
       "  {'page': 353, 'page_label': '328', 'source': './AI_Book.pdf'},\n",
       "  {'page': 354, 'page_label': '329', 'source': './AI_Book.pdf'},\n",
       "  {'page': 355, 'page_label': '330', 'source': './AI_Book.pdf'},\n",
       "  {'page': 356, 'page_label': '331', 'source': './AI_Book.pdf'},\n",
       "  {'page': 357, 'page_label': '332', 'source': './AI_Book.pdf'},\n",
       "  {'page': 358, 'page_label': '333', 'source': './AI_Book.pdf'},\n",
       "  {'page': 359, 'page_label': '334', 'source': './AI_Book.pdf'},\n",
       "  {'page': 360, 'page_label': '335', 'source': './AI_Book.pdf'},\n",
       "  {'page': 361, 'page_label': '336', 'source': './AI_Book.pdf'},\n",
       "  {'page': 362, 'page_label': '337', 'source': './AI_Book.pdf'},\n",
       "  {'page': 363, 'page_label': '338', 'source': './AI_Book.pdf'},\n",
       "  {'page': 364, 'page_label': '339', 'source': './AI_Book.pdf'},\n",
       "  {'page': 365, 'page_label': '340', 'source': './AI_Book.pdf'},\n",
       "  {'page': 366, 'page_label': '341', 'source': './AI_Book.pdf'},\n",
       "  {'page': 367, 'page_label': '342', 'source': './AI_Book.pdf'},\n",
       "  {'page': 368, 'page_label': '343', 'source': './AI_Book.pdf'},\n",
       "  {'page': 369, 'page_label': '344', 'source': './AI_Book.pdf'},\n",
       "  {'page': 370, 'page_label': '345', 'source': './AI_Book.pdf'},\n",
       "  {'page': 371, 'page_label': '346', 'source': './AI_Book.pdf'},\n",
       "  {'page': 372, 'page_label': '347', 'source': './AI_Book.pdf'},\n",
       "  {'page': 373, 'page_label': '348', 'source': './AI_Book.pdf'},\n",
       "  {'page': 374, 'page_label': '349', 'source': './AI_Book.pdf'},\n",
       "  {'page': 375, 'page_label': '350', 'source': './AI_Book.pdf'},\n",
       "  {'page': 376, 'page_label': '351', 'source': './AI_Book.pdf'},\n",
       "  {'page': 377, 'page_label': '352', 'source': './AI_Book.pdf'},\n",
       "  {'page': 378, 'page_label': '353', 'source': './AI_Book.pdf'},\n",
       "  {'page': 379, 'page_label': '354', 'source': './AI_Book.pdf'},\n",
       "  {'page': 380, 'page_label': '355', 'source': './AI_Book.pdf'},\n",
       "  {'page': 381, 'page_label': '356', 'source': './AI_Book.pdf'},\n",
       "  {'page': 382, 'page_label': '357', 'source': './AI_Book.pdf'},\n",
       "  {'page': 383, 'page_label': '358', 'source': './AI_Book.pdf'},\n",
       "  {'page': 384, 'page_label': '359', 'source': './AI_Book.pdf'},\n",
       "  {'page': 385, 'page_label': '360', 'source': './AI_Book.pdf'},\n",
       "  {'page': 386, 'page_label': '361', 'source': './AI_Book.pdf'},\n",
       "  {'page': 387, 'page_label': '362', 'source': './AI_Book.pdf'},\n",
       "  {'page': 388, 'page_label': '363', 'source': './AI_Book.pdf'},\n",
       "  {'page': 389, 'page_label': '364', 'source': './AI_Book.pdf'},\n",
       "  {'page': 390, 'page_label': '365', 'source': './AI_Book.pdf'},\n",
       "  {'page': 391, 'page_label': '366', 'source': './AI_Book.pdf'},\n",
       "  {'page': 392, 'page_label': '367', 'source': './AI_Book.pdf'},\n",
       "  {'page': 393, 'page_label': '368', 'source': './AI_Book.pdf'},\n",
       "  {'page': 394, 'page_label': '369', 'source': './AI_Book.pdf'},\n",
       "  {'page': 395, 'page_label': '370', 'source': './AI_Book.pdf'},\n",
       "  {'page': 396, 'page_label': '371', 'source': './AI_Book.pdf'},\n",
       "  {'page': 397, 'page_label': '372', 'source': './AI_Book.pdf'},\n",
       "  {'page': 398, 'page_label': '373', 'source': './AI_Book.pdf'},\n",
       "  {'page': 399, 'page_label': '374', 'source': './AI_Book.pdf'},\n",
       "  {'page': 400, 'page_label': '375', 'source': './AI_Book.pdf'},\n",
       "  {'page': 401, 'page_label': '376', 'source': './AI_Book.pdf'},\n",
       "  {'page': 402, 'page_label': '377', 'source': './AI_Book.pdf'},\n",
       "  {'page': 403, 'page_label': '378', 'source': './AI_Book.pdf'},\n",
       "  {'page': 404, 'page_label': '379', 'source': './AI_Book.pdf'},\n",
       "  {'page': 405, 'page_label': '380', 'source': './AI_Book.pdf'},\n",
       "  {'page': 406, 'page_label': '381', 'source': './AI_Book.pdf'},\n",
       "  {'page': 407, 'page_label': '382', 'source': './AI_Book.pdf'},\n",
       "  {'page': 408, 'page_label': '383', 'source': './AI_Book.pdf'},\n",
       "  {'page': 409, 'page_label': '384', 'source': './AI_Book.pdf'},\n",
       "  {'page': 410, 'page_label': '385', 'source': './AI_Book.pdf'},\n",
       "  {'page': 411, 'page_label': '386', 'source': './AI_Book.pdf'},\n",
       "  {'page': 412, 'page_label': '387', 'source': './AI_Book.pdf'},\n",
       "  {'page': 413, 'page_label': '388', 'source': './AI_Book.pdf'},\n",
       "  {'page': 414, 'page_label': '389', 'source': './AI_Book.pdf'},\n",
       "  {'page': 415, 'page_label': '390', 'source': './AI_Book.pdf'},\n",
       "  {'page': 416, 'page_label': '391', 'source': './AI_Book.pdf'},\n",
       "  {'page': 417, 'page_label': '392', 'source': './AI_Book.pdf'},\n",
       "  {'page': 418, 'page_label': '393', 'source': './AI_Book.pdf'},\n",
       "  {'page': 419, 'page_label': '394', 'source': './AI_Book.pdf'},\n",
       "  {'page': 420, 'page_label': '395', 'source': './AI_Book.pdf'},\n",
       "  {'page': 421, 'page_label': '396', 'source': './AI_Book.pdf'},\n",
       "  {'page': 422, 'page_label': '397', 'source': './AI_Book.pdf'},\n",
       "  {'page': 423, 'page_label': '398', 'source': './AI_Book.pdf'},\n",
       "  {'page': 424, 'page_label': '399', 'source': './AI_Book.pdf'},\n",
       "  {'page': 425, 'page_label': '400', 'source': './AI_Book.pdf'},\n",
       "  {'page': 426, 'page_label': '401', 'source': './AI_Book.pdf'},\n",
       "  {'page': 428, 'page_label': '403', 'source': './AI_Book.pdf'},\n",
       "  {'page': 429, 'page_label': '404', 'source': './AI_Book.pdf'},\n",
       "  {'page': 430, 'page_label': '405', 'source': './AI_Book.pdf'},\n",
       "  {'page': 431, 'page_label': '406', 'source': './AI_Book.pdf'},\n",
       "  {'page': 432, 'page_label': '407', 'source': './AI_Book.pdf'},\n",
       "  {'page': 433, 'page_label': '408', 'source': './AI_Book.pdf'},\n",
       "  {'page': 434, 'page_label': '409', 'source': './AI_Book.pdf'},\n",
       "  {'page': 435, 'page_label': '410', 'source': './AI_Book.pdf'},\n",
       "  {'page': 436, 'page_label': '411', 'source': './AI_Book.pdf'},\n",
       "  {'page': 437, 'page_label': '412', 'source': './AI_Book.pdf'},\n",
       "  {'page': 438, 'page_label': '413', 'source': './AI_Book.pdf'},\n",
       "  {'page': 439, 'page_label': '414', 'source': './AI_Book.pdf'},\n",
       "  {'page': 440, 'page_label': '415', 'source': './AI_Book.pdf'},\n",
       "  {'page': 441, 'page_label': '416', 'source': './AI_Book.pdf'},\n",
       "  {'page': 442, 'page_label': '417', 'source': './AI_Book.pdf'},\n",
       "  {'page': 443, 'page_label': '418', 'source': './AI_Book.pdf'},\n",
       "  {'page': 444, 'page_label': '419', 'source': './AI_Book.pdf'},\n",
       "  {'page': 445, 'page_label': '420', 'source': './AI_Book.pdf'},\n",
       "  {'page': 446, 'page_label': '421', 'source': './AI_Book.pdf'},\n",
       "  {'page': 447, 'page_label': '422', 'source': './AI_Book.pdf'},\n",
       "  {'page': 448, 'page_label': '423', 'source': './AI_Book.pdf'},\n",
       "  {'page': 449, 'page_label': '424', 'source': './AI_Book.pdf'},\n",
       "  {'page': 450, 'page_label': '425', 'source': './AI_Book.pdf'},\n",
       "  {'page': 451, 'page_label': '426', 'source': './AI_Book.pdf'},\n",
       "  {'page': 452, 'page_label': '427', 'source': './AI_Book.pdf'},\n",
       "  {'page': 453, 'page_label': '428', 'source': './AI_Book.pdf'},\n",
       "  {'page': 454, 'page_label': '429', 'source': './AI_Book.pdf'},\n",
       "  {'page': 455, 'page_label': '430', 'source': './AI_Book.pdf'},\n",
       "  {'page': 456, 'page_label': '431', 'source': './AI_Book.pdf'},\n",
       "  {'page': 457, 'page_label': '432', 'source': './AI_Book.pdf'},\n",
       "  {'page': 458, 'page_label': '433', 'source': './AI_Book.pdf'},\n",
       "  {'page': 459, 'page_label': '434', 'source': './AI_Book.pdf'},\n",
       "  {'page': 460, 'page_label': '435', 'source': './AI_Book.pdf'},\n",
       "  {'page': 461, 'page_label': '436', 'source': './AI_Book.pdf'},\n",
       "  {'page': 462, 'page_label': '437', 'source': './AI_Book.pdf'},\n",
       "  {'page': 463, 'page_label': '438', 'source': './AI_Book.pdf'},\n",
       "  {'page': 464, 'page_label': '439', 'source': './AI_Book.pdf'},\n",
       "  {'page': 465, 'page_label': '440', 'source': './AI_Book.pdf'},\n",
       "  {'page': 466, 'page_label': '441', 'source': './AI_Book.pdf'},\n",
       "  {'page': 467, 'page_label': '442', 'source': './AI_Book.pdf'},\n",
       "  {'page': 468, 'page_label': '443', 'source': './AI_Book.pdf'},\n",
       "  {'page': 469, 'page_label': '444', 'source': './AI_Book.pdf'},\n",
       "  {'page': 470, 'page_label': '445', 'source': './AI_Book.pdf'},\n",
       "  {'page': 471, 'page_label': '446', 'source': './AI_Book.pdf'},\n",
       "  {'page': 472, 'page_label': '447', 'source': './AI_Book.pdf'},\n",
       "  {'page': 473, 'page_label': '448', 'source': './AI_Book.pdf'},\n",
       "  {'page': 474, 'page_label': '449', 'source': './AI_Book.pdf'},\n",
       "  {'page': 475, 'page_label': '450', 'source': './AI_Book.pdf'},\n",
       "  {'page': 476, 'page_label': '451', 'source': './AI_Book.pdf'},\n",
       "  {'page': 477, 'page_label': '452', 'source': './AI_Book.pdf'},\n",
       "  {'page': 478, 'page_label': '453', 'source': './AI_Book.pdf'},\n",
       "  {'page': 479, 'page_label': '454', 'source': './AI_Book.pdf'},\n",
       "  {'page': 480, 'page_label': '455', 'source': './AI_Book.pdf'},\n",
       "  {'page': 481, 'page_label': '456', 'source': './AI_Book.pdf'},\n",
       "  {'page': 482, 'page_label': '457', 'source': './AI_Book.pdf'},\n",
       "  {'page': 483, 'page_label': '458', 'source': './AI_Book.pdf'},\n",
       "  {'page': 484, 'page_label': '459', 'source': './AI_Book.pdf'},\n",
       "  {'page': 485, 'page_label': '460', 'source': './AI_Book.pdf'},\n",
       "  {'page': 486, 'page_label': '461', 'source': './AI_Book.pdf'},\n",
       "  {'page': 487, 'page_label': '462', 'source': './AI_Book.pdf'},\n",
       "  {'page': 488, 'page_label': '463', 'source': './AI_Book.pdf'},\n",
       "  {'page': 489, 'page_label': '464', 'source': './AI_Book.pdf'},\n",
       "  {'page': 490, 'page_label': '465', 'source': './AI_Book.pdf'},\n",
       "  {'page': 491, 'page_label': '466', 'source': './AI_Book.pdf'},\n",
       "  {'page': 492, 'page_label': '467', 'source': './AI_Book.pdf'},\n",
       "  {'page': 493, 'page_label': '468', 'source': './AI_Book.pdf'},\n",
       "  {'page': 494, 'page_label': '469', 'source': './AI_Book.pdf'},\n",
       "  {'page': 495, 'page_label': '470', 'source': './AI_Book.pdf'},\n",
       "  {'page': 496, 'page_label': '471', 'source': './AI_Book.pdf'},\n",
       "  {'page': 497, 'page_label': '472', 'source': './AI_Book.pdf'},\n",
       "  {'page': 498, 'page_label': '473', 'source': './AI_Book.pdf'},\n",
       "  {'page': 499, 'page_label': '474', 'source': './AI_Book.pdf'},\n",
       "  {'page': 500, 'page_label': '475', 'source': './AI_Book.pdf'},\n",
       "  {'page': 501, 'page_label': '476', 'source': './AI_Book.pdf'},\n",
       "  {'page': 502, 'page_label': '477', 'source': './AI_Book.pdf'},\n",
       "  {'page': 503, 'page_label': '478', 'source': './AI_Book.pdf'},\n",
       "  {'page': 504, 'page_label': '479', 'source': './AI_Book.pdf'},\n",
       "  {'page': 505, 'page_label': '480', 'source': './AI_Book.pdf'},\n",
       "  {'page': 506, 'page_label': '481', 'source': './AI_Book.pdf'},\n",
       "  {'page': 507, 'page_label': '482', 'source': './AI_Book.pdf'},\n",
       "  {'page': 508, 'page_label': '483', 'source': './AI_Book.pdf'},\n",
       "  {'page': 509, 'page_label': '484', 'source': './AI_Book.pdf'}],\n",
       " 'documents': ['Aurlien Gron\\nHands-on Machine Learning with\\nScikit-Learn, Keras, and\\nTensorFlow\\nConcepts, Tools, and Techniques to\\nBuild Intelligent Systems\\nSECOND EDITION\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing',\n",
       "  '978-1-492-03264-9\\n[LSI]\\nHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurlien Gron\\nCopyright  2019 Aurlien Gron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by OReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nOReilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Nicole Tache\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.\\nThe OReilly logo is a registered trademark of OReilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of OReilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.',\n",
       "  'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1. The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii',\n",
       "  'Frame the Problem                                                                                                    39\\nSelect a Performance Measure                                                                                  42\\nCheck the Assumptions                                                                                             45\\nGet the Data                                                                                                                    45\\nCreate the Workspace                                                                                                45\\nDownload the Data                                                                                                    49\\nTake a Quick Look at the Data Structure                                                                50\\nCreate a Test Set                                                                                                          54\\nDiscover and Visualize the Data to Gain Insights                                                     58\\nVisualizing Geographical Data                                                                                 59\\nLooking for Correlations                                                                                           62\\nExperimenting with Attribute Combinations                                                        65\\nPrepare the Data for Machine Learning Algorithms                                                66\\nData Cleaning                                                                                                             67\\nHandling Text and Categorical Attributes                                                              69\\nCustom Transformers                                                                                                71\\nFeature Scaling                                                                                                            72\\nTransformation Pipelines                                                                                          73\\nSelect and Train a Model                                                                                               75\\nTraining and Evaluating on the Training Set                                                         75\\nBetter Evaluation Using Cross-Validation                                                              76\\nFine-Tune Y our Model                                                                                                  79\\nGrid Search                                                                                                                 79\\nRandomized Search                                                                                                   81\\nEnsemble Methods                                                                                                     82\\nAnalyze the Best Models and Their Errors                                                             82\\nEvaluate Y our System on the Test Set                                                                      83\\nLaunch, Monitor, and Maintain Y our System                                                            84\\nTry It Out!                                                                                                                       85\\nExercises                                                                                                                          85\\n3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\\nMNIST                                                                                                                             87\\nTraining a Binary Classifier                                                                                          90\\nPerformance Measures                                                                                                  90',\n",
       "  'Performance Measures                                                                                                  90\\nMeasuring Accuracy Using Cross-Validation                                                        91\\nConfusion Matrix                                                                                                       92\\nPrecision and Recall                                                                                                   94\\nPrecision/Recall Tradeoff                                                                                          95\\nThe ROC Curve                                                                                                          99\\nMulticlass Classification                                                                                             102\\nError Analysis                                                                                                              104\\niv | Table of Contents',\n",
       "  'Multilabel Classification                                                                                             108\\nMultioutput Classification                                                                                          109\\nExercises                                                                                                                        110\\n4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\\nLinear Regression                                                                                                        114\\nThe Normal Equation                                                                                              116\\nComputational Complexity                                                                                    119\\nGradient Descent                                                                                                         119\\nBatch Gradient Descent                                                                                           123\\nStochastic Gradient Descent                                                                                   126\\nMini-batch Gradient Descent                                                                                 129\\nPolynomial Regression                                                                                                130\\nLearning Curves                                                                                                           132\\nRegularized Linear Models                                                                                         136\\nRidge Regression                                                                                                      137\\nLasso Regression                                                                                                      139\\nElastic Net                                                                                                                 142\\nEarly Stopping                                                                                                          142\\nLogistic Regression                                                                                                      144\\nEstimating Probabilities                                                                                          144\\nTraining and Cost Function                                                                                   145\\nDecision Boundaries                                                                                                146\\nSoftmax Regression                                                                                                  149\\nExercises                                                                                                                        153\\n5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nLinear SVM Classification                                                                                          155\\nSoft Margin Classification                                                                                       156\\nNonlinear SVM Classification                                                                                   159\\nPolynomial Kernel                                                                                                   160\\nAdding Similarity Features                                                                                     161\\nGaussian RBF Kernel                                                                                               162\\nComputational Complexity                                                                                    163\\nSVM Regression                                                                                                           164',\n",
       "  'SVM Regression                                                                                                           164\\nUnder the Hood                                                                                                           166\\nDecision Function and Predictions                                                                       166\\nTraining Objective                                                                                                   167\\nQuadratic Programming                                                                                         169\\nThe Dual Problem                                                                                                    170\\nKernelized SVM                                                                                                       171\\nOnline SVMs                                                                                                            174\\nTable of Contents | v',\n",
       "  'Exercises                                                                                                                        175\\n6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  177\\nTraining and Visualizing a Decision Tree                                                                177\\nMaking Predictions                                                                                                     179\\nEstimating Class Probabilities                                                                                   181\\nThe CART Training Algorithm                                                                                 182\\nComputational Complexity                                                                                        183\\nGini Impurity or Entropy?                                                                                         183\\nRegularization Hyperparameters                                                                              184\\nRegression                                                                                                                     185\\nInstability                                                                                                                      188\\nExercises                                                                                                                        189\\n7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\\nVoting Classifiers                                                                                                         192\\nBagging and Pasting                                                                                                    195\\nBagging and Pasting in Scikit-Learn                                                                     196\\nOut-of-Bag Evaluation                                                                                            197\\nRandom Patches and Random Subspaces                                                                198\\nRandom Forests                                                                                                           199\\nExtra-Trees                                                                                                                200\\nFeature Importance                                                                                                  200\\nBoosting                                                                                                                        201\\nAdaBoost                                                                                                                   202\\nGradient Boosting                                                                                                    205\\nStacking                                                                                                                         210\\nExercises                                                                                                                        213\\n8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  215\\nThe Curse of Dimensionality                                                                                     216\\nMain Approaches for Dimensionality Reduction                                                   218\\nProjection                                                                                                                  218\\nManifold Learning                                                                                                   220\\nPCA                                                                                                                                222',\n",
       "  'PCA                                                                                                                                222\\nPreserving the Variance                                                                                          222\\nPrincipal Components                                                                                            223\\nProjecting Down to d Dimensions                                                                        224\\nUsing Scikit-Learn                                                                                                    224\\nExplained Variance Ratio                                                                                        225\\nChoosing the Right Number of Dimensions                                                       225\\nPCA for Compression                                                                                             226\\nvi | Table of Contents',\n",
       "  'Randomized PCA                                                                                                    227\\nIncremental PCA                                                                                                     227\\nKernel PCA                                                                                                                   228\\nSelecting a Kernel and Tuning Hyperparameters                                                229\\nLLE                                                                                                                                 232\\nOther Dimensionality Reduction Techniques                                                         234\\nExercises                                                                                                                        235\\n9. Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  237\\nClustering                                                                                                                      238\\nK-Means                                                                                                                    240\\nLimits of K-Means                                                                                                   250\\nUsing clustering for image segmentation                                                             251\\nUsing Clustering for Preprocessing                                                                       252\\nUsing Clustering for Semi-Supervised Learning                                                 254\\nDBSCAN                                                                                                                   256\\nOther Clustering Algorithms                                                                                 259\\nGaussian Mixtures                                                                                                       260\\nAnomaly Detection using Gaussian Mixtures                                                     266\\nSelecting the Number of Clusters                                                                          267\\nBayesian Gaussian Mixture Models                                                                      270\\nOther Anomaly Detection and Novelty Detection Algorithms                        274\\nPart II. Neural Networks and Deep Learning\\n10. Introduction to Artificial Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .  277\\nFrom Biological to Artificial Neurons                                                                      278\\nBiological Neurons                                                                                                   279\\nLogical Computations with Neurons                                                                    281\\nThe Perceptron                                                                                                         281\\nMulti-Layer Perceptron and Backpropagation                                                    286\\nRegression MLPs                                                                                                      289\\nClassification MLPs                                                                                                 290\\nImplementing MLPs with Keras                                                                                292\\nInstalling TensorFlow 2                                                                                           293\\nBuilding an Image Classifier Using the Sequential API                                     294\\nBuilding a Regression MLP Using the Sequential API                                       303\\nBuilding Complex Models Using the Functional API                                        304\\nBuilding Dynamic Models Using the Subclassing API                                       309',\n",
       "  'Building Complex Models Using the Functional API                                        304\\nBuilding Dynamic Models Using the Subclassing API                                       309\\nSaving and Restoring a Model                                                                                311\\nUsing Callbacks                                                                                                        311\\nTable of Contents | vii',\n",
       "  'Visualization Using TensorBoard                                                                          313\\nFine-Tuning Neural Network Hyperparameters                                                     315\\nNumber of Hidden Layers                                                                                      319\\nNumber of Neurons per Hidden Layer                                                                 320\\nLearning Rate, Batch Size and Other Hyperparameters                                     320\\nExercises                                                                                                                        322\\n11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  325\\nVanishing/Exploding Gradients Problems                                                              326\\nGlorot and He Initialization                                                                                   327\\nNonsaturating Activation Functions                                                                     329\\nBatch Normalization                                                                                                333\\nGradient Clipping                                                                                                    338\\nReusing Pretrained Layers                                                                                          339\\nTransfer Learning With Keras                                                                                341\\nUnsupervised Pretraining                                                                                       343\\nPretraining on an Auxiliary Task                                                                           344\\nFaster Optimizers                                                                                                         344\\nMomentum Optimization                                                                                      345\\nNesterov Accelerated Gradient                                                                              346\\nAdaGrad                                                                                                                    347\\nRMSProp                                                                                                                   349\\nAdam and Nadam Optimization                                                                           349\\nLearning Rate Scheduling                                                                                       352\\nAvoiding Overfitting Through Regularization                                                        356\\n1 and 2 Regularization                                                                                           356\\nDropout                                                                                                                     357\\nMonte-Carlo (MC) Dropout                                                                                  360\\nMax-Norm Regularization                                                                                      362\\nSummary and Practical Guidelines                                                                           363\\nExercises                                                                                                                        364\\n12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\\nA Quick Tour of TensorFlow                                                                                     368\\nUsing TensorFlow like NumPy                                                                                  371\\nTensors and Operations                                                                                          371',\n",
       "  'Tensors and Operations                                                                                          371\\nTensors and NumPy                                                                                                373\\nType Conversions                                                                                                     374\\nVariables                                                                                                                    374\\nOther Data Structures                                                                                             375\\nCustomizing Models and Training Algorithms                                                      376\\nCustom Loss Functions                                                                                           376\\nviii | Table of Contents',\n",
       "  'Saving and Loading Models That Contain Custom Components                    377\\nCustom Activation Functions, Initializers, Regularizers, and Constraints     379\\nCustom Metrics                                                                                                        380\\nCustom Layers                                                                                                          383\\nCustom Models                                                                                                        386\\nLosses and Metrics Based on Model Internals                                                     388\\nComputing Gradients Using Autodiff                                                                   389\\nCustom Training Loops                                                                                          393\\nTensorFlow Functions and Graphs                                                                           396\\nAutograph and Tracing                                                                                           398\\nTF Function Rules                                                                                                    400\\n13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\\nThe Data API                                                                                                                404\\nChaining Transformations                                                                                      405\\nShuffling the Data                                                                                                    406\\nPreprocessing the Data                                                                                            409\\nPutting Everything Together                                                                                  410\\nPrefetching                                                                                                                411\\nUsing the Dataset With tf.keras                                                                              413\\nThe TFRecord Format                                                                                                414\\nCompressed TFRecord Files                                                                                   415\\nA Brief Introduction to Protocol Buffers                                                              415\\nTensorFlow Protobufs                                                                                             416\\nLoading and Parsing Examples                                                                              418\\nHandling Lists of Lists Using the SequenceExample Protobuf                          419\\nThe Features API                                                                                                         420\\nCategorical Features                                                                                                 421\\nCrossed Categorical Features                                                                                 421\\nEncoding Categorical Features Using One-Hot Vectors                                    422\\nEncoding Categorical Features Using Embeddings                                            423\\nUsing Feature Columns for Parsing                                                                      426\\nUsing Feature Columns in Y our Models                                                               426\\nTF Transform                                                                                                               428\\nThe TensorFlow Datasets (TFDS) Project                                                                429\\n14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .  431\\nThe Architecture of the Visual Cortex                                                                     432',\n",
       "  'The Architecture of the Visual Cortex                                                                     432\\nConvolutional Layer                                                                                                    434\\nFilters                                                                                                                         436\\nStacking Multiple Feature Maps                                                                             437\\nTensorFlow Implementation                                                                                  439\\nTable of Contents | ix',\n",
       "  'Memory Requirements                                                                                           441\\nPooling Layer                                                                                                                442\\nTensorFlow Implementation                                                                                  444\\nCNN Architectures                                                                                                      446\\nLeNet-5                                                                                                                      449\\nAlexNet                                                                                                                      450\\nGoogLeNet                                                                                                                452\\nVGGNet                                                                                                                     456\\nResNet                                                                                                                        457\\nXception                                                                                                                    459\\nSENet                                                                                                                         461\\nImplementing a ResNet-34 CNN Using Keras                                                        464\\nUsing Pretrained Models From Keras                                                                      465\\nPretrained Models for Transfer Learning                                                                 467\\nClassification and Localization                                                                                  469\\nObject Detection                                                                                                          471\\nFully Convolutional Networks (FCNs)                                                                 473\\nY ou Only Look Once (YOLO)                                                                                475\\nSemantic Segmentation                                                                                               478\\nExercises                                                                                                                        482\\nx | Table of Contents',\n",
       "  '1 Available on Hintons home page at http://www.cs.toronto.edu/~hinton/.\\n2 Despite the fact that Y ann Lecuns deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.\\nPreface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique Deep Learning.  Training a deep neural net\\nwas widely considered impossible at the time, 2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in todays high-tech products, ranking your web\\nsearch results, powering your smartphones speech recognition, recommending vid\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec\\nognize faces? Or learn to walk around?\\nxi',\n",
       "  'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n Segment customers and find the best marketing strategy for each group\\n Recommend products for each client based on what similar clients bought\\n Detect which transactions are likely to be fraudulent\\n Forecast next years revenue\\n And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple\\nment programs capable of learning from data.\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n Scikit-Learn is very easy to use, yet it implements many Machine Learning algo\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n TensorFlow is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n Keras is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras, which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface',\n",
       "  'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2.\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Pythons main scientific libraries, in particular NumPy, Pandas, and\\nMatplotlib.\\nAlso, if you care about whats under the hood you should have a reasonable under\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta\\ntistics).\\nIf you dont know Python yet, http://learnpython.org/ is a great place to start. The offi\\ncial tutorial on python.org is also quite good.\\nIf you have never used Jupyter, Chapter 2 will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Pythons scientific libraries, the provided Jupyter note\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n The main steps in a typical Machine Learning project.\\n Learning by fitting a model to data.\\n Optimizing a cost function.\\n Handling, cleaning, and preparing data.\\n Selecting and engineering features.\\n Selecting a model and tuning hyperparameters using cross-validation.\\n The main challenges of Machine Learning, in particular underfitting and overfit\\nting (the bias/variance tradeoff).\\n Reducing the dimensionality of the training data to fight the curse of dimension\\nality.\\n Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii',\n",
       "  ' The most common learning algorithms: Linear and Polynomial Regression,\\nLogistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\\nTrees, Random Forests, and Ensemble methods.\\nxiv | Preface',\n",
       "  'Part II, Neural Networks and Deep Learning, covers the following topics:\\n What are neural nets? What are they good for?\\n Building and training neural nets using TensorFlow and Keras.\\n The most important neural net architectures: feedforward neural nets, convolu\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n Techniques for training deep neural nets.\\n Scaling neural networks for large datasets.\\n Learning strategies with Reinforcement Learning.\\n Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDont jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I). Deep Learn\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ngs ML\\ncourse on Coursera  and Geoffrey Hintons course on neural networks and Deep\\nLearning are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learns exceptional User Guide. Y ou may also enjoy Dataquest, which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora.\\nFinally, the Deep Learning website has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n Joel Grus, Data Science from Scratch  (OReilly). This book presents the funda\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv',\n",
       "  ' Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra\\nries (Pylearn 2 and Theano).\\n Franois Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math\\nematical theory.\\n Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4).\\n Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 3rd\\nEdition (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter\\nmined by context.\\nxvi | Preface',\n",
       "  'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2. It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin\\nguish the code from the outputs. For example, this code defines the square() func\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii',\n",
       "  'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless youre reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom OReilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi\\ncant amount of example code from this book into your products documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example:  Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurlien Gron (OReilly). Copyright 2019\\nAurlien Gron, 978-1-492-03264-9.  If you feel your use of code examples falls out\\nside fair use or the permission given above, feel free to contact us at permis\\nsions@oreilly.com.\\nOReilly Safari\\nSafari (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac\\ntive tutorials, and curated playlists from over 250 publishers, including OReilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nOReilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface',\n",
       "  '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow or https://homl.info/oreilly.\\nTo comment or ask technical questions about this book, send email to bookques\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1. Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten\\nsorFlow models, and more.\\n2. Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3. Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlows imple\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4. Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan\\ndas, Matplotlib and other libraries.\\n5. Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix',\n",
       "  'Table P-1. Chapter mapping between 1st and 2nd edition\\n1st Ed. chapter 2nd Ed. Chapter % Changes 2nd Ed. Title\\n1 1 <10% The Machine Learning Landscape\\n2 2 <10% End-to-End Machine Learning Project\\n3 3 <10% Classification\\n4 4 <10% Training Models\\n5 5 <10% Support Vector Machines\\n6 6 <10% Decision Trees\\n7 7 <10% Ensemble Learning and Random Forests\\n8 8 <10% Dimensionality Reduction\\nN/A 9 100% new Unsupervised Learning Techniques\\n10 10 ~75% Introduction to Artificial Neural Networks with Keras\\n11 11 ~50% Training Deep Neural Networks\\n9 12 100% rewritten Custom Models and Training with TensorFlow\\nPart of 12 13 100% rewritten Loading and Preprocessing Data with TensorFlow\\n13 14 ~50% Deep Computer Vision Using Convolutional Neural Networks\\nPart of 14 15 ~75% Processing Sequences Using RNNs and CNNs\\nPart of 14 16 ~90% Natural Language Processing with RNNs and Attention\\n15 17 ~75% Autoencoders and GANs\\n16 18 ~75% Reinforcement Learning\\nPart of 12 19 100% rewritten Deploying your TensorFlow Models\\nMore specifically, here are the main changes for each 2 nd edition chapter (other than\\nclarifications, corrections and code updates):\\n Chapter 1\\n Added a section on handling mismatch between the training set and the vali\\ndation & test sets.\\n Chapter 2\\n Added how to compute a confidence interval.\\n Improved the installation instructions (e.g., for Windows).\\n Introduced the upgraded OneHotEncoder and the new ColumnTransformer.\\n Chapter 4\\n Explained the need for training instances to be Independent and Identically\\nDistributed (IID).\\n Chapter 7\\n Added a short section about XGBoost.\\nxx | Preface',\n",
       "  ' Chapter 9  new chapter including:\\n Clustering with K-Means, how to choose the number of clusters, how to use it\\nfor dimensionality reduction, semi-supervised learning, image segmentation,\\nand more.\\n The DBSCAN clustering algorithm and an overview of other clustering algo\\nrithms available in Scikit-Learn.\\n Gaussian mixture models, the Expectation-Maximization (EM) algorithm,\\nBayesian variational inference, and how mixture models can be used for clus\\ntering, density estimation, anomaly detection and novelty detection.\\n Overview of other anomaly detection and novelty detection algorithms.\\n Chapter 10 (mostly new)\\n Added an introduction to the Keras API, including all its APIs (Sequential,\\nFunctional and Subclassing), persistence and callbacks (including the Tensor\\nBoard callback).\\n Chapter 11 (many changes)\\n Introduced self-normalizing nets, the SELU activation function and Alpha\\nDropout.\\n Introduced self-supervised learning.\\n Added Nadam optimization.\\n Added Monte-Carlo Dropout.\\n Added a note about the risks of adaptive optimization methods.\\n Updated the practical guidelines.\\n Chapter 12  completely rewritten chapter, including:\\n A tour of TensorFlow 2\\n TensorFlows lower-level Python API\\n Writing custom loss functions, metrics, layers, models\\n Using auto-differentiation and creating custom training algorithms.\\n TensorFlow Functions and graphs (including tracing and autograph).\\n Chapter 13  new chapter, including:\\n The Data API\\n Loading/Storing data efficiently using TFRecords\\n The Features API (including an introduction to embeddings).\\n An overview of TF Transform and TF Datasets\\n Moved the low-level implementation of the neural network to the exercises.\\nPreface | xxi',\n",
       "  ' Removed details about queues and readers that are now superseded by the\\nData API.\\n Chapter 14\\n Added Xception and SENet architectures.\\n Added a Keras implementation of ResNet-34.\\n Showed how to use pretrained models using Keras.\\n Added an end-to-end transfer learning example.\\n Added classification and localization.\\n Introduced Fully Convolutional Networks (FCNs).\\n Introduced object detection using the YOLO architecture.\\n Introduced semantic segmentation using R-CNN.\\n Chapter 15\\n Added an introduction to Wavenet.\\n Moved the EncoderDecoder architecture and Bidirectional RNNs to Chapter\\n16.\\n Chapter 16\\n Explained how to use the Data API to handle sequential data.\\n Showed an end-to-end example of text generation using a Character RNN,\\nusing both a stateless and a stateful RNN.\\n Showed an end-to-end example of sentiment analysis using an LSTM.\\n Explained masking in Keras.\\n Showed how to reuse pretrained embeddings using TF Hub.\\n Showed how to build an EncoderDecoder for Neural Machine Translation\\nusing TensorFlow Addons/seq2seq.\\n Introduced beam search.\\n Explained attention mechanisms.\\n Added a short overview of visual attention and a note on explainability.\\n Introduced the fully attention-based Transformer architecture, including posi\\ntional embeddings and multi-head attention.\\n Added an overview of recent language models (2018).\\n Chapters 17, 18 and 19: coming soon.\\nxxii | Preface',\n",
       "  '3 Deep Learning with Python,  Franois Chollet (2017).\\nAcknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github if you find\\nerrors in the code examples (or just to ask questions), or to submit errata if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn) or publicly\\n(e.g., in an Amazon review).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran\\nois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2 nd\\nedition, having its author review the book was invaluable. I highly recommend Fran\\noiss excellent book Deep Learning with Python 3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, Andr\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1 st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom OMalley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev\\neral errors while he was writing the Korean translation of the 1 st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlows\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii',\n",
       "  'Many thanks as well to OReillys fantastic staff, in particular Nicole Tache, who gave\\nme insightful feedback, always cheerful, encouraging, and helpful: I could not dream\\nof a better editor. Big thanks to Michele Cronin as well, who was very helpful (and\\npatient) at the start of this 2nd edition. Thanks to Marie Beaugureau, Ben Lorica, Mike\\nLoukides, and Laurel Ruma for believing in this project and helping me define its\\nscope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical\\nquestions regarding formatting, asciidoc, and LaTeX, and thanks to Rachel Mona\\nghan, Nick Adams, and all of the production team for their final review and their\\nhundreds of corrections.\\nI would also like to thank my former Google colleagues, in particular the Y ouTube\\nvideo classification team, for teaching me so much about Machine Learning. I could\\nnever have started the first edition without them. Special thanks to my personal ML\\ngurus: Clment Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James\\nPack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn,\\nRich Washington, and everyone I worked with at Y ouTube and in the amazing Goo\\ngle research teams in Mountain View. All these people are just as nice and helpful as\\nthey are bright, and thats saying a lot.\\nI will never forget the kind people who reviewed the 1st edition of this book, including\\nDavid Andrzejewski, Eddy Hung, Grgoire Mesnil, Iain Smears, Ingrid von Glehn,\\nJustin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim Smaoune, Vin\\ncent Guilbeau and of course my dear brother Sylvain.\\nLast but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\\nthree wonderful children, Alexandre, Rmi, and Gabrielle, for encouraging me to\\nwork hard on this book, as well as for their insatiable curiosity: explaining some of\\nthe most difficult concepts in this book to my wife and children helped me clarify my\\nthoughts and directly improved many parts of this book. Plus, they keep bringing me\\ncookies and coffee! What more can one dream of?\\nxxiv | Preface',\n",
       "  'PART I\\nThe Fundamentals of\\nMachine Learning',\n",
       "  'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear Machine Learning,  they picture a robot: a dependable but\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, its already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter.\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any\\nmore). It was followed by hundreds of ML applications that now quietly power hun\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn something? If I download a copy of Wikipedia, has my\\ncomputer really learned something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3',\n",
       "  'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and lets\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2. If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data.\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\nArthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\nTom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called ham) emails. The examples that the system uses to learn are\\ncalled the training set. Each training example is called a training instance (or sample).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata, and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni\\nques (Figure 1-1):\\n4 | Chapter 1: The Machine Learning Landscape',\n",
       "  '1. First you would look at what spam typically looks like. Y ou might notice that\\nsome words or phrases (such as 4U,  credit card,  free,  and amazing) tend to\\ncome up a lot in the subject. Perhaps you would also notice a few other patterns\\nin the senders name, the emails body, and so on.\\n2. Y ou would write a detection algorithm for each of the patterns that you noticed,\\nand your program would flag emails as spam if a number of these patterns are\\ndetected.\\n3. Y ou would test your program, and repeat steps 1 and 2 until it is good enough.\\nFigure 1-1. The traditional approach\\nSince the problem is not trivial, your program will likely become a long list of com\\nplex rulespretty hard to maintain.\\nIn contrast, a spam filter based on Machine Learning techniques automatically learns\\nwhich words and phrases are good predictors of spam by detecting unusually fre\\nquent patterns of words in the spam examples compared to the ham examples\\n(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\\naccurate.\\nWhy Use Machine Learning? | 5',\n",
       "  'Figure 1-2. Machine Learning approach\\nMoreover, if spammers notice that all their emails containing 4U are blocked, they\\nmight start writing For U instead. A spam filter using traditional programming\\ntechniques would need to be updated to flag For U emails. If spammers keep work\\ning around your spam filter, you will need to keep writing new rules forever.\\nIn contrast, a spam filter based on Machine Learning techniques automatically noti\\nces that For U has become unusually frequent in spam flagged by users, and it starts\\nflagging them without your intervention (Figure 1-3).\\nFigure 1-3. Automatically adapting to change\\nAnother area where Machine Learning shines is for problems that either are too com\\nplex for traditional approaches or have no known algorithm. For example, consider \\nspeech recognition: say you want to start simple and write a program capable of dis\\ntinguishing the words one and two.  Y ou might notice that the word two starts\\nwith a high-pitch sound (T), so you could hardcode an algorithm that measures\\nhigh-pitch sound intensity and use that to distinguish ones and twos. Obviously this\\ntechnique will not scale to thousands of words spoken by millions of very different\\n6 | Chapter 1: The Machine Learning Landscape',\n",
       "  'people in noisy environments and in dozens of languages. The best solution (at least\\ntoday) is to write an algorithm that learns by itself, given many example recordings\\nfor each word.\\nFinally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be\\ninspected to see what they have learned (although for some algorithms this can be\\ntricky). For instance, once the spam filter has been trained on enough spam, it can\\neasily be inspected to reveal the list of words and combinations of words that it\\nbelieves are the best predictors of spam. Sometimes this will reveal unsuspected cor\\nrelations or new trends, and thereby lead to a better understanding of the problem.\\nApplying ML techniques to dig into large amounts of data can help discover patterns\\nthat were not immediately apparent. This is called data mining.\\nFigure 1-4. Machine Learning can help humans learn\\nTo summarize, Machine Learning is great for:\\n Problems for which existing solutions require a lot of hand-tuning or long lists of\\nrules: one Machine Learning algorithm can often simplify code and perform bet\\nter.\\n Complex problems for which there is no good solution at all using a traditional\\napproach: the best Machine Learning techniques can find a solution.\\n Fluctuating environments: a Machine Learning system can adapt to new data.\\n Getting insights about complex problems and large amounts of data.\\nWhy Use Machine Learning? | 7',\n",
       "  'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n Whether or not they are trained with human supervision (supervised, unsuper\\nvised, semisupervised, and Reinforcement Learning)\\n Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLets look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn\\ning.\\nSupervised learning\\nIn supervised learning, the training data you feed to the algorithm includes the desired\\nsolutions, called labels (Figure 1-5).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape',\n",
       "  '1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\\nfact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\\nthis regression to the mean. This name was then applied to the methods he used to analyze correlations\\nbetween variables.\\nA typical supervised learning task is classification. The spam filter is a good example\\nof this: it is trained with many example emails along with their class (spam or ham),\\nand it must learn how to classify new emails.\\nAnother typical task is to predict a target numeric value, such as the price of a car,\\ngiven a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \\ncalled regression (Figure 1-6).1 To train the system, you need to give it many examples\\nof cars, including both their predictors and their labels (i.e., their prices).\\nIn Machine Learning an attribute is a data type (e.g., Mileage),\\nwhile a feature has several meanings depending on the context, but\\ngenerally means an attribute plus its value (e.g., Mileage =\\n15,000). Many people use the words attribute and feature inter\\nchangeably, though.\\nFigure 1-6. Regression\\nNote that some regression algorithms can be used for classification as well, and vice\\nversa. For example, Logistic Regression is commonly used for classification, as it can\\noutput a value that corresponds to the probability of belonging to a given class (e.g.,\\n20% chance of being spam).\\nTypes of Machine Learning Systems | 9',\n",
       "  '2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\\nmachines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.\\nHere are some of the most important supervised learning algorithms (covered in this\\nbook):\\n k-Nearest Neighbors\\n Linear Regression\\n Logistic Regression\\n Support Vector Machines (SVMs)\\n Decision Trees and Random Forests\\n Neural networks2\\nUnsupervised learning\\nIn unsupervised learning , as you might guess, the training data is unlabeled\\n(Figure 1-7). The system tries to learn without a teacher.\\nFigure 1-7. An unlabeled training set for unsupervised learning\\nHere are some of the most important unsupervised learning algorithms (most of\\nthese are covered in Chapter 8 and Chapter 9):\\n Clustering\\n K-Means\\n DBSCAN\\n Hierarchical Cluster Analysis (HCA)\\n Anomaly detection and novelty detection\\n One-class SVM\\n Isolation Forest\\n10 | Chapter 1: The Machine Learning Landscape',\n",
       "  ' Visualization and dimensionality reduction\\n Principal Component Analysis (PCA)\\n Kernel PCA\\n Locally-Linear Embedding (LLE)\\n t-distributed Stochastic Neighbor Embedding (t-SNE)\\n Association rule learning\\n Apriori\\n Eclat\\nFor example, say you have a lot of data about your blogs visitors. Y ou may want to\\nrun a clustering algorithm to try to detect groups of similar visitors ( Figure 1-8). At\\nno point do you tell the algorithm which group a visitor belongs to: it finds those\\nconnections without your help. For example, it might notice that 40% of your visitors\\nare males who love comic books and generally read your blog in the evening, while\\n20% are young sci-fi lovers who visit during the weekends, and so on. If you use a\\nhierarchical clustering  algorithm, it may also subdivide each group into smaller\\ngroups. This may help you target your posts for each group.\\nFigure 1-8. Clustering\\nVisualization algorithms are also good examples of unsupervised learning algorithms:\\nyou feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\\nresentation of your data that can easily be plotted ( Figure 1-9). These algorithms try\\nto preserve as much structure as they can (e.g., trying to keep separate clusters in the\\ninput space from overlapping in the visualization), so you can understand how the\\ndata is organized and perhaps identify unsuspected patterns.\\nTypes of Machine Learning Systems | 11',\n",
       "  '3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\\nand so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), T-SNE visual\\nization of the semantic word space. \\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters3\\nA related task is dimensionality reduction, in which the goal is to simplify the data\\nwithout losing too much information. One way to do this is to merge several correla\\nted features into one. For example, a cars mileage may be very correlated with its age,\\nso the dimensionality reduction algorithm will merge them into one feature that rep\\nresents the cars wear and tear. This is called feature extraction.\\nIt is often a good idea to try to reduce the dimension of your train\\ning data using a dimensionality reduction algorithm before you\\nfeed it to another Machine Learning algorithm (such as a super\\nvised learning algorithm). It will run much faster, the data will take\\nup less disk and memory space, and in some cases it may also per\\nform better.\\nY et another important unsupervised task is anomaly detectionfor example, detect\\ning unusual credit card transactions to prevent fraud, catching manufacturing defects,\\nor automatically removing outliers from a dataset before feeding it to another learn\\ning algorithm. The system is shown mostly normal instances during training, so it\\nlearns to recognize them and when it sees a new instance it can tell whether it looks\\n12 | Chapter 1: The Machine Learning Landscape',\n",
       "  '4 Thats when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\\nmixes up two people who look alike, so you need to provide a few labels per person and manually clean up\\nsome clusters.\\nlike a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\\ntask is novelty detection: the difference is that novelty detection algorithms expect to\\nsee only normal data during training, while anomaly detection algorithms are usually\\nmore tolerant, they can often perform well even with a small percentage of outliers in\\nthe training set.\\nFigure 1-10. Anomaly detection\\nFinally, another common unsupervised task is association rule learning, in which the\\ngoal is to dig into large amounts of data and discover interesting relations between\\nattributes. For example, suppose you own a supermarket. Running an association rule\\non your sales logs may reveal that people who purchase barbecue sauce and potato\\nchips also tend to buy steak. Thus, you may want to place these items close to each \\nother.\\nSemisupervised learning\\nSome algorithms can deal with partially labeled training data, usually a lot of unla\\nbeled data and a little bit of labeled data. This is called semisupervised learning\\n(Figure 1-11).\\nSome photo-hosting services, such as Google Photos, are good examples of this. Once\\nyou upload all your family photos to the service, it automatically recognizes that the\\nsame person A shows up in photos 1, 5, and 11, while another person B shows up in\\nphotos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\\nthe system needs is for you to tell it who these people are. Just one label per person, 4\\nand it is able to name everyone in every photo, which is useful for searching photos.\\nTypes of Machine Learning Systems | 13',\n",
       "  'Figure 1-11. Semisupervised learning\\nMost semisupervised learning algorithms are combinations of unsupervised and\\nsupervised algorithms. For example, deep belief networks (DBNs) are based on unsu\\npervised components called restricted Boltzmann machines (RBMs) stacked on top of\\none another. RBMs are trained sequentially in an unsupervised manner, and then the\\nwhole system is fine-tuned using supervised learning techniques.\\nReinforcement Learning\\nReinforcement Learning is a very different beast. The learning system, called an agent\\nin this context, can observe the environment, select and perform actions, and get\\nrewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It\\nmust then learn by itself what is the best strategy, called a policy, to get the most\\nreward over time. A policy defines what action the agent should choose when it is in a\\ngiven situation.\\n14 | Chapter 1: The Machine Learning Landscape',\n",
       "  'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning, the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline learning.\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15',\n",
       "  'batch learning system can adapt to change. Simply update the data and train a new\\nversion of the system from scratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of data can\\ntake many hours, so you would typically train a new system only every 24 hours or\\neven just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre\\ndict stock prices), then you need a more reactive solution.\\nAlso, training on the full set of data requires a lot of computing resources (CPU,\\nmemory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\\nyou automate your system to train from scratch every day, it will end up costing you a\\nlot of money. If the amount of data is huge, it may even be impossible to use a batch\\nlearning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has limited\\nresources (e.g., a smartphone application or a rover on Mars), then carrying around\\nlarge amounts of training data and taking up a lot of resources to train for hours\\nevery day is a showstopper.\\nFortunately, a better option in all these cases is to use algorithms that are capable of\\nlearning incrementally.\\nOnline learning\\nIn online learning, you train the system incrementally by feeding it data instances\\nsequentially, either individually or by small groups called mini-batches. Each learning\\nstep is fast and cheap, so the system can learn about new data on the fly, as it arrives\\n(see Figure 1-13).\\nFigure 1-13. Online learning\\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock\\nprices) and need to adapt to change rapidly or autonomously. It is also a good option\\n16 | Chapter 1: The Machine Learning Landscape',\n",
       "  'if you have limited computing resources: once an online learning system has learned\\nabout new data instances, it does not need them anymore, so you can discard them\\n(unless you want to be able to roll back to a previous state and replay the data). This\\ncan save a huge amount of space.\\nOnline learning algorithms can also be used to train systems on huge datasets that\\ncannot fit in one machines main memory (this is called out-of-core learning). The\\nalgorithm loads part of the data, runs a training step on that data, and repeats the\\nprocess until it has run on all of the data (see Figure 1-14).\\nOut-of-core learning is usually done offline (i.e., not on the live\\nsystem), so online learning can be a confusing name. Think of it as\\nincremental learning.\\nFigure 1-14. Using online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should adapt to\\nchanging data: this is called the learning rate. If you set a high learning rate, then your\\nsystem will rapidly adapt to new data, but it will also tend to quickly forget the old\\ndata (you dont want a spam filter to flag only the latest kinds of spam it was shown).\\nConversely, if you set a low learning rate, the system will have more inertia; that is, it\\nwill learn more slowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points (outliers).\\nA big challenge with online learning is that if bad data is fed to the system, the sys\\ntems performance will gradually decline. If we are talking about a live system, your\\nclients will notice. For example, bad data could come from a malfunctioning sensor\\non a robot, or from someone spamming a search engine to try to rank high in search\\nTypes of Machine Learning Systems | 17',\n",
       "  'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize.\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by usersnot the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity between two emails. A (very basic) simi\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com\\nmon with a known spam email.\\nThis is called instance-based learning: the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape',\n",
       "  'Figure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these exam\\nples, then use that model to make predictions. This is called model-based learning\\n(Figure 1-16).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down\\nload the Better Life Index data from the OECDs website as well as stats about GDP\\nper capita from the IMFs website. Then you join the tables and sort by GDP per cap\\nita. Table 1-1 shows an excerpt of what you get.\\nTypes of Machine Learning Systems | 19',\n",
       "  '5 By convention, the Greek letter  (theta) is frequently used to represent model parameters.\\nTable 1-1. Does money make people happier?\\nCountry GDP per capita (USD) Life satisfaction\\nHungary 12,240 4.9\\nKorea 27,195 5.8\\nFrance 37,675 6.5\\nAustralia 50,962 7.3\\nUnited States 55,805 7.2\\nLets plot the data for a few random countries (Figure 1-17).\\nFigure 1-17. Do you see a trend here?\\nThere does seem to be a trend here! Although the data is noisy (i.e., partly random), it\\nlooks like life satisfaction goes up more or less linearly as the countrys GDP per cap\\nita increases. So you decide to model life satisfaction as a linear function of GDP per\\ncapita. This step is called model selection: you selected a linear model of life satisfac\\ntion with just one attribute, GDP per capita (Equation 1-1).\\nEquation 1-1. A simple linear model\\nlife_satisfaction = 0 + 1  GDP_per_capita\\nThis model has two model parameters, 0 and 1.5 By tweaking these parameters, you\\ncan make your model represent any linear function, as shown in Figure 1-18.\\n20 | Chapter 1: The Machine Learning Landscape',\n",
       "  'Figure 1-18. A few possible linear models\\nBefore you can use your model, you need to define the parameter values 0 and 1.\\nHow can you know which values will make your model perform best? To answer this\\nquestion, you need to specify a performance measure. Y ou can either define a utility\\nfunction (or fitness function) that measures how good your model is, or you can define\\na cost function that measures how bad it is. For linear regression problems, people\\ntypically use a cost function that measures the distance between the linear models\\npredictions and the training examples; the objective is to minimize this distance.\\nThis is where the Linear Regression algorithm comes in: you feed it your training\\nexamples and it finds the parameters that make the linear model fit best to your data.\\nThis is called training the model. In our case the algorithm finds that the optimal\\nparameter values are 0 = 4.85 and 1 = 4.91  105.\\nNow the model fits the training data as closely as possible (for a linear model), as you\\ncan see in Figure 1-19.\\nFigure 1-19. The linear model that fits the training data best\\nTypes of Machine Learning Systems | 21',\n",
       "  '6 The prepare_country_stats() functions definition is not shown here (see this chapters Jupyter notebook if\\nyou want all the gory details). Its just boring Pandas code that joins the life satisfaction data from the OECD\\nwith the GDP per capita data from the IMF .\\n7 Its okay if you dont understand all the code yet; we will present Scikit-Learn in the following chapters.\\nY ou are finally ready to run the model to make predictions. For example, say you\\nwant to know how happy Cypriots are, and the OECD data does not have the answer.\\nFortunately, you can use your model to make a good prediction: you look up Cypruss\\nGDP per capita, find $22,587, and then apply your model and find that life satisfac\\ntion is likely to be somewhere around 4.85 + 22,587  4.91  10-5 = 5.96.\\nTo whet your appetite, Example 1-1 shows the Python code that loads the data, pre\\npares it, 6 creates a scatterplot for visualization, and then trains a linear model and\\nmakes a prediction.7\\nExample 1-1. Training and running a linear model using Scikit-Learn\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport sklearn.linear_model\\n# Load the data\\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=\\',\\')\\ngdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=\\',\\',delimiter=\\'\\\\t\\',\\n                             encoding=\\'latin1\\', na_values=\"n/a\")\\n# Prepare the data\\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\\nX = np.c_[country_stats[\"GDP per capita\"]]\\ny = np.c_[country_stats[\"Life satisfaction\"]]\\n# Visualize the data\\ncountry_stats.plot(kind=\\'scatter\\', x=\"GDP per capita\", y=\\'Life satisfaction\\')\\nplt.show()\\n# Select a linear model\\nmodel = sklearn.linear_model.LinearRegression()\\n# Train the model\\nmodel.fit(X, y)\\n# Make a prediction for Cyprus\\nX_new = [[22587]]  # Cyprus\\' GDP per capita\\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\\n22 | Chapter 1: The Machine Learning Landscape',\n",
       "  'If you had used an instance-based learning algorithm instead, you\\nwould have found that Slovenia has the closest GDP per capita to\\nthat of Cyprus ($20,732), and since the OECD data tells us that\\nSlovenians life satisfaction is 5.7, you would have predicted a life\\nsatisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\\ntwo next closest countries, you will find Portugal and Spain with\\nlife satisfactions of 5.1 and 6.5, respectively. Averaging these three\\nvalues, you get 5.77, which is pretty close to your model-based pre\\ndiction. This simple algorithm is called k-Nearest Neighbors regres\\nsion (in this example, k = 3).\\nReplacing the Linear Regression model with k-Nearest Neighbors\\nregression in the previous code is as simple as replacing these two\\nlines:\\nimport sklearn.linear_model\\nmodel = sklearn.linear_model.LinearRegression()\\nwith these two:\\nimport sklearn.neighbors\\nmodel = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\\nIf all went well, your model will make good predictions. If not, you may need to use\\nmore attributes (employment rate, health, air pollution, etc.), get more or better qual\\nity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres\\nsion model).\\nIn summary:\\n Y ou studied the data.\\n Y ou selected a model.\\n Y ou trained it on the training data (i.e., the learning algorithm searched for the\\nmodel parameter values that minimize a cost function).\\n Finally, you applied the model to make predictions on new cases (this is called\\ninference), hoping that this model will generalize well.\\nThis is what a typical Machine Learning project looks like. In Chapter 2  you will\\nexperience this first-hand by going through an end-to-end project.\\nWe have covered a lot of ground so far: you now know what Machine Learning is\\nreally about, why it is useful, what some of the most common categories of ML sys\\ntems are, and what a typical project workflow looks like. Now lets look at what can go\\nwrong in learning and prevent you from making accurate predictions.\\nTypes of Machine Learning Systems | 23',\n",
       "  'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are bad algorithm and bad data.  Lets start\\nwith examples of bad data.\\nInsufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay apple (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape',\n",
       "  '8 For example, knowing whether to write to,  two,  or too depending on the context.\\n9 Figure reproduced with permission from Banko and Brill (2001), Learning Curves for Confusion Set Disam\\nbiguation. \\n10 The Unreasonable Effectiveness of Data,  Peter Norvig et al. (2009).\\nThe Unreasonable Effectiveness of Data\\nIn a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\\nBrill showed that very different Machine Learning algorithms, including fairly simple\\nones, performed almost identically well on a complex problem of natural language\\ndisambiguation8 once they were given enough data (as you can see in Figure 1-20).\\nFigure 1-20. The importance of data versus algorithms9\\nAs the authors put it: these results suggest that we may want to reconsider the trade-\\noff between spending time and money on algorithm development versus spending it\\non corpus development. \\nThe idea that data matters more than algorithms for complex problems was further\\npopularized by Peter Norvig et al. in a paper titled The Unreasonable Effectiveness\\nof Data published in 2009. 10 It should be noted, however, that small- and medium-\\nsized datasets are still very common, and it is not always easy or cheap to get extra\\ntraining data, so dont abandon algorithms just yet.\\nMain Challenges of Machine Learning | 25',\n",
       "  'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias.\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape',\n",
       "  'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digests\\nsampling method:\\n First, to obtain the addresses to send the polls to, the Literary Digest used tele\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who dont care much about poli\\ntics, people who dont like the Literary Digest, and other key groups. This is a spe\\ncial type of sampling bias called nonresponse bias.\\nHere is another example: say you want to build a system to recognize funk music vid\\neos. One way to build your training set is to search funk music on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTubes search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of funk carioca videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering, involves:\\nMain Challenges of Machine Learning | 27',\n",
       "  ' Feature selection: selecting the most useful features to train on among existing\\nfeatures.\\n Feature extraction: combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, lets look at a couple of exam\\nples of bad algorithms.\\nOverfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting: it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the countrys\\nname. In that case, a complex model may detect patterns like the fact that all coun\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape',\n",
       "  'are you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\\nthis pattern occurred in the training data by pure chance, but the model has no way\\nto tell whether a pattern is real or simply the result of noise in the data.\\nOverfitting happens when the model is too complex relative to the\\namount and noisiness of the training data. The possible solutions\\nare:\\n To simplify the model by selecting one with fewer parameters\\n(e.g., a linear model rather than a high-degree polynomial\\nmodel), by reducing the number of attributes in the training\\ndata or by constraining the model\\n To gather more training data\\n To reduce the noise in the training data (e.g., fix data errors\\nand remove outliers)\\nConstraining a model to make it simpler and reduce the risk of overfitting is called\\nregularization. For example, the linear model we defined earlier has two parameters,\\n0 and 1. This gives the learning algorithm two degrees of freedom to adapt the model\\nto the training data: it can tweak both the height ( 0) and the slope ( 1) of the line. If\\nwe forced 1 = 0, the algorithm would have only one degree of freedom and would\\nhave a much harder time fitting the data properly: all it could do is move the line up\\nor down to get as close as possible to the training instances, so it would end up\\naround the mean. A very simple model indeed! If we allow the algorithm to modify 1\\nbut we force it to keep it small, then the learning algorithm will effectively have some\\nwhere in between one and two degrees of freedom. It will produce a simpler model\\nthan with two degrees of freedom, but more complex than with just one. Y ou want to\\nfind the right balance between fitting the training data perfectly and keeping the\\nmodel simple enough to ensure that it will generalize well.\\nFigure 1-23  shows three models: the dotted line represents the original model that\\nwas trained with a few countries missing, the dashed line is our second model trained\\nwith all countries, and the solid line is a linear model trained with the same data as\\nthe first model but with a regularization constraint. Y ou can see that regularization\\nforced the model to have a smaller slope, which fits a bit less the training data that the\\nmodel was trained on, but actually allows it to generalize better to new examples.\\nMain Challenges of Machine Learning | 29',\n",
       "  'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper\\nparameter. A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting the Training Data\\nAs you might guess, underfitting is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam\\nples.\\nThe main options to fix this problem are:\\n Selecting a more powerful model, with more parameters\\n Feeding better features to the learning algorithm (feature engineering)\\n Reducing the constraints on the model (e.g., reducing the regularization hyper\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so lets step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape',\n",
       "  ' Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nTheres just one last important topic to cover: once you have trained a model, you\\ndont want to just hope it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Lets see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complainnot the best idea.\\nA better option is to split your data into two sets: the training set and the test set. As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error (or out-of-\\nsample error), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train\\ning data.\\nIt is common to use 80% of the data for training and hold out 20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: thats probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31',\n",
       "  'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set. This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation: you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set (or sometimes the development set, or\\ndev set). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali\\ndation process, you train the best model on the full training set (including the valida\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation, using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape',\n",
       "  '11 The Lack of A Priori Distinctions Between Learning Algorithms,  D. Wolpert (1996).\\nData Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they wont be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set. After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How\\never, to decide what data to discard and what data to keep, you must make assump\\ntions. For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33',\n",
       "  'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1. How would you define Machine Learning?\\n2. Can you name four types of problems where it shines?\\n3. What is a labeled training set?\\n4. What are the two most common supervised tasks?\\n5. Can you name four common unsupervised tasks?\\n6. What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7. What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8. Would you frame the problem of spam detection as a supervised learning prob\\nlem or an unsupervised learning problem?\\n9. What is an online learning system?\\n10. What is out-of-core learning?\\n11. What type of learning algorithm relies on a similarity measure to make predic\\ntions?\\n12. What is the difference between a model parameter and a learning algorithms\\nhyperparameter?\\n13. What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14. Can you name four of the main challenges in Machine Learning?\\n15. If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16. What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape',\n",
       "  '17. What is the purpose of a validation set?\\n18. What can go wrong if you tune hyperparameters using the test set?\\n19. What is repeated cross-validation and why would you prefer it to using a single\\nvalidation set?\\nSolutions to these exercises are available in ???.\\nExercises | 35',\n",
       "  '1 The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1. Look at the big picture.\\n2. Get the data.\\n3. Discover and visualize the data to gain insights.\\n4. Prepare the data for Machine Learning algorithms.\\n5. Select a model and train it.\\n6. Fine-tune your model.\\n7. Present your solution.\\n8. Launch, monitor, and maintain your system.\\n37',\n",
       "  '2 The original dataset appeared in R. Kelley Pace and Ronald Barry, Sparse Spatial Autoregressions,  Statistics\\n& Probability Letters 33, no. 3 (1997): 291297.\\nWorking with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n Popular open data repositories:\\n UC Irvine Machine Learning Repository\\n Kaggle datasets\\n Amazons AWS datasets\\n Meta portals (they list open data repositories):\\n http://dataportals.org/\\n http://opendatamonitor.eu/\\n http://quandl.com/\\n Other pages listing many popular open data repositories:\\n Wikipedias list of Machine Learning datasets\\n Quora.com question\\n Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos\\nitory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen\\nsus data. This data has metrics such as the population, median income, median hous\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them dis\\ntricts for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39',\n",
       "  '3 A piece of information fed to a Machine Learning system is often called a signal in reference to Shannons\\ninformation theory: you want a high signal/noise ratio.\\nfrom this model? This is important because it will determine how you frame the\\nproblem, what algorithms you will select, what performance measure you will use to\\nevaluate your model, and how much effort you should spend tweaking it.\\nY our boss answers that your models output (a prediction of a districts median hous\\ning price) will be fed to another Machine Learning system (see Figure 2-2 ), along\\nwith many other signals.3 This downstream system will determine whether it is worth\\ninvesting in a given area or not. Getting this right is critical, as it directly affects reve\\nnue.\\nFigure 2-2. A Machine Learning pipeline for real estate investments\\nPipelines\\nA sequence of data processing components is called a data pipeline. Pipelines are very\\ncommon in Machine Learning systems, since there is a lot of data to manipulate and\\nmany data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a large amount\\nof data, processes it, and spits out the result in another data store, and then some time\\nlater the next component in the pipeline pulls this data and spits out its own output,\\nand so on. Each component is fairly self-contained: the interface between components\\nis simply the data store. This makes the system quite simple to grasp (with the help of\\na data flow graph), and different teams can focus on different components. Moreover,\\nif a component breaks down, the downstream components can often continue to run\\nnormally (at least for a while) by just using the last output from the broken compo\\nnent. This makes the architecture quite robust.\\n40 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'On the other hand, a broken component can go unnoticed for some time if proper\\nmonitoring is not implemented. The data gets stale and the overall systems perfor\\nmance drops.\\nThe next question to ask is what the current solution looks like (if any). It will often\\ngive you a reference performance, as well as insights on how to solve the problem.\\nY our boss answers that the district housing prices are currently estimated manually\\nby experts: a team gathers up-to-date information about a district, and when they\\ncannot get the median housing price, they estimate it using complex rules.\\nThis is costly and time-consuming, and their estimates are not great; in cases where\\nthey manage to find out the actual median housing price, they often realize that their\\nestimates were off by more than 20%. This is why the company thinks that it would\\nbe useful to train a model to predict a districts median housing price given other data\\nabout that district. The census data looks like a great dataset to exploit for this pur\\npose, since it includes the median housing prices of thousands of districts, as well as\\nother data.\\nOkay, with all this information you are now ready to start designing your system.\\nFirst, you need to frame the problem: is it supervised, unsupervised, or Reinforce\\nment Learning? Is it a classification task, a regression task, or something else? Should\\nyou use batch learning or online learning techniques? Before you read on, pause and\\ntry to answer these questions for yourself.\\nHave you found the answers? Lets see: it is clearly a typical supervised learning task\\nsince you are given labeled training examples (each instance comes with the expected\\noutput, i.e., the districts median housing price). Moreover, it is also a typical regres\\nsion task, since you are asked to predict a value. More specifically, this is a multiple\\nregression problem since the system will use multiple features to make a prediction (it\\nwill use the districts population, the median income, etc.). It is also a univariate\\nregression problem since we are only trying to predict a single value for each district.\\nIf we were trying to predict multiple values per district, it would be a multivariate\\nregression problem. Finally, there is no continuous flow of data coming in the system,\\nthere is no particular need to adjust to changing data rapidly, and the data is small\\nenough to fit in memory, so plain batch learning should do just fine.\\nIf the data was huge, you could either split your batch learning\\nwork across multiple servers (using the MapReduce technique), or\\nyou could use an online learning technique instead.\\nLook at the Big Picture | 41',\n",
       "  'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1 shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X, h = 1\\nm \\ni = 1\\nm\\nh x i  y i 2\\n42 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '4 Recall that the transpose operator flips a column vector into a row vector (and vice versa).\\nNotations\\nThis equation introduces several very common Machine Learning notations that we\\nwill use throughout this book:\\n m is the number of instances in the dataset you are measuring the RMSE on.\\n For example, if you are evaluating the RMSE on a validation set of 2,000 dis\\ntricts, then m = 2,000.\\n x(i) is a vector of all the feature values (excluding the label) of the ith instance in\\nthe dataset, and y(i) is its label (the desired output value for that instance).\\n For example, if the first district in the dataset is located at longitude 118.29,\\nlatitude 33.91, and it has 1,416 inhabitants with a median income of $38,372,\\nand the median house value is $156,400 (ignoring the other features for now),\\nthen:\\nx 1 =\\n118 . 29\\n33 . 91\\n1, 416\\n38, 372\\nand:\\ny 1 = 156, 400\\n X is a matrix containing all the feature values (excluding labels) of all instances in\\nthe dataset. There is one row per instance and the ith row is equal to the transpose\\nof x(i), noted (x(i))T.4\\n For example, if the first district is as just described, then the matrix X looks\\nlike this:\\nX =\\nx 1 T\\nx 2 T\\n\\nx 1999 T\\nx 2000 T\\n= 118 . 29 33 . 91 1, 416 38, 372\\n   \\nLook at the Big Picture | 43',\n",
       "  ' h is your systems prediction function, also called a hypothesis. When your system\\nis given an instances feature vector x(i), it outputs a predicted value (i) = h(x(i))\\nfor that instance ( is pronounced y-hat).\\n For example, if your system predicts that the median housing price in the first\\ndistrict is $158,400, then (1) = h(x(1)) = 158,400. The prediction error for this\\ndistrict is (1)  y(1) = 2,000.\\n RMSE(X,h) is the cost function measured on the set of examples using your\\nhypothesis h.\\nWe use lowercase italic font for scalar values (such as m or y(i)) and function names\\n(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\\nmatrices (such as X).\\nEven though the RMSE is generally the preferred performance measure for regression\\ntasks, in some contexts you may prefer to use another function. For example, suppose\\nthat there are many outlier districts. In that case, you may consider using the Mean\\nAbsolute Error (also called the Average Absolute Deviation; see Equation 2-2):\\nEquation 2-2. Mean Absolute Error\\nMAE X, h = 1\\nm \\ni = 1\\nm\\nh x i  y i\\nBoth the RMSE and the MAE are ways to measure the distance between two vectors:\\nthe vector of predictions and the vector of target values. Various distance measures,\\nor norms, are possible:\\n Computing the root of a sum of squares (RMSE) corresponds to the Euclidean\\nnorm: it is the notion of distance you are familiar with. It is also called the  2\\nnorm, noted   2 (or just   ).\\n Computing the sum of absolutes (MAE) corresponds to the  1 norm, noted   1.\\nIt is sometimes called the Manhattan norm because it measures the distance\\nbetween two points in a city if you can only travel along orthogonal city blocks.\\n More generally, the  k norm of a vector v containing n elements is defined as\\n  k = v0\\nk + v1\\nk + + vn\\nk\\n1\\nk .  0 just gives the number of non-zero ele\\nments in the vector, and  gives the maximum absolute value in the vector.\\n The higher the norm index, the more it focuses on large values and neglects small\\nones. This is why the RMSE is more sensitive to outliers than the MAE. But when\\n44 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '5 The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.\\noutliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., cheap, \\nmedium,  or expensive) and then uses those categories instead of the prices them\\nselves? In this case, getting the price perfectly right is not important at all; your sys\\ntem just needs to get the category right. If thats so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou dont want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y oure\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIts time to get your hands dirty. Dont hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note\\nbook is available at https://github.com/ageron/handson-ml2.\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/.5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to Download the Data on page 49. If you dont have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys\\nGet the Data | 45',\n",
       "  \"6 We will show the installation steps using pip in a bash shell on a Linux or MacOS system. Y ou may need to\\nadapt these commands to your own system. On Windows, we recommend installing Anaconda instead.\\n7 If you want to upgrade pip for all users on your machine rather than just your own user, you should remove\\nthe --user option and make sure you have administrator rights (e.g., by adding sudo before the whole com\\nmand on Linux or MacOSX).\\n8 Alternative tools include venv (very similar to virtualenv and included in the standard library), virtualenv\\nwrapper (provides extra functionalities on top of virtualenv), pyenv (allows easy switching between Python\\nversions), and pipenv (a great packaging tool by the same author as the popular requests library, built on top\\nof pip, virtualenv and more).\\ntems packaging system (e.g., apt-get on Ubuntu, or MacPorts or HomeBrew on\\nMacOS), install a Scientific Python distribution such as Anaconda and use its packag\\ning system, or just use Pythons own packaging system, pip, which is included by\\ndefault with the Python binary installers (since Python 2.7.9). 6 Y ou can check to see if\\npip is installed by typing the following command:\\n$ python3 -m pip --version\\npip 19.0.2 from [...]/lib/python3.6/site-packages (python 3.6)\\nY ou should make sure you have a recent version of pip installed. To upgrade the pip\\nmodule, type:7\\n$ python3 -m pip install --user -U pip\\nCollecting pip\\n[...]\\nSuccessfully installed pip-19.0.2\\nCreating an Isolated Environment\\nIf you would like to work in an isolated environment (which is strongly recom\\nmended so you can work on different projects without having conflicting library ver\\nsions), install virtualenv8 by running the following pip command (again, if you want\\nvirtualenv to be installed for all users on your machine, remove --user and run this\\ncommand with administrator rights):\\n$ python3 -m pip install --user -U virtualenv\\nCollecting virtualenv\\n[...]\\nSuccessfully installed virtualenv\\nNow you can create an isolated Python environment by typing:\\n$ cd $ML_PATH\\n$ virtualenv env\\nUsing base prefix '[...]'\\nNew python executable in [...]/ml/env/bin/python3.6\\nAlso creating executable in [...]/ml/env/bin/python\\nInstalling setuptools, pip, wheel...done.\\n46 | Chapter 2: End-to-End Machine Learning Project\",\n",
       "  '9 Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\\nOctave.\\nNow every time you want to activate this environment, just open a terminal and type:\\n$ cd $ML_PATH\\n$ source env/bin/activate # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate  # on Windows\\nTo deactivate this environment, just type deactivate. While the environment is\\nactive, any package you install using pip will be installed in this isolated environment,\\nand Python will only have access to these packages (if you also want access to the sys\\ntems packages, you should create the environment using virtualenvs --system-site-\\npackages option). Check out virtualenvs documentation for more information.\\nNow you can install all the required modules and their dependencies using this sim\\nple pip command (if you are not using a virtualenv, you will need the --user option\\nor administrator rights):\\n$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\\nCollecting jupyter\\n  Downloading jupyter-1.0.0-py2.py3-none-any.whl\\nCollecting matplotlib\\n  [...]\\nTo check your installation, try to import every module like this:\\n$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\\nThere should be no output and no error. Now you can fire up Jupyter by typing:\\n$ jupyter notebook\\n[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml\\n[I 15:24 NotebookApp] 0 active kernels\\n[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\\n[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all\\nkernels (twice to skip confirmation).\\nA Jupyter server is now running in your terminal, listening to port 8888. Y ou can visit\\nthis server by opening your web browser to http://localhost:8888/ (this usually hap\\npens automatically when the server starts). Y ou should see your empty workspace\\ndirectory (containing only the env directory if you followed the preceding virtualenv\\ninstructions).\\nNow create a new Python notebook by clicking on the New button and selecting the\\nappropriate Python version9 (see Figure 2-3).\\nThis does three things: first, it creates a new notebook file called Untitled.ipynb in\\nyour workspace; second, it starts a Jupyter Python kernel to run this notebook; and\\nGet the Data | 47',\n",
       "  'third, it opens this notebook in a new tab. Y ou should start by renaming this note\\nbook to Housing (this will automatically rename the file to Housing.ipynb) by click\\ning Untitled and typing the new name.\\nFigure 2-3. Your workspace in Jupyter\\nA notebook contains a list of cells. Each cell can contain executable code or formatted\\ntext. Right now the notebook contains only one empty code cell, labeled In [1]: . Try\\ntyping print(\"Hello world!\")  in the cell, and click on the play button (see\\nFigure 2-4) or press Shift-Enter. This sends the current cell to this notebooks Python\\nkernel, which runs it and returns the output. The result is displayed below the cell,\\nand since we reached the end of the notebook, a new cell is automatically created. Go\\nthrough the User Interface Tour from Jupyters Help menu to learn the basics.\\nFigure 2-4. Hello world Python notebook\\n48 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '10 Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.\\nDownload the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations, 10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz, which contains a\\ncomma-separated value (CSV) file called housing.csv with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves import urllib\\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\\n    if not os.path.isdir(housing_path):\\n        os.makedirs(housing_path)\\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\\n    urllib.request.urlretrieve(housing_url, tgz_path)\\n    housing_tgz = tarfile.open(tgz_path)\\n    housing_tgz.extractall(path=housing_path)\\n    housing_tgz.close()\\nNow when you call fetch_housing_data(), it creates a datasets/housing directory in\\nyour workspace, downloads the housing.tgz file, and extracts the housing.csv from it in\\nthis directory.\\nNow lets load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49',\n",
       "  'import pandas as pd\\ndef load_housing_data(housing_path=HOUSING_PATH):\\n    csv_path = os.path.join(housing_path, \"housing.csv\")\\n    return pd.read_csv(csv_path)\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLets take a look at the top five rows using the DataFrames head() method (see\\nFigure 2-5).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude, latitude, housing_median_age, total_rooms, total_bed\\nrooms, population, households, median_income, median_house_value, and\\nocean_proximity.\\nThe info() method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attributes type and number of non-null values (see\\nFigure 2-6).\\n50 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Figure 2-6. Housing info\\nThere are 20,640 instances in the dataset, which means that it is fairly small by\\nMachine Learning standards, but its perfect to get started. Notice that the total_bed\\nrooms attribute has only 20,433 non-null values, meaning that 207 districts are miss\\ning this feature. We will need to take care of this later.\\nAll attributes are numerical, except the ocean_proximity field. Its type is object, so it\\ncould hold any kind of Python object, but since you loaded this data from a CSV file\\nyou know that it must be a text attribute. When you looked at the top five rows, you\\nprobably noticed that the values in the ocean_proximity column were repetitive,\\nwhich means that it is probably a categorical attribute. Y ou can find out what cate\\ngories exist and how many districts belong to each category by using the\\nvalue_counts() method:\\n>>> housing[\"ocean_proximity\"].value_counts()\\n<1H OCEAN     9136\\nINLAND        6551\\nNEAR OCEAN    2658\\nNEAR BAY      2290\\nISLAND           5\\nName: ocean_proximity, dtype: int64\\nLets look at the other fields. The describe() method shows a summary of the\\nnumerical attributes (Figure 2-7).\\nGet the Data | 51',\n",
       "  '12 The standard deviation is generally denoted  (the Greek letter sigma), and it is the square root of the var\\niance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped normal\\ndistribution (also called a Gaussian distribution), which is very common, the 68-95-99.7 rule applies: about\\n68% of the values fall within 1 of the mean, 95% within 2, and 99.7% within 3.\\nFigure 2-7. Summary of each numerical attribute\\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are\\nignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std\\nrow shows the standard deviation, which measures how dispersed the values are. 12\\nThe 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indi\\ncates the value below which a given percentage of observations in a group of observa\\ntions falls. For example, 25% of the districts have a housing_median_age lower than\\n18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\\n25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).\\nAnother quick way to get a feel of the type of data you are dealing with is to plot a \\nhistogram for each numerical attribute. A histogram shows the number of instances\\n(on the vertical axis) that have a given value range (on the horizontal axis). Y ou can\\neither plot this one attribute at a time, or you can call the hist() method on the\\nwhole dataset, and it will plot a histogram for each numerical attribute (see\\nFigure 2-8 ). For example, you can see that slightly over 800 districts have a\\nmedian_house_value equal to about $100,000.\\n%matplotlib inline   # only in a Jupyter notebook\\nimport matplotlib.pyplot as plt\\nhousing.hist(bins=50, figsize=(20,15))\\nplt.show()\\n52 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'The hist() method relies on Matplotlib, which in turn relies on a\\nuser-specified graphical backend to draw on your screen. So before\\nyou can plot anything, you need to specify which backend Matplot\\nlib should use. The simplest option is to use Jupyters magic com\\nmand %matplotlib inline. This tells Jupyter to set up Matplotlib\\nso it uses Jupyters own backend. Plots are then rendered within the\\nnotebook itself. Note that calling show() is optional in a Jupyter\\nnotebook, as Jupyter will automatically display plots when a cell is\\nexecuted.\\nFigure 2-8. A histogram for each numerical attribute\\nNotice a few things in these histograms:\\n1. First, the median income attribute does not look like it is expressed in US dollars\\n(USD). After checking with the team that collected the data, you are told that the\\ndata has been scaled and capped at 15 (actually 15.0001) for higher median\\nincomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers\\nrepresent roughly tens of thousands of dollars (e.g., 3 actually means about\\n$30,000). Working with preprocessed attributes is common in Machine Learning,\\nGet the Data | 53',\n",
       "  'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2. The housing median age and the median house value were also capped. The lat\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your systems out\\nput) to see if this is a problem or not. If they tell you that they need precise pre\\ndictions even beyond $500,000, then you have mainly two options:\\na. Collect proper labels for the districts whose labels were capped.\\nb. Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3. These attributes have very different scales. We will discuss this later in this chap\\nter when we explore feature scaling.\\n4. Finally, many histograms are tail heavy: they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '13 In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like\\nin the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented\\nblocks), and the outputs have no prefix.\\n14 Y ou will often see people set the random seed to 42. This number has no special property, other than to be\\nThe Answer to the Ultimate Question of Life, the Universe, and Everything.\\nimport numpy as np\\ndef split_train_test(data, test_ratio):\\n    shuffled_indices = np.random.permutation(len(data))\\n    test_set_size = int(len(data) * test_ratio)\\n    test_indices = shuffled_indices[:test_set_size]\\n    train_indices = shuffled_indices[test_set_size:]\\n    return data.iloc[train_indices], data.iloc[test_indices]\\nY ou can then use this function like this:13\\n>>> train_set, test_set = split_train_test(housing, 0.2)\\n>>> len(train_set)\\n16512\\n>>> len(test_set)\\n4128\\nWell, this works, but it is not perfect: if you run the program again, it will generate a\\ndifferent test set! Over time, you (or your Machine Learning algorithms) will get to\\nsee the whole dataset, which is what you want to avoid.\\nOne solution is to save the test set on the first run and then load it in subsequent\\nruns. Another option is to set the random number generators seed (e.g., np.ran\\ndom.seed(42))14 before calling np.random.permutation(), so that it always generates\\nthe same shuffled indices.\\nBut both these solutions will break next time you fetch an updated dataset. A com\\nmon solution is to use each instances identifier to decide whether or not it should go\\nin the test set (assuming instances have a unique and immutable identifier). For\\nexample, you could compute a hash of each instances identifier and put that instance\\nin the test set if the hash is lower or equal to 20% of the maximum hash value. This\\nensures that the test set will remain consistent across multiple runs, even if you\\nrefresh the dataset. The new test set will contain 20% of the new instances, but it will\\nnot contain any instance that was previously in the training set. Here is a possible\\nimplementation:\\nfrom zlib import crc32\\ndef test_set_check(identifier, test_ratio):\\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\\ndef split_train_test_by_id(data, test_ratio, id_column):\\n    ids = data[id_column]\\nGet the Data | 55',\n",
       "  '15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\\nthey will end up in the same set (test or train). This introduces some unfortunate sampling bias.\\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\\nUnfortunately, the housing dataset does not have an identifier column. The simplest\\nsolution is to use the row index as the ID:\\nhousing_with_id = housing.reset_index()   # adds an `index` column\\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\\nIf you use the row index as a unique identifier, you need to make sure that new data\\ngets appended to the end of the dataset, and no row ever gets deleted. If this is not\\npossible, then you can try to use the most stable features to build a unique identifier.\\nFor example, a districts latitude and longitude are guaranteed to be stable for a few\\nmillion years, so you could combine them into an ID like so:15\\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\\nScikit-Learn provides a few functions to split datasets into multiple subsets in various\\nways. The simplest function is train_test_split, which does pretty much the same\\nthing as the function split_train_test defined earlier, with a couple of additional\\nfeatures. First there is a random_state parameter that allows you to set the random\\ngenerator seed as explained previously, and second you can pass it multiple datasets\\nwith an identical number of rows, and it will split them on the same indices (this is\\nvery useful, for example, if you have a separate DataFrame for labels):\\nfrom sklearn.model_selection import train_test_split\\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\\nSo far we have considered purely random sampling methods. This is generally fine if\\nyour dataset is large enough (especially relative to the number of attributes), but if it\\nis not, you run the risk of introducing a significant sampling bias. When a survey\\ncompany decides to call 1,000 people to ask them a few questions, they dont just pick\\n1,000 people randomly in a phone book. They try to ensure that these 1,000 people\\nare representative of the whole population. For example, the US population is com\\nposed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\\ntry to maintain this ratio in the sample: 513 female and 487 male. This is called strati\\nfied sampling: the population is divided into homogeneous subgroups called strata,\\nand the right number of instances is sampled from each stratum to guarantee that the\\ntest set is representative of the overall population. If they used purely random sam\\npling, there would be about 12% chance of sampling a skewed test set with either less\\nthan 49% female or more than 54% female. Either way, the survey results would be\\nsignificantly biased.\\n56 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Suppose you chatted with experts who told you that the median income is a very\\nimportant attribute to predict median housing prices. Y ou may want to ensure that\\nthe test set is representative of the various categories of incomes in the whole dataset.\\nSince the median income is a continuous numerical attribute, you first need to create\\nan income category attribute. Lets look at the median income histogram more closely\\n(back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,\\n$15,000$60,000), but some median incomes go far beyond 6. It is important to have\\na sufficient number of instances in your dataset for each stratum, or else the estimate\\nof the stratums importance may be biased. This means that you should not have too\\nmany strata, and each stratum should be large enough. The following code uses the\\npd.cut() function to create an income category attribute with 5 categories (labeled\\nfrom 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\\n1.5 to 3, and so on:\\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\\n                               labels=[1, 2, 3, 4, 5])\\nThese income categories are represented in Figure 2-9:\\nhousing[\"income_cat\"].hist()\\nFigure 2-9. Histogram of income categories\\nNow you are ready to do stratified sampling based on the income category. For this\\nyou can use Scikit-Learns StratifiedShuffleSplit class:\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\\n    strat_train_set = housing.loc[train_index]\\n    strat_test_set = housing.loc[test_index]\\nGet the Data | 57',\n",
       "  'Lets see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data\\nset. Figure 2-10 compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified versus purely random sampling\\nNow you should remove the income_cat attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set, strat_test_set):\\n    set_.drop(\"income_cat\", axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now its time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Lets create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set.copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data (Figure 2-11):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points (Figure 2-12):\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59',\n",
       "  '16 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\\ndown to San Diego (as you might expect). Y ou can add a patch of yellow around Sacramento as well.\\nFigure 2-12. A better visualization highlighting high-density areas\\nNow thats much better: you can clearly see the high-density areas, namely the Bay\\nArea and around Los Angeles and San Diego, plus a long line of fairly high density in\\nthe Central Valley, in particular around Sacramento and Fresno.\\nMore generally, our brains are very good at spotting patterns on pictures, but you\\nmay need to play around with visualization parameters to make the patterns stand\\nout.\\nNow lets look at the housing prices (Figure 2-13). The radius of each circle represents\\nthe districts population (option s), and the color represents the price (option c). We\\nwill use a predefined color map (option cmap) called jet, which ranges from blue\\n(low values) to red (high prices):16\\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\\n)\\nplt.legend()\\n60 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Figure 2-13. California housing prices\\nDiscover and Visualize the Data to Gain Insights | 61',\n",
       "  'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient (also called Pearsons r) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix = housing.corr()\\nNow lets look at how much each attribute correlates with the median house value:\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from 1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to 1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14 shows various plots along with the correlation coeffi\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Figure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia;\\npublic domain image)\\nThe correlation coefficient only measures linear correlations (if x\\ngoes up, then y generally goes up/down). It may completely miss\\nout on nonlinear relationships (e.g., if x is close to zero then y gen\\nerally goes up). Note how all the plots of the bottom row have a\\ncorrelation coefficient equal to zero despite the fact that their axes\\nare clearly not independent: these are examples of nonlinear rela\\ntionships. Also, the second row shows examples where the correla\\ntion coefficient is equal to 1 or 1; notice that this has nothing to\\ndo with the slope. For example, your height in inches has a correla\\ntion coefficient of 1 with your height in feet or in nanometers.\\nAnother way to check for correlation between attributes is to use Pandas\\nscatter_matrix function, which plots every numerical attribute against every other\\nnumerical attribute. Since there are now 11 numerical attributes, you would get 11 2 =\\n121 plots, which would not fit on a page, so lets just focus on a few promising\\nattributes that seem most correlated with the median housing value (Figure 2-15):\\nfrom pandas.plotting import scatter_matrix\\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\\n              \"housing_median_age\"]\\nscatter_matrix(housing[attributes], figsize=(12, 8))\\nDiscover and Visualize the Data to Gain Insights | 63',\n",
       "  'Figure 2-15. Scatter matrix\\nThe main diagonal (top left to bottom right) would be full of straight lines if Pandas\\nplotted each variable against itself, which would not be very useful. So instead Pandas\\ndisplays a histogram of each attribute (other options are available; see Pandas docu\\nmentation for more details).\\nThe most promising attribute to predict the median house value is the median\\nincome, so lets zoom in on their correlation scatterplot (Figure 2-16):\\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\\n             alpha=0.1)\\nThis plot reveals a few things. First, the correlation is indeed very strong; you can\\nclearly see the upward trend and the points are not too dispersed. Second, the price\\ncap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\\nplot reveals other less obvious straight lines: a horizontal line around $450,000,\\nanother around $350,000, perhaps one around $280,000, and a few more below that.\\nY ou may want to try removing the corresponding districts to prevent your algorithms\\nfrom learning to reproduce these data quirks.\\n64 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you dont know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Lets create these new\\nattributes:\\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\\nAnd now lets look at the correlation matrix again:\\n>>> corr_matrix = housing.corr()\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65',\n",
       "  'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrictobviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIts time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first lets revert to a clean training set (by copying strat_train_set once again),\\nand lets separate the predictors and the labels since we dont necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop() \\ncreates a copy of the data and does not affect strat_train_set):\\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so lets create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so lets fix this. Y ou have three options:\\n Get rid of the corresponding districts.\\n Get rid of the whole attribute.\\n Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrames dropna(), drop(), and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\"])    # option 1\\nhousing.drop(\"total_bedrooms\", axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\"].median()  # option 3\\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also dont forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer.\\nHere is how to use it. First, you need to create a SimpleImputer instance, specifying\\nthat you want to replace each attributes missing values with the median of that\\nattribute:\\nfrom sklearn.impute import SimpleImputer\\nimputer = SimpleImputer(strategy=\"median\")\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity:\\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\\nNow you can fit the imputer instance to the training data using the fit() method:\\nimputer.fit(housing_num)\\nThe imputer has simply computed the median of each attribute and stored the result\\nin its statistics_ instance variable. Only the total_bedrooms attribute had missing\\nvalues, but we cannot be sure that there wont be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67',\n",
       "  '17 For more details on the design principles, see  API design for machine learning software: experiences from\\nthe scikit-learn project,  L. Buitinck, G. Louppe, M. Blondel, F . Pedregosa, A. Mller, et al. (2013).\\n>>> housing_num.median().values\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nNow you can use this trained imputer to transform the training set by replacing\\nmissing values by the learned medians:\\nX = imputer.transform(housing_num)\\nThe result is a plain NumPy array containing the transformed features. If you want to\\nput it back into a Pandas DataFrame, its simple:\\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)\\nScikit-Learn Design\\nScikit-Learns API is remarkably well designed. The main design principles are:17\\n Consistency. All objects share a consistent and simple interface:\\n Estimators. Any object that can estimate some parameters based on a dataset\\nis called an estimator (e.g., an imputer is an estimator). The estimation itself is\\nperformed by the fit() method, and it takes only a dataset as a parameter (or\\ntwo for supervised learning algorithms; the second dataset contains the\\nlabels). Any other parameter needed to guide the estimation process is con\\nsidered a hyperparameter (such as an imputers strategy), and it must be set\\nas an instance variable (generally via a constructor parameter).\\n Transformers. Some estimators (such as an imputer) can also transform a\\ndataset; these are called transformers. Once again, the API is quite simple: the\\ntransformation is performed by the transform() method with the dataset to\\ntransform as a parameter. It returns the transformed dataset. This transforma\\ntion generally relies on the learned parameters, as is the case for an imputer.\\nAll transformers also have a convenience method called fit_transform() \\nthat is equivalent to calling fit() and then transform() (but sometimes\\nfit_transform() is optimized and runs much faster).\\n Predictors. Finally, some estimators are capable of making predictions given a\\ndataset; they are called predictors. For example, the LinearRegression model \\nin the previous chapter was a predictor: it predicted life satisfaction given a\\ncountrys GDP per capita. A predictor has a predict() method that takes a\\ndataset of new instances and returns a dataset of corresponding predictions. It\\nalso has a score() method that measures the quality of the predictions given\\n68 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '18 Some predictors also provide methods to measure the confidence of their predictions.\\n19 This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas Series.factorize() method.\\na test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n Inspection. All the estimators hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy), and all the estimators learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_).\\n Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n Composition. Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat = housing[[\"ocean_proximity\"]]\\n>>> housing_cat.head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so lets con\\nvert these categories from text to numbers. For this, we can use Scikit-Learns Ordina\\nlEncoder class19:\\n>>> from sklearn.preprocessing import OrdinalEncoder\\n>>> ordinal_encoder = OrdinalEncoder()\\nPrepare the Data for Machine Learning Algorithms | 69',\n",
       "  \"20 Before Scikit-Learn 0.20, it could only encode integer categorical values, but since 0.20 it can also handle\\nother types of inputs, including text categorical inputs.\\n>>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\\n>>> housing_cat_encoded[:10]\\narray([[0.],\\n       [0.],\\n       [4.],\\n       [1.],\\n       [0.],\\n       [1.],\\n       [0.],\\n       [1.],\\n       [0.],\\n       [0.]])\\nY ou can get the list of categories using the categories_ instance variable. It is a list\\ncontaining a 1D array of categories for each categorical attribute (in this case, a list\\ncontaining a single array since there is just one categorical attribute):\\n>>> ordinal_encoder.categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nOne issue with this representation is that ML algorithms will assume that two nearby\\nvalues are more similar than two distant values. This may be fine in some cases (e.g.,\\nfor ordered categories such as bad , average , good , excellent), but it is obviously\\nnot the case for the ocean_proximity column (for example, categories 0 and 4 are\\nclearly more similar than categories 0 and 1). To fix this issue, a common solution is\\nto create one binary attribute per category: one attribute equal to 1 when the category\\nis <1H OCEAN (and 0 otherwise), another attribute equal to 1 when the category is\\nINLAND (and 0 otherwise), and so on. This is called one-hot encoding, because\\nonly one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new\\nattributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEn\\ncoder class to convert categorical values into one-hot vectors20:\\n>>> from sklearn.preprocessing import OneHotEncoder\\n>>> cat_encoder = OneHotEncoder()\\n>>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\\n>>> housing_cat_1hot\\n<16512x5 sparse matrix of type '<class 'numpy.float64'>'\\n  with 16512 stored elements in Compressed Sparse Row format>\\nNotice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\\nuseful when you have categorical attributes with thousands of categories. After one-\\nhot encoding we get a matrix with thousands of columns, and the matrix is full of\\nzeros except for a single 1 per row. Using up tons of memory mostly to store zeros\\nwould be very wasteful, so instead a sparse matrix only stores the location of the non\\n70 | Chapter 2: End-to-End Machine Learning Project\",\n",
       "  \"21 See SciPys documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray() method:\\n>>> housing_cat_1hot.toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoders categories_\\ninstance variable:\\n>>> cat_encoder.categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity feature with the distance to the ocean (similarly,\\na country code could be replaced with the countrys population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding. Each\\ncategorys representation would be learned during training: this is\\nan example of representation learning (see Chapter 13 and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self), transform(), and fit_transform(). Y ou can get the last one for\\nfree by simply adding TransformerMixin as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args and **kargs in your constructor) you will get\\ntwo extra methods ( get_params() and set_params()) that will be useful for auto\\nPrepare the Data for Machine Learning Algorithms | 71\",\n",
       "  'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\\n        self.add_bedrooms_per_room = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform(self, X, y=None):\\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\\n        population_per_household = X[:, population_ix] / X[:, households_ix]\\n        if self.add_bedrooms_per_room:\\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\\n            return np.c_[X, rooms_per_household, population_per_household,\\n                         bedrooms_per_room]\\n        else:\\n            return np.c_[X, rooms_per_household, population_per_household]\\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\\nhousing_extra_attribs = attr_adder.transform(housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room,\\nset to True by default (it is often helpful to provide sensible defaults). This hyperpara\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling. With few exceptions, Machine Learning algorithms dont perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling and standardization.\\nMin-max scaling (many people call this normalization) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'transformer called MinMaxScaler for this. It has a feature_range hyperparameter\\nthat lets you change the range if you dont want 01 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 015 down to 00.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler for stand\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nnum_pipeline = Pipeline([\\n        (\\'imputer\\', SimpleImputer(strategy=\"median\")),\\n        (\\'attribs_adder\\', CombinedAttributesAdder()),\\n        (\\'std_scaler\\', StandardScaler()),\\n    ])\\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\\nThe Pipeline constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform() method). The names can be anything you like (as long as they are\\nunique and dont contain double underscores __): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipelines fit() method, it calls fit_transform() sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit() method.\\nPrepare the Data for Machine Learning Algorithms | 73',\n",
       "  '22 Just like for pipelines, the name can be anything as long as it does not contain double underscores.\\nThe pipeline exposes the same methods as the final estimator. In this example, the last\\nestimator is a StandardScaler, which is a transformer, so the pipeline has a trans\\nform() method that applies all the transforms to the data in sequence (and of course\\nalso a fit_transform() method, which is the one we used).\\nSo far, we have handled the categorical columns and the numerical columns sepa\\nrately. It would be more convenient to have a single transformer able to handle all col\\numns, applying the appropriate transformations to each column. In version 0.20,\\nScikit-Learn introduced the ColumnTransformer for this purpose, and the good news\\nis that it works great with Pandas DataFrames. Lets use it to apply all the transforma\\ntions to the housing data:\\nfrom sklearn.compose import ColumnTransformer\\nnum_attribs = list(housing_num)\\ncat_attribs = [\"ocean_proximity\"]\\nfull_pipeline = ColumnTransformer([\\n        (\"num\", num_pipeline, num_attribs),\\n        (\"cat\", OneHotEncoder(), cat_attribs),\\n    ])\\nhousing_prepared = full_pipeline.fit_transform(housing)\\nHere is how this works: first we import the ColumnTransformer class, next we get the\\nlist of numerical column names and the list of categorical column names, and we\\nconstruct a ColumnTransformer. The constructor requires a list of tuples, where each\\ntuple contains a name 22, a transformer and a list of names (or indices) of columns\\nthat the transformer should be applied to. In this example, we specify that the numer\\nical columns should be transformed using the num_pipeline that we defined earlier,\\nand the categorical columns should be transformed using a OneHotEncoder. Finally,\\nwe apply this ColumnTransformer to the housing data: it applies each transformer to\\nthe appropriate columns and concatenates the outputs along the second axis (the\\ntransformers must return the same number of rows).\\nNote that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns\\na dense matrix. When there is such a mix of sparse and dense matrices, the Colum\\nnTransformer estimates the density of the final matrix (i.e., the ratio of non-zero\\ncells), and it returns a sparse matrix if the density is lower than a given threshold (by\\ndefault, sparse_threshold=0.3). In this example, it returns a dense matrix. And\\nthats it! We have a preprocessing pipeline that takes the full housing data and applies\\nthe appropriate transformations to each column.\\n74 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Instead of a transformer, you can specify the string \"drop\" if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\" if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder hyperparameter to any\\ntransformer (or to \"passthrough\") if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas, or roll out your own custom transformer to get the same function\\nality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Lets first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model import LinearRegression\\nlin_reg = LinearRegression()\\nlin_reg.fit(housing_prepared, housing_labels)\\nDone! Y ou now have a working Linear Regression model. Lets try it out on a few\\ninstances from the training set:\\n>>> some_data = housing.iloc[:5]\\n>>> some_labels = housing_labels.iloc[:5]\\n>>> some_data_prepared = full_pipeline.transform(some_data)\\n>>> print(\"Predictions:\", lin_reg.predict(some_data_prepared))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\", list(some_labels))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75',\n",
       "  'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Lets measure this regression models RMSE on the whole train\\ning set using Scikit-Learns mean_squared_error function:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> housing_predictions = lin_reg.predict(housing_prepared)\\n>>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\\n>>> lin_rmse = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts\\nmedian_housing_values range between $120,000 and $265,000, so a typical predic\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula\\ntion), but first lets try a more complex model to see how it does.\\nLets train a DecisionTreeRegressor. This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6). The code should look familiar by now:\\nfrom sklearn.tree import DecisionTreeRegressor\\ntree_reg = DecisionTreeRegressor()\\ntree_reg.fit(housing_prepared, housing_labels)\\nNow that the model is trained, lets evaluate it on the training set:\\n>>> housing_predictions = tree_reg.predict(housing_prepared)\\n>>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\\n>>> tree_rmse = np.sqrt(tree_mse)\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you dont want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'train your models against the smaller training set and evaluate them against the vali\\ndation set. Its a bit of work, but nothing too difficult and it would work fairly well.\\nA great alternative is to use Scikit-Learns K-fold cross-validation feature. The follow\\ning code randomly splits the training set into 10 distinct subsets called folds, then it\\ntrains and evaluates the Decision Tree model 10 times, picking a different fold for\\nevaluation every time and training on the other 9 folds. The result is an array con\\ntaining the 10 evaluation scores:\\nfrom sklearn.model_selection import cross_val_score\\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\\n                         scoring=\"neg_mean_squared_error\", cv=10)\\ntree_rmse_scores = np.sqrt(-scores)\\nScikit-Learns cross-validation features expect a utility function\\n(greater is better) rather than a cost function (lower is better), so\\nthe scoring function is actually the opposite of the MSE (i.e., a neg\\native value), which is why the preceding code computes -scores\\nbefore calculating the square root.\\nLets look at the results:\\n>>> def display_scores(scores):\\n...     print(\"Scores:\", scores)\\n...     print(\"Mean:\", scores.mean())\\n...     print(\"Standard deviation:\", scores.std())\\n...\\n>>> display_scores(tree_rmse_scores)\\nScores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\\n 71115.88230639 75585.14172901 70262.86139133 70273.6325285\\n 75366.87952553 71231.65726027]\\nMean: 71407.68766037929\\nStandard deviation: 2439.4345041191004\\nNow the Decision Tree doesnt look as good as it did earlier. In fact, it seems to per\\nform worse than the Linear Regression model! Notice that cross-validation allows\\nyou to get not only an estimate of the performance of your model, but also a measure\\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\\nscore of approximately 71,407, generally 2,439. Y ou would not have this information\\nif you just used one validation set. But cross-validation comes at the cost of training\\nthe model several times, so it is not always possible.\\nLets compute the same scores for the Linear Regression model just to be sure:\\n>>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\\n...                              scoring=\"neg_mean_squared_error\", cv=10)\\n...\\n>>> lin_rmse_scores = np.sqrt(-lin_scores)\\n>>> display_scores(lin_rmse_scores)\\nSelect and Train a Model | 77',\n",
       "  'Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\\n 71552.91566558 67665.10082067]\\nMean: 69052.46136345083\\nStandard deviation: 2731.674001798348\\nThats right: the Decision Tree model is overfitting so badly that it performs worse\\nthan the Linear Regression model.\\nLets try one last model now: the RandomForestRegressor. As we will see in Chap\\nter 7, Random Forests work by training many Decision Trees on random subsets of\\nthe features, then averaging out their predictions. Building a model on top of many\\nother models is called Ensemble Learning, and it is often a great way to push ML algo\\nrithms even further. We will skip most of the code since it is essentially the same as\\nfor the other models:\\n>>> from sklearn.ensemble import RandomForestRegressor\\n>>> forest_reg = RandomForestRegressor()\\n>>> forest_reg.fit(housing_prepared, housing_labels)\\n>>> [...]\\n>>> forest_rmse\\n18603.515021376355\\n>>> display_scores(forest_rmse_scores)\\nScores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\\n 49308.39426421 53446.37892622 48634.8036574  47585.73832311\\n 53490.10699751 50021.5852922 ]\\nMean: 50182.303100336096\\nStandard deviation: 2097.0810550985693\\nWow, this is much better: Random Forests look very promising. However, note that\\nthe score on the training set is still much lower than on the validation sets, meaning\\nthat the model is still overfitting the training set. Possible solutions for overfitting are\\nto simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\\nHowever, before you dive much deeper in Random Forests, you should try out many\\nother models from various categories of Machine Learning algorithms (several Sup\\nport Vector Machines with different kernels, possibly a neural network, etc.), without\\nspending too much time tweaking the hyperparameters. The goal is to shortlist a few\\n(two to five) promising models.\\n78 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Pythons pickle module, or using\\nsklearn.externals.joblib, which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals import joblib\\njoblib.dump(my_model, \"my_model.pkl\")\\n# and later...\\nmy_model_loaded = joblib.load(\"my_model.pkl\")\\nFine-Tune Your Model\\nLets assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Lets look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learns GridSearchCV to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi\\nnation of hyperparameter values for the RandomForestRegressor:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = [\\n    {\\'n_estimators\\': [3, 10, 30], \\'max_features\\': [2, 4, 6, 8]},\\n    {\\'bootstrap\\': [False], \\'n_estimators\\': [3, 10], \\'max_features\\': [2, 3, 4]},\\n  ]\\nforest_reg = RandomForestRegressor()\\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\\n                           scoring=\\'neg_mean_squared_error\\',\\n                           return_train_score=True)\\ngrid_search.fit(housing_prepared, housing_labels)\\nFine-Tune Your Model | 79',\n",
       "  'When you have no idea what value a hyperparameter should have,\\na simple approach is to try out consecutive powers of 10 (or a\\nsmaller number if you want a more fine-grained search, as shown\\nin this example with the n_estimators hyperparameter).\\nThis param_grid tells Scikit-Learn to first evaluate all 3  4 = 12 combinations of\\nn_estimators and max_features hyperparameter values specified in the first dict\\n(dont worry about what these hyperparameters mean for now; they will be explained\\nin Chapter 7), then try all 2  3 = 6 combinations of hyperparameter values in the\\nsecond dict, but this time with the bootstrap hyperparameter set to False instead of\\nTrue (which is the default value for this hyperparameter).\\nAll in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRe\\ngressor hyperparameter values, and it will train each model five times (since we are\\nusing five-fold cross validation). In other words, all in all, there will be 18  5 = 90\\nrounds of training! It may take quite a long time, but when it is done you can get the\\nbest combination of parameters like this:\\n>>> grid_search.best_params_\\n{\\'max_features\\': 8, \\'n_estimators\\': 30}\\nSince 8 and 30 are the maximum values that were evaluated, you\\nshould probably try searching again with higher values, since the\\nscore may continue to improve.\\nY ou can also get the best estimator directly:\\n>>> grid_search.best_estimator_\\nRandomForestRegressor(bootstrap=True, criterion=\\'mse\\', max_depth=None,\\n           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\\n           min_impurity_split=None, min_samples_leaf=1,\\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\\n           n_estimators=30, n_jobs=None, oob_score=False, random_state=None,\\n           verbose=0, warm_start=False)\\nIf GridSearchCV is initialized with refit=True (which is the\\ndefault), then once it finds the best estimator using cross-\\nvalidation, it retrains it on the whole training set. This is usually a\\ngood idea since feeding it more data will likely improve its perfor\\nmance.\\nAnd of course the evaluation scores are also available:\\n>>> cvres = grid_search.cv_results_\\n>>> for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\\n80 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  \"...     print(np.sqrt(-mean_score), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features hyperpara\\nmeter to 8, and the n_estimators hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc\\ncessfully fine-tuned your best model!\\nDont forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room hyperparameter of your\\nCombinedAttributesAdder transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space is large, it is\\noften preferable to use RandomizedSearchCV instead. This class can be used in much\\nthe same way as the GridSearchCV class, but instead of trying out all possible combi\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene\\nfits:\\nFine-Tune Your Model | 81\",\n",
       "  ' If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val\\nues per hyperparameter with the grid search approach).\\n Y ou have more control over the computing budget you want to allocate to hyper\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or ensemble) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7.\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances = grid_search.best_estimator_.feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLets display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\\n>>> cat_encoder = full_pipeline.named_transformers_[\"cat\"]\\n>>> cat_one_hot_attribs = list(cat_encoder.categories_[0])\\n>>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances, attributes), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  '(0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline to transform the data (call transform(), not\\nfit_transform(), you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model = grid_search.best_estimator_\\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\\ny_test = strat_test_set[\"median_house_value\"].copy()\\nX_test_prepared = full_pipeline.transform(X_test)\\nfinal_predictions = final_model.predict(X_test_prepared)\\nfinal_mse = mean_squared_error(y_test, final_predictions)\\nfinal_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence interval for the generalization error using\\nscipy.stats.t.interval():\\n>>> from scipy import stats\\n>>> confidence = 0.95\\n>>> squared_errors = (final_predictions - y_test) ** 2\\n>>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\\n...                          loc=squared_errors.mean(),\\n...                          scale=stats.sem(squared_errors)))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83',\n",
       "  'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your systems limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\nthe median income is the number one predictor of housing prices). In this Califor\\nnia housing example, the final performance of the system is not better than the\\nexperts , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your systems live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to rot as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your systems performance will require sampling the systems predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the systems input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc\\ntioning sensor sending random values, or another teams output becoming stale), but\\nit may take a while before your systems performance degrades enough to trigger an\\nalert. If you monitor your systems inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you dont, you are very\\nlikely to refresh your model only every six months (at best), and your systems perfor\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project',\n",
       "  'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/: you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapters housing dataset:\\n1. Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyper\\nparameters such as kernel=\"linear\" (with various values for the C hyperpara\\nmeter) or kernel=\"rbf\" (with various values for the C and gamma\\nhyperparameters). Dont worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2. Try replacing GridSearchCV with RandomizedSearchCV.\\n3. Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4. Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5. Automatically explore some preparation options using GridSearchCV.\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2.\\nTry It Out! | 85',\n",
       "  '1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data.\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2 we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud\\nied so much that it is often called the Hello World of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87',\n",
       "  '>>> from sklearn.datasets import fetch_openml\\n>>> mnist = fetch_openml(\\'mnist_784\\', version=1)\\n>>> mnist.keys()\\ndict_keys([\\'data\\', \\'target\\', \\'feature_names\\', \\'DESCR\\', \\'details\\',\\n           \\'categories\\', \\'url\\'])\\nDatasets loaded by Scikit-Learn generally have a similar dictionary structure includ\\ning:\\n A DESCR key describing the dataset\\n A data key containing an array with one row per instance and one column per\\nfeature\\n A target key containing an array with the labels\\nLets look at these arrays:\\n>>> X, y = mnist[\"data\"], mnist[\"target\"]\\n>>> X.shape\\n(70000, 784)\\n>>> y.shape\\n(70000,)\\nThere are 70,000 images, and each image has 784 features. This is because each image\\nis 2828 pixels, and each feature simply represents one pixels intensity, from 0\\n(white) to 255 (black). Lets take a peek at one digit from the dataset. All you need to\\ndo is grab an instances feature vector, reshape it to a 2828 array, and display it using\\nMatplotlibs imshow() function:\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nsome_digit = X[0]\\nsome_digit_image = some_digit.reshape(28, 28)\\nplt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\\nplt.axis(\"off\")\\nplt.show()\\nThis looks like a 5, and indeed thats what the label tells us:\\n88 | Chapter 3: Classification',\n",
       "  \">>> y[0]\\n'5'\\nNote that the label is a string. We prefer numbers, so lets cast y to integers:\\n>>> y = y.astype(np.uint8)\\nFigure 3-1 shows a few more images from the MNIST dataset to give you a feel for\\nthe complexity of the classification task.\\nFigure 3-1. A few digits from the MNIST dataset\\nBut wait! Y ou should always create a test set and set it aside before inspecting the data\\nclosely. The MNIST dataset is actually already split into a training set (the first 60,000\\nimages) and a test set (the last 10,000 images):\\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\\nThe training set is already shuffled for us, which is good as this guarantees that all\\ncross-validation folds will be similar (you dont want one fold to be missing some dig\\nits). Moreover, some learning algorithms are sensitive to the order of the training\\nMNIST | 89\",\n",
       "  '2 Shuffling may be a bad idea in some contextsfor example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf\\nfling the dataset ensures that this wont happen.2\\nTraining a Binary Classifier\\nLets simplify the problem for now and only try to identify one digitfor example,\\nthe number 5. This 5-detector will be an example of a binary classifier, capable of\\ndistinguishing between just two classes, 5 and not-5. Lets create the target vectors for\\nthis classification task:\\ny_train_5 = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5 = (y_test == 5)\\nOkay, now lets pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent (SGD) classifier, using Scikit-Learns SGDClassifier class. This clas\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning), as we will see later. Lets create\\nan SGDClassifier and train it on the whole training set:\\nfrom sklearn.linear_model import SGDClassifier\\nsgd_clf = SGDClassifier(random_state=42)\\nsgd_clf.fit(X_train, y_train_5)\\nThe SGDClassifier relies on randomness during training (hence\\nthe name stochastic). If you want reproducible results, you\\nshould set the random_state parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True). Looks like it guessed right\\nin this particular case! Now, lets evaluate this models performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification',\n",
       "  'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap\\nter 2.\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learns cross_val_score() function, and prints the \\nsame result:\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.base import clone\\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\\n    clone_clf = clone(sgd_clf)\\n    X_train_folds = X_train[train_index]\\n    y_train_folds = y_train_5[train_index]\\n    X_test_fold = X_train[test_index]\\n    y_test_fold = y_train_5[test_index]\\n    clone_clf.fit(X_train_folds, y_train_folds)\\n    y_pred = clone_clf.predict(X_test_fold)\\n    n_correct = sum(y_pred == y_test_fold)\\n    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLets use the cross_val_score() function to evaluate your SGDClassifier model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2):\\nPerformance Measures | 91',\n",
       "  '>>> from sklearn.model_selection import cross_val_score\\n>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesnt it? Well, before you get too excited, lets look at a very\\ndumb classifier that just classifies every single image in the not-5 class:\\nfrom sklearn.base import BaseEstimator\\nclass Never5Classifier(BaseEstimator):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this models accuracy? Lets find out:\\n>>> never_5_clf = Never5Classifier()\\n>>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\\narray([0.91125, 0.90855, 0.90915])\\nThats right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu\\nsion matrix. The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5 th row and 3 rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlets keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict() function:\\nfrom sklearn.model_selection import cross_val_predict\\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\\nJust like the cross_val_score() function, cross_val_predict() performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic\\n92 | Chapter 3: Classification',\n",
       "  'tions made on each test fold. This means that you get a clean prediction for each\\ninstance in the training set (clean meaning that the prediction is made by a model\\nthat never saw the data during training).\\nNow you are ready to get the confusion matrix using the confusion_matrix() func\\ntion. Just pass it the target classes ( y_train_5) and the predicted classes\\n(y_train_pred):\\n>>> from sklearn.metrics import confusion_matrix\\n>>> confusion_matrix(y_train_5, y_train_pred)\\narray([[53057,  1522],\\n       [ 1325,  4096]])\\nEach row in a confusion matrix represents an actual class, while each column repre\\nsents a predicted class. The first row of this matrix considers non-5 images (the nega\\ntive class): 53,057 of them were correctly classified as non-5s (they are called true\\nnegatives), while the remaining 1,522 were wrongly classified as 5s ( false positives).\\nThe second row considers the images of 5s (the positive class): 1,325 were wrongly\\nclassified as non-5s ( false negatives), while the remaining 4,096 were correctly classi\\nfied as 5s ( true positives). A perfect classifier would have only true positives and true\\nnegatives, so its confusion matrix would have nonzero values only on its main diago\\nnal (top left to bottom right):\\n>>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection\\n>>> confusion_matrix(y_train_5, y_train_perfect_predictions)\\narray([[54579,     0],\\n       [    0,  5421]])\\nThe confusion matrix gives you a lot of information, but sometimes you may prefer a\\nmore concise metric. An interesting one to look at is the accuracy of the positive pre\\ndictions; this is called the precision of the classifier (Equation 3-1).\\nEquation 3-1. Precision\\nprecision = TP\\nTP + FP\\nTP is the number of true positives, and FP is the number of false positives.\\nA trivial way to have perfect precision is to make one single positive prediction and\\nensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\\nclassifier would ignore all but one positive instance. So precision is typically used\\nalong with another metric named recall, also called sensitivity or true positive rate\\nPerformance Measures | 93',\n",
       "  '(TPR): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2).\\nEquation 3-2. Recall\\nrecall = TP\\nTP + FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2 may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci\\nsion and recall:\\n>>> from sklearn.metrics import precision_score, recall_score\\n>>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore, in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean of precision and recall ( Equation 3-3). Whereas the regular mean\\n94 | Chapter 3: Classification',\n",
       "  'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F 1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1 = 2\\n1\\nprecision + 1\\nrecall\\n= 2  precision  recall\\nprecision + recall = TP\\nTP + FN + FP\\n2\\nTo compute the F1 score, simply call the f1_score() function:\\n>>> from sklearn.metrics import f1_score\\n>>> f1_score(y_train_5, y_train_pred)\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con\\ntexts you really care about recall. For example, if you trained a classifier to detect vid\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas\\nsifiers video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you cant have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff.\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, lets look at how the SGDClassifier makes its classifica\\ntion decisions. For each instance, it computes a score based on a decision function, \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3 shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci\\nsion threshold is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95',\n",
       "  'Figure 3-3. Decision threshold and precision/recall tradeoff\\nScikit-Learn does not let you set the threshold directly, but it does give you access to\\nthe decision scores that it uses to make predictions. Instead of calling the classifiers\\npredict() method, you can call its decision_function() method, which returns a\\nscore for each instance, and then make predictions based on those scores using any\\nthreshold you want:\\n>>> y_scores = sgd_clf.decision_function([some_digit])\\n>>> y_scores\\narray([2412.53175101])\\n>>> threshold = 0\\n>>> y_some_digit_pred = (y_scores > threshold)\\narray([ True])\\nThe SGDClassifier uses a threshold equal to 0, so the previous code returns the same\\nresult as the predict() method (i.e., True). Lets raise the threshold:\\n>>> threshold = 8000\\n>>> y_some_digit_pred = (y_scores > threshold)\\n>>> y_some_digit_pred\\narray([False])\\nThis confirms that raising the threshold decreases recall. The image actually repre\\nsents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\\nthreshold is increased to 8,000.\\nNow how do you decide which threshold to use? For this you will first need to get the\\nscores of all instances in the training set using the cross_val_predict() function\\nagain, but this time specifying that you want it to return decision scores instead of\\npredictions:\\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\\n                             method=\"decision_function\")\\nNow with these scores you can compute precision and recall for all possible thresh\\nolds using the precision_recall_curve() function:\\n96 | Chapter 3: Classification',\n",
       "  'from sklearn.metrics import precision_recall_curve\\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\\nFinally, you can plot precision and recall as functions of the threshold value using\\nMatplotlib (Figure 3-4):\\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\\n    [...] # highlight the threshold, add the legend, axis label and grid\\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\\nplt.show()\\nFigure 3-4. Precision and recall versus the decision threshold\\nY ou may wonder why the precision curve is bumpier than the recall\\ncurve in Figure 3-4. The reason is that precision may sometimes go\\ndown when you raise the threshold (although in general it will go\\nup). To understand why, look back at Figure 3-3 and notice what\\nhappens when you start from the central threshold and move it just\\none digit to the right: precision goes from 4/5 (80%) down to 3/4\\n(75%). On the other hand, recall can only go down when the thres\\nhold is increased, which explains why its curve looks smooth.\\nAnother way to select a good precision/recall tradeoff is to plot precision directly\\nagainst recall, as shown in Figure 3-5 (the same threshold as earlier is highlighed).\\nPerformance Measures | 97',\n",
       "  'Figure 3-5. Precision versus recall\\nY ou can see that precision really starts to fall sharply around 80% recall. Y ou will\\nprobably want to select a precision/recall tradeoff just before that dropfor example,\\nat around 60% recall. But of course the choice depends on your project.\\nSo lets suppose you decide to aim for 90% precision. Y ou look up the first plot and\\nfind that you need to use a threshold of about 8,000. To be more precise you can\\nsearch for the lowest threshold that gives you at least 90% precision ( np.argmax()\\nwill give us the first index of the maximum value, which in this case means the first\\nTrue value):\\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816\\nTo make predictions (on the training set for now), instead of calling the classifiers\\npredict() method, you can just run this code:\\ny_train_pred_90 = (y_scores >= threshold_90_precision)\\nLets check these predictions precision and recall:\\n>>> precision_score(y_train_5, y_train_pred_90)\\n0.9000380083618396\\n>>> recall_score(y_train_5, y_train_pred_90)\\n0.4368197749492714\\nGreat, you have a 90% precision classifier ! As you can see, it is fairly easy to create a\\nclassifier with virtually any precision you want: just set a high enough threshold, and\\nyoure done. Hmm, not so fast. A high-precision classifier is not very useful if its \\nrecall is too low!\\n98 | Chapter 3: Classification',\n",
       "  \"If someone says lets reach 99% precision,  you should ask, at\\nwhat recall?\\nThe ROC Curve\\nThe receiver operating characteristic (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot\\nting precision versus recall, the ROC curve plots the true positive rate (another name\\nfor recall) against the false positive rate. The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate, \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus\\n1  specificity.\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres\\nhold values, using the roc_curve() function:\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6:\\ndef plot_roc_curve(fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth=2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve(fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\",\n",
       "  'Figure 3-6. ROC curve\\nOnce again there is a tradeoff: the higher the recall (TPR), the more false positives\\n(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\\nrandom classifier; a good classifier stays as far away from that line as possible (toward\\nthe top-left corner).\\nOne way to compare classifiers is to measure the area under the curve (AUC). A per\\nfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\\nhave a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\\nAUC:\\n>>> from sklearn.metrics import roc_auc_score\\n>>> roc_auc_score(y_train_5, y_scores)\\n0.9611778893101814\\nSince the ROC curve is so similar to the precision/recall (or PR)\\ncurve, you may wonder how to decide which one to use. As a rule\\nof thumb, you should prefer the PR curve whenever the positive\\nclass is rare or when you care more about the false positives than\\nthe false negatives, and the ROC curve otherwise. For example,\\nlooking at the previous ROC curve (and the ROC AUC score), you\\nmay think that the classifier is really good. But this is mostly\\nbecause there are few positives (5s) compared to the negatives\\n(non-5s). In contrast, the PR curve makes it clear that the classifier\\nhas room for improvement (the curve could be closer to the top-\\nright corner).\\n100 | Chapter 3: Classification',\n",
       "  'Lets train a RandomForestClassifier and compare its ROC curve and ROC AUC\\nscore to the SGDClassifier. First, you need to get scores for each instance in the\\ntraining set. But due to the way it works (see Chapter 7), the RandomForestClassi\\nfier class does not have a decision_function() method. Instead it has a pre\\ndict_proba() method. Scikit-Learn classifiers generally have one or the other. The\\npredict_proba() method returns an array containing a row per instance and a col\\numn per class, each containing the probability that the given instance belongs to the\\ngiven class (e.g., 70% chance that the image represents a 5):\\nfrom sklearn.ensemble import RandomForestClassifier\\nforest_clf = RandomForestClassifier(random_state=42)\\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\\n                                    method=\"predict_proba\")\\nBut to plot a ROC curve, you need scores, not probabilities. A simple solution is to\\nuse the positive classs probability as the score:\\ny_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class\\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\\nNow you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\\nwell to see how they compare (Figure 3-7):\\nplt.plot(fpr, tpr, \"b:\", label=\"SGD\")\\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\\nplt.legend(loc=\"lower right\")\\nplt.show()\\nFigure 3-7. Comparing ROC curves\\nPerformance Measures | 101',\n",
       "  'As you can see in Figure 3-7, the RandomForestClassifiers ROC curve looks much\\nbetter than the SGDClassifiers: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score(y_train_5, y_scores_forest)\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now lets try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers (also\\ncalled multinomial classifiers) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all (OvA) strategy \\n(also called one-versus-the-rest).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one (OvO) strategy. If there are N classes, you need to\\ntrain N  ( N  1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification',\n",
       "  'Scikit-Learn detects when you try to use a binary classification algorithm for a multi\\nclass classification task, and it automatically runs OvA (except for SVM classifiers for\\nwhich it uses OvO). Lets try this with the SGDClassifier:\\n>>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5\\n>>> sgd_clf.predict([some_digit])\\narray([5], dtype=uint8)\\nThat was easy! This code trains the SGDClassifier on the training set using the origi\\nnal target classes from 0 to 9 ( y_train), instead of the 5-versus-all target classes\\n(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\\nScikit-Learn actually trained 10 binary classifiers, got their decision scores for the\\nimage, and selected the class with the highest score.\\nTo see that this is indeed the case, you can call the decision_function() method.\\nInstead of returning just one score per instance, it now returns 10 scores, one per\\nclass:\\n>>> some_digit_scores = sgd_clf.decision_function([some_digit])\\n>>> some_digit_scores\\narray([[-15955.22627845, -38080.96296175, -13326.66694897,\\n           573.52692379, -17680.6846644 ,   2412.53175101,\\n        -25526.86498156, -12290.15704709,  -7946.05205023,\\n        -10631.35888549]])\\nThe highest score is indeed the one corresponding to class 5:\\n>>> np.argmax(some_digit_scores)\\n5\\n>>> sgd_clf.classes_\\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\\n>>> sgd_clf.classes_[5]\\n5\\nWhen a classifier is trained, it stores the list of target classes in its\\nclasses_ attribute, ordered by value. In this case, the index of each\\nclass in the classes_ array conveniently matches the class itself\\n(e.g., the class at index 5 happens to be class 5), but in general you\\nwont be so lucky.\\nIf you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\\nthe OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance\\nand pass a binary classifier to its constructor. For example, this code creates a multi\\nclass classifier using the OvO strategy, based on a SGDClassifier:\\n>>> from sklearn.multiclass import OneVsOneClassifier\\n>>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\\n>>> ovo_clf.fit(X_train, y_train)\\n>>> ovo_clf.predict([some_digit])\\nMulticlass Classification | 103',\n",
       "  'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_)\\n45\\nTraining a RandomForestClassifier is just as easy:\\n>>> forest_clf.fit(X_train, y_train)\\n>>> forest_clf.predict([some_digit])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers can directly classify instances into multiple classes. Y ou can call\\npredict_proba() to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf.predict_proba([some_digit])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5 th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Lets evaluate the SGDClassifiers accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam\\nple, simply scaling the inputs (as discussed in Chapter 2) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing import StandardScaler\\n>>> scaler = StandardScaler()\\n>>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\\n>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV, and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification',\n",
       "  'First, you can look at the confusion matrix. Y ou need to make predictions using the\\ncross_val_predict() function, then call the confusion_matrix() function, just like\\nyou did earlier:\\n>>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\\n>>> conf_mx = confusion_matrix(y_train, y_train_pred)\\n>>> conf_mx\\narray([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\\n       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\\n       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\\n       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\\n       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\\n       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\\n       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\\n       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\\n       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\\n       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])\\nThats a lot of numbers. Its often more convenient to look at an image representation\\nof the confusion matrix, using Matplotlibs matshow() function:\\nplt.matshow(conf_mx, cmap=plt.cm.gray)\\nplt.show()\\nThis confusion matrix looks fairly good, since most images are on the main diagonal,\\nwhich means that they were classified correctly. The 5s look slightly darker than the\\nother digits, which could mean that there are fewer images of 5s in the dataset or that\\nthe classifier does not perform as well on 5s as on other digits. In fact, you can verify\\nthat both are the case.\\nLets focus the plot on the errors. First, you need to divide each value in the confusion\\nmatrix by the number of images in the corresponding class, so you can compare error\\nError Analysis | 105',\n",
       "  'rates instead of absolute number of errors (which would make abundant classes look\\nunfairly bad):\\nrow_sums = conf_mx.sum(axis=1, keepdims=True)\\nnorm_conf_mx = conf_mx / row_sums\\nNow lets fill the diagonal with zeros to keep only the errors, and lets plot the result:\\nnp.fill_diagonal(norm_conf_mx, 0)\\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\\nplt.show()\\nNow you can clearly see the kinds of errors the classifier makes. Remember that rows\\nrepresent actual classes, while columns represent predicted classes. The column for\\nclass 8 is quite bright, which tells you that many images get misclassified as 8s. How\\never, the row for class 8 is not that bad, telling you that actual 8s in general get prop\\nerly classified as 8s. As you can see, the confusion matrix is not necessarily\\nsymmetrical. Y ou can also see that 3s and 5s often get confused (in both directions).\\nAnalyzing the confusion matrix can often give you insights on ways to improve your\\nclassifier. Looking at this plot, it seems that your efforts should be spent on reducing\\nthe false 8s. For example, you could try to gather more training data for digits that\\nlook like 8s (but are not) so the classifier can learn to distinguish them from real 8s.\\nOr you could engineer new features that would help the classifierfor example, writ\\ning an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has\\nnone). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or\\nOpenCV) to make some patterns stand out more, such as closed loops.\\nAnalyzing individual errors can also be a good way to gain insights on what your\\nclassifier is doing and why it is failing, but it is more difficult and time-consuming.\\n106 | Chapter 3: Classification',\n",
       "  '3 But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\\ncomplex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\\nnot mean that it is.\\nFor example, lets plot examples of 3s and 5s (the plot_digits() function just uses\\nMatplotlibs imshow() function; see this chapters Jupyter notebook for details):\\ncl_a, cl_b = 3, 5\\nX_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\\nX_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\\nX_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\\nX_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\\nplt.figure(figsize=(8,8))\\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\\nplt.show()\\nThe two 55 blocks on the left show digits classified as 3s, and the two 55 blocks on\\nthe right show images classified as 5s. Some of the digits that the classifier gets wrong\\n(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\\nwould have trouble classifying them (e.g., the 5 on the 1 st row and 2 nd column truly\\nlooks like a badly written 3). However, most misclassified images seem like obvious\\nerrors to us, and its hard to understand why the classifier made the mistakes it did. 3\\nThe reason is that we used a simple SGDClassifier, which is a linear model. All it\\ndoes is assign a weight per class to each pixel, and when it sees a new image it just\\nsums up the weighted pixel intensities to get a score for each class. So since 3s and 5s\\ndiffer only by a few pixels, this model will easily confuse them.\\nError Analysis | 107',\n",
       "  'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n Alice yes, Bob no, Charlie yes). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification system.\\nWe wont go into face recognition just yet, but lets look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors import KNeighborsClassifier\\ny_train_large = (y_train >= 7)\\ny_train_odd = (y_train % 2 == 1)\\ny_multilabel = np.c_[y_train_large, y_train_odd]\\nknn_clf = KNeighborsClassifier()\\nknn_clf.fit(X_train, y_multilabel)\\nThis code creates a y_multilabel array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large (False) and odd (True).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F 1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F 1 score across all\\nlabels:\\n108 | Chapter 3: Classification',\n",
       "  '4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\\n>>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifiers score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\" in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification (or simply multioutput classification). It is simply a generaliza\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, lets build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifiers output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLets start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPys randint() function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod = X_test + noise\\ny_train_mod = X_train\\ny_test_mod = X_test\\nMultioutput Classification | 109',\n",
       "  '5 Y ou can use the shift() function from the scipy.ndimage.interpolation module. For example,\\nshift(image, [2, 1], cval=0) shifts the image 2 pixels down and 1 pixel to the right.\\nLets take a peek at an image from the test set (yes, were snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlets train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod, y_train_mod)\\nclean_digit = knn_clf.predict([X_test_mod[some_index]])\\nplot_digit(clean_digit)\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights and n_neighbors hyperparameters).\\n2. Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel. 5 Then, for each image in the training set, create four shif\\n110 | Chapter 3: Classification',\n",
       "  'ted copies (one per direction) and add them to the training set. Finally, train your\\nbest model on this expanded training set and measure its accuracy on the test set.\\nY ou should observe that your model performs even better now! This technique of\\nartificially growing the training set is called data augmentation or training set\\nexpansion.\\n3. Tackle the Titanic dataset. A great place to start is on Kaggle.\\n4. Build a spam classifier (a more challenging exercise):\\n Download examples of spam and ham from Apache SpamAssassins public\\ndatasets.\\n Unzip the datasets and familiarize yourself with the data format.\\n Split the datasets into a training set and a test set.\\n Write a data preparation pipeline to convert each email into a feature vector.\\nY our preparation pipeline should transform an email into a (sparse) vector\\nindicating the presence or absence of each possible word. For example, if all\\nemails only ever contain four words, Hello,  how,  are,  you,  then the email\\nHello you Hello Hello you would be converted into a vector [1, 0, 0, 1]\\n(meaning [Hello is present, how is absent, are is absent, you is\\npresent]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\\neach word.\\n Y ou may want to add hyperparameters to your preparation pipeline to control\\nwhether or not to strip off email headers, convert each email to lowercase,\\nremove punctuation, replace all URLs with URL,  replace all numbers with\\nNUMBER,  or even perform stemming (i.e., trim off word endings; there are\\nPython libraries available to do this).\\n Then try out several classifiers and see if you can build a great spam classifier,\\nwith both high recall and high precision.\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2.\\nExercises | 111',\n",
       "  'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any\\nthing about whats under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratchall this\\nwithout knowing how they actually work. Indeed, in many situations you dont really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding whats under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n Using a direct closed-form equation that directly computes the model parame\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113',\n",
       "  ' Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II: Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1, we looked at a simple regression model of life satisfaction: life_satisfac\\ntion = 0 + 1  GDP_per_capita.\\nThis model is just a linear function of the input feature GDP_per_capita. 0 and 1 are\\nthe models parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term (also called the intercept\\nterm), as shown in Equation 4-1.\\nEquation 4-1. Linear Regression model prediction\\ny = 0 + 1x1 + 2x2 + + nxn\\n  is the predicted value.\\n114 | Chapter 4: Training Models',\n",
       "  ' n is the number of features.\\n xi is the ith feature value.\\n j is the j th model parameter (including the bias term 0 and the feature weights\\n1, 2, , n).\\nThis can be written much more concisely using a vectorized form, as shown in Equa\\ntion 4-2.\\nEquation 4-2. Linear Regression model prediction (vectorized form)\\ny = h x =   x\\n  is the models parameter vector, containing the bias term 0 and the feature\\nweights 1 to n.\\n x is the instances feature vector, containing x0 to xn, with x0 always equal to 1.\\n   x is the dot product of the vectors  and x, which is of course equal to\\n0x0 + 1x1 + 2x2 + + nxn.\\n h is the hypothesis function, using the model parameters .\\nIn Machine Learning, vectors are often represented as column vec\\ntors, which are 2D arrays with a single column. If  and x are col\\numn vectors, then the prediction is: y=Tx, where T is the\\ntranspose of  (a row vector instead of a column vector) and Tx is\\nthe matrix multiplication of T and x. It is of course the same pre\\ndiction, except it is now represented as a single cell matrix rather\\nthan a scalar value. In this book we will use this notation to avoid\\nswitching between dot products and matrix multiplications.\\nOkay, thats the Linear Regression model, so now how do we train it? Well, recall that\\ntraining a model means setting its parameters so that the model best fits the training\\nset. For this purpose, we first need a measure of how well (or poorly) the model fits\\nthe training data. In Chapter 2 we saw that the most common performance measure\\nof a regression model is the Root Mean Square Error (RMSE) ( Equation 2-1). There\\nfore, to train a Linear Regression model, you need to find the value of  that minimi\\nzes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE)\\nLinear Regression | 115',\n",
       "  '1 It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2 The demonstration that this returns the value of  that minimizes the cost function is outside the scope of this\\nbook.\\nthan the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis h on a training set X is calculated using\\nEquation 4-3.\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X, h = 1\\nm \\ni = 1\\nm\\nTx i  y i 2\\nMost of these notations were presented in Chapter 2 (see Notations on page 43).\\nThe only difference is that we write h instead of just h in order to make it clear that\\nthe model is parametrized by the vector . To simplify notations, we will just write\\nMSE() instead of MSE(X, h).\\nThe Normal Equation\\nTo find the value of  that minimizes the cost function, there is a closed-form solution\\nin other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation (Equation 4-4).2\\nEquation 4-4. Normal Equation\\n = XTX\\n1\\n XT  y\\n  is the value of  that minimizes the cost function.\\n y is the vector of target values containing y(1) to y(m).\\nLets generate some linear-looking data to test this equation on (Figure 4-1):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models',\n",
       "  'Figure 4-1. Randomly generated linear dataset\\nNow lets compute  using the Normal Equation. We will use the inv() function from\\nNumPys Linear Algebra module (np.linalg) to compute the inverse of a matrix, and\\nthe dot() method for matrix multiplication:\\nX_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\\nThe actual function that we used to generate the data is y = 4 + 3x1 + Gaussian noise.\\nLets see what the equation found:\\n>>> theta_best\\narray([[4.21509616],\\n       [2.77011339]])\\nWe would have hoped for 0 = 4 and 1 = 3 instead of 0 = 4.215 and 1 = 2.770. Close\\nenough, but the noise made it impossible to recover the exact parameters of the origi\\nnal function.\\nNow you can make predictions using :\\n>>> X_new = np.array([[0], [2]])\\n>>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\\n>>> y_predict = X_new_b.dot(theta_best)\\n>>> y_predict\\narray([[4.21509616],\\n       [9.75532293]])\\nLets plot this models predictions (Figure 4-2):\\nplt.plot(X_new, y_predict, \"r-\")\\nplt.plot(X, y, \"b.\")\\nLinear Regression | 117',\n",
       "  '3 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_).\\nplt.axis([0, 2, 0, 15])\\nplt.show()\\nFigure 4-2. Linear Regression model predictions\\nPerforming linear regression using Scikit-Learn is quite simple:3\\n>>> from sklearn.linear_model import LinearRegression\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([4.21509616]), array([[2.77011339]]))\\n>>> lin_reg.predict(X_new)\\narray([[4.21509616],\\n       [9.75532293]])\\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the\\nname stands for least squares), which you could call directly:\\n>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\\n>>> theta_best_svd\\narray([[4.21509616],\\n       [2.77011339]])\\nThis function computes  = X+y, where + is the pseudoinverse of X (specifically the\\nMoore-Penrose inverse). Y ou can use np.linalg.pinv() to compute the pseudoin\\nverse directly:\\n>>> np.linalg.pinv(X_b).dot(y)\\narray([[4.21509616],\\n       [2.77011339]])\\n118 | Chapter 4: Training Models',\n",
       "  'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U  VT (see\\nnumpy.linalg.svd()). The pseudoinverse is computed as X+ = V+UT. To compute\\nthe matrix +, the algorithm takes  and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1)  ( n + 1)\\nmatrix (where n is the number of features). The computational complexity of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learns LinearRegression class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119',\n",
       "  'Suppose you are lost in the mountains in a dense fog; you can only feel the slope of\\nthe ground below your feet. A good strategy to get to the bottom of the valley quickly\\nis to go downhill in the direction of the steepest slope. This is exactly what Gradient\\nDescent does: it measures the local gradient of the error function with regards to the \\nparameter vector , and it goes in the direction of descending gradient. Once the gra\\ndient is zero, you have reached a minimum!\\nConcretely, you start by filling  with random values (this is called random initializa\\ntion), and then you improve it gradually, taking one baby step at a time, each step\\nattempting to decrease the cost function (e.g., the MSE), until the algorithm converges\\nto a minimum (see Figure 4-3).\\nFigure 4-3. Gradient Descent\\nAn important parameter in Gradient Descent is the size of the steps, determined by \\nthe learning rate hyperparameter. If the learning rate is too small, then the algorithm\\nwill have to go through many iterations to converge, which will take a long time (see\\nFigure 4-4).\\n120 | Chapter 4: Training Models',\n",
       "  'Figure 4-4. Learning rate too small\\nOn the other hand, if the learning rate is too high, you might jump across the valley\\nand end up on the other side, possibly even higher up than you were before. This\\nmight make the algorithm diverge, with larger and larger values, failing to find a good\\nsolution (see Figure 4-5).\\nFigure 4-5. Learning rate too large\\nFinally, not all cost functions look like nice regular bowls. There may be holes, ridges,\\nplateaus, and all sorts of irregular terrains, making convergence to the minimum very\\ndifficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran\\ndom initialization starts the algorithm on the left, then it will converge to a local mini\\nmum, which is not as good as the global minimum. If it starts on the right, then it will\\ntake a very long time to cross the plateau, and if you stop too early you will never\\nreach the global minimum.\\nGradient Descent | 121',\n",
       "  '4 Technically speaking, its derivative is Lipschitz continuous.\\n5 Since feature 1 is smaller, it takes a larger change in 1 to affect the cost function, which is why the bowl is\\nelongated along the 1 axis.\\nFigure 4-6. Gradient Descent pitfalls\\nFortunately, the MSE cost function for a Linear Regression model happens to be a\\nconvex function, which means that if you pick any two points on the curve, the line\\nsegment joining them never crosses the curve. This implies that there are no local\\nminima, just one global minimum. It is also a continuous function with a slope that\\nnever changes abruptly.4 These two facts have a great consequence: Gradient Descent\\nis guaranteed to approach arbitrarily close the global minimum (if you wait long\\nenough and if the learning rate is not too high).\\nIn fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\\nthe features have very different scales. Figure 4-7 shows Gradient Descent on a train\\ning set where features 1 and 2 have the same scale (on the left), and on a training set\\nwhere feature 1 has much smaller values than feature 2 (on the right).5\\nFigure 4-7. Gradient Descent with and without feature scaling\\n122 | Chapter 4: Training Models',\n",
       "  'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learns StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the models parameter space: the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func\\ntion with regards to each model parameter j. In other words, you need to calculate\\nhow much the cost function will change if you change j just a little bit. This is called \\na partial derivative. It is like asking what is the slope of the mountain under my feet\\nif I face east? and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa\\ntion 4-5 computes the partial derivative of the cost function with regards to parame\\nter j, noted j MSE().\\nEquation 4-5. Partial derivatives of the cost function\\n\\nj\\nMSE  = 2\\nm \\ni = 1\\nm\\nTx i  y i xj\\ni\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted MSE(), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123',\n",
       "  '6 Eta () is the 7th letter of the Greek alphabet.\\nEquation 4-6. Gradient vector of the cost function\\n MSE  =\\n\\n0\\nMSE \\n\\n1\\nMSE \\n\\n\\nn\\nMSE \\n= 2\\nmXT X  y\\nNotice that this formula involves calculations over the full training\\nset X, at each Gradient Descent step! This is why the algorithm is\\ncalled Batch Gradient Descent: it uses the whole batch of training\\ndata at every step (actually, Full Gradient Descent would probably\\nbe a better name). As a result it is terribly slow on very large train\\ning sets (but we will see much faster Gradient Descent algorithms\\nshortly). However, Gradient Descent scales well with the number of\\nfeatures; training a Linear Regression model when there are hun\\ndreds of thousands of features is much faster using Gradient\\nDescent than using the Normal Equation or SVD decomposition.\\nOnce you have the gradient vector, which points uphill, just go in the opposite direc\\ntion to go downhill. This means subtracting MSE() from . This is where the \\nlearning rate  comes into play: 6 multiply the gradient vector by  to determine the\\nsize of the downhill step (Equation 4-7).\\nEquation 4-7. Gradient Descent step\\n next step =    MSE \\nLets look at a quick implementation of this algorithm:\\neta = 0.1  # learning rate\\nn_iterations = 1000\\nm = 100\\ntheta = np.random.randn(2,1)  # random initialization\\nfor iteration in range(n_iterations):\\n    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\\n    theta = theta - eta * gradients\\n124 | Chapter 4: Training Models',\n",
       "  'That wasnt too hard! Lets look at the resulting theta:\\n>>> theta\\narray([[4.21509616],\\n       [2.77011339]])\\nHey, thats exactly what the Normal Equation found! Gradient Descent worked per\\nfectly. But what if you had used a different learning rate eta? Figure 4-8 shows the\\nfirst 10 steps of Gradient Descent using three different learning rates (the dashed line\\nrepresents the starting point).\\nFigure 4-8. Gradient Descent with various learning rates\\nOn the left, the learning rate is too low: the algorithm will eventually reach the solu\\ntion, but it will take a long time. In the middle, the learning rate looks pretty good: in\\njust a few iterations, it has already converged to the solution. On the right, the learn\\ning rate is too high: the algorithm diverges, jumping all over the place and actually\\ngetting further and further away from the solution at every step.\\nTo find a good learning rate, you can use grid search (see Chapter 2). However, you\\nmay want to limit the number of iterations so that grid search can eliminate models\\nthat take too long to converge.\\nY ou may wonder how to set the number of iterations. If it is too low, you will still be\\nfar away from the optimal solution when the algorithm stops, but if it is too high, you\\nwill waste time while the model parameters do not change anymore. A simple solu\\ntion is to set a very large number of iterations but to interrupt the algorithm when the\\ngradient vector becomes tinythat is, when its norm becomes smaller than a tiny\\nnumber  (called the tolerance)because this happens when Gradient Descent has\\n(almost) reached the minimum.\\nGradient Descent | 125',\n",
       "  '7 Out-of-core algorithms are discussed in Chapter 1.\\nConvergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/) iterations to reach the optimum within a range of  depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9). So once the algo\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models',\n",
       "  'When the cost function is very irregular (as in Figure 4-6), this can actually help the\\nalgorithm jump out of local minima, so Stochastic Gradient Descent has a better\\nchance of finding the global minimum than Batch Gradient Descent does.\\nTherefore randomness is good to escape from local optima, but bad because it means\\nthat the algorithm can never settle at the minimum. One solution to this dilemma is\\nto gradually reduce the learning rate. The steps start out large (which helps make\\nquick progress and escape local minima), then get smaller and smaller, allowing the\\nalgorithm to settle at the global minimum. This process is akin to simulated anneal\\ning, an algorithm inspired from the process of annealing in metallurgy where molten\\nmetal is slowly cooled down. The function that determines the learning rate at each\\niteration is called the learning schedule. If the learning rate is reduced too quickly, you\\nmay get stuck in a local minimum, or even end up frozen halfway to the minimum. If\\nthe learning rate is reduced too slowly, you may jump around the minimum for a\\nlong time and end up with a suboptimal solution if you halt training too early.\\nThis code implements Stochastic Gradient Descent using a simple learning schedule:\\nn_epochs = 50\\nt0, t1 = 5, 50  # learning schedule hyperparameters\\ndef learning_schedule(t):\\n    return t0 / (t + t1)\\ntheta = np.random.randn(2,1)  # random initialization\\nfor epoch in range(n_epochs):\\n    for i in range(m):\\n        random_index = np.random.randint(m)\\n        xi = X_b[random_index:random_index+1]\\n        yi = y[random_index:random_index+1]\\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\\n        eta = learning_schedule(epoch * m + i)\\n        theta = theta - eta * gradients\\nBy convention we iterate by rounds of m iterations; each round is called an epoch. \\nWhile the Batch Gradient Descent code iterated 1,000 times through the whole train\\ning set, this code goes through the training set only 50 times and reaches a fairly good\\nsolution:\\n>>> theta\\narray([[4.21076011],\\n       [2.74856079]])\\nFigure 4-10 shows the first 20 steps of training (notice how irregular the steps are).\\nGradient Descent | 127',\n",
       "  'Figure 4-10. Stochastic Gradient Descent first 20 steps\\nNote that since instances are picked randomly, some instances may be picked several\\ntimes per epoch while others may not be picked at all. If you want to be sure that the\\nalgorithm goes through every instance at each epoch, another approach is to shuffle\\nthe training set (making sure to shuffle the input features and the labels jointly), then\\ngo through it instance by instance, then shuffle it again, and so on. However, this gen\\nerally converges more slowly.\\nWhen using Stochastic Gradient Descent, the training instances\\nmust be independent and identically distributed (IID), to ensure\\nthat the parameters get pulled towards the global optimum, on\\naverage. A simple way to ensure this is to shuffle the instances dur\\ning training (e.g., pick each instance randomly, or shuffle the train\\ning set at the beginning of each epoch). If you do not do this, for\\nexample if the instances are sorted by label, then SGD will start by\\noptimizing for one label, then the next, and so on, and it will not\\nsettle close to the global minimum.\\nTo perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe\\ngressor class, which defaults to optimizing the squared error cost function. The fol\\nlowing code runs for maximum 1000 epochs ( max_iter=1000) or until the loss drops\\nby less than 1e-3 during one epoch ( tol=1e-3), starting with a learning rate of 0.1\\n(eta0=0.1), using the default learning schedule (different from the preceding one),\\nand it does not use any regularization (penalty=None; more details on this shortly):\\nfrom sklearn.linear_model import SGDRegressor\\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\\nsgd_reg.fit(X, y.ravel())\\n128 | Chapter 4: Training Models',\n",
       "  'Once again, you find a solution quite close to the one returned by the Normal Equa\\ntion:\\n>>> sgd_reg.intercept_, sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent. It is quite simple to understand once you know Batch and Stochastic Gradi\\nent Descent: at each step, instead of computing the gradients based on the full train\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches. The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithms progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GDs path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, dont forget that Batch GD takes a lot of time to take each step, and Stochas\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129',\n",
       "  '8 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9 A quadratic equation is of the form y = ax2 + bx + c.\\nLets compare the algorithms weve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1.\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast 2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast 2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression.\\nLets look at an example. First, lets generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models',\n",
       "  'Figure 4-12. Generated nonlinear and noisy dataset\\nClearly, a straight line will never fit this data properly. So lets use Scikit-Learns Poly\\nnomialFeatures class to transform our training data, adding the square (2 nd-degree\\npolynomial) of each feature in the training set as new features (in this case there is\\njust one feature):\\n>>> from sklearn.preprocessing import PolynomialFeatures\\n>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\\n>>> X_poly = poly_features.fit_transform(X)\\n>>> X[0]\\narray([-0.75275929])\\n>>> X_poly[0]\\narray([-0.75275929, 0.56664654])\\nX_poly now contains the original feature of X plus the square of this feature. Now you\\ncan fit a LinearRegression model to this extended training data (Figure 4-13):\\n>>> lin_reg = LinearRegression()\\n>>> lin_reg.fit(X_poly, y)\\n>>> lin_reg.intercept_, lin_reg.coef_\\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\\nPolynomial Regression | 131',\n",
       "  'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y = 0 . 56x1\\n2 + 0 . 93x1 + 1 . 78 when in fact the original\\nfunction was y = 0 . 5x1\\n2 + 1 . 0x1 + 2 . 0 + Gaussian noise.\\nNote that when there are multiple features, Polynomial Regression is capable of find\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures with degree=3 would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d) transforms an array containing n\\nfeatures into an array containing n+d!d!n! features, where n! is the\\nfactorial of n, equal to 1  2  3    n. Beware of the combinato\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14 applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2 nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi\\nble to the training instances.\\n132 | Chapter 4: Training Models',\n",
       "  'Figure 4-14. High-degree Polynomial Regression\\nOf course, this high-degree Polynomial Regression model is severely overfitting the\\ntraining data, while the linear model is underfitting it. The model that will generalize\\nbest in this case is the quadratic model. It makes sense since the data was generated\\nusing a quadratic model, but in general you wont know what function generated the\\ndata, so how can you decide how complex your model should be? How can you tell\\nthat your model is overfitting or underfitting the data?\\nIn Chapter 2 you used cross-validation to get an estimate of a models generalization\\nperformance. If a model performs well on the training data but generalizes poorly\\naccording to the cross-validation metrics, then your model is overfitting. If it per\\nforms poorly on both, then it is underfitting. This is one way to tell when a model is\\ntoo simple or too complex.\\nAnother way is to look at the learning curves: these are plots of the models perfor\\nmance on the training set and the validation set as a function of the training set size\\n(or the training iteration). To generate the plots, simply train the model several times\\non different sized subsets of the training set. The following code defines a function\\nthat plots the learning curves of a model given some training data:\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\ndef plot_learning_curves(model, X, y):\\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\\n    train_errors, val_errors = [], []\\n    for m in range(1, len(X_train)):\\n        model.fit(X_train[:m], y_train[:m])\\n        y_train_predict = model.predict(X_train[:m])\\nLearning Curves | 133',\n",
       "  'y_val_predict = model.predict(X_val)\\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\\nLets look at the learning curves of the plain Linear Regression model (a straight line;\\nFigure 4-15):\\nlin_reg = LinearRegression()\\nplot_learning_curves(lin_reg, X, y)\\nFigure 4-15. Learning curves\\nThis deserves a bit of explanation. First, lets look at the performance on the training\\ndata: when there are just one or two instances in the training set, the model can fit\\nthem perfectly, which is why the curve starts at zero. But as new instances are added\\nto the training set, it becomes impossible for the model to fit the training data per\\nfectly, both because the data is noisy and because it is not linear at all. So the error on\\nthe training data goes up until it reaches a plateau, at which point adding new instan\\nces to the training set doesnt make the average error much better or worse. Now lets\\nlook at the performance of the model on the validation data. When the model is\\ntrained on very few training instances, it is incapable of generalizing properly, which\\nis why the validation error is initially quite big. Then as the model is shown more\\ntraining examples, it learns and thus the validation error slowly goes down. However,\\nonce again a straight line cannot do a good job modeling the data, so the error ends\\nup at a plateau, very close to the other curve.\\nThese learning curves are typical of an underfitting model. Both curves have reached\\na plateau; they are close and fairly high.\\n134 | Chapter 4: Training Models',\n",
       "  'If your model is underfitting the training data, adding more train\\ning examples will not help. Y ou need to use a more complex model\\nor come up with better features.\\nNow lets look at the learning curves of a 10 th-degree polynomial model on the same\\ndata (Figure 4-16):\\nfrom sklearn.pipeline import Pipeline\\npolynomial_regression = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\\n        (\"lin_reg\", LinearRegression()),\\n    ])\\nplot_learning_curves(polynomial_regression, X, y)\\nThese learning curves look a bit like the previous ones, but there are two very impor\\ntant differences:\\n The error on the training data is much lower than with the Linear Regression\\nmodel.\\n There is a gap between the curves. This means that the model performs signifi\\ncantly better on the training data than on the validation data, which is the hall\\nmark of an overfitting model. However, if you used a much larger training set,\\nthe two curves would continue to get closer.\\nFigure 4-16. Learning curves for the polynomial model\\nLearning Curves | 135',\n",
       "  '10 This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodels generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the models excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a models complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a models complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models',\n",
       "  '11 It is common to use the notation J() for cost functions that dont have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis\\ncussed.\\n12 Norms are discussed in Chapter 2.\\nRidge Regression\\nRidge Regression (also called Tikhonov regularization) is a regularized version of Lin\\near Regression: a regularization term equal to i = 1\\nn i\\n2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe models performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter  controls how much you want to regularize the model. If  = 0\\nthen Ridge Regression is just Linear Regression. If  is very large, then all weights end\\nup very close to zero and the result is a flat line going through the datas mean. Equa\\ntion 4-8 presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJ  = MSE  + 1\\n2 i = 1\\nn i\\n2\\nNote that the bias term 0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( 1 to n), then the regularization term is\\nsimply equal to ( w 2)2, where  w 2 represents the 2 norm of the weight vector.12\\nFor Gradient Descent, just add w to the MSE gradient vector (Equation 4-6).\\nIt is important to scale the data (e.g., using a StandardScaler) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137',\n",
       "  '13 A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right).\\nFigure 4-17 shows several Ridge models trained on some linear data using different \\nvalue. On the left, plain Ridge models are used, leading to linear predictions. On the\\nright, the data is first expanded using PolynomialFeatures(degree=10), then it is\\nscaled using a StandardScaler, and finally the Ridge models are applied to the result\\ning features: this is Polynomial Regression with Ridge regularization. Note how\\nincreasing  leads to flatter (i.e., less extreme, more reasonable) predictions; this\\nreduces the models variance but increases its bias.\\nAs with Linear Regression, we can perform Ridge Regression either by computing a \\nclosed-form equation or by performing Gradient Descent. The pros and cons are the\\nsame. Equation 4-9 shows the closed-form solution (where A is the ( n + 1)  ( n + 1)\\nidentity matrix13 except with a 0 in the top-left cell, corresponding to the bias term).\\nFigure 4-17. Ridge Regression\\nEquation 4-9. Ridge Regression closed-form solution\\n = XTX + A\\n1\\n XT  y\\nHere is how to perform Ridge Regression with Scikit-Learn using a closed-form solu\\ntion (a variant of Equation 4-9 using a matrix factorization technique by Andr-Louis\\nCholesky):\\n>>> from sklearn.linear_model import Ridge\\n>>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\\n>>> ridge_reg.fit(X, y)\\n138 | Chapter 4: Training Models',\n",
       "  '14 Alternatively you can use the Ridge class with the \"sag\" solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation Minimizing Finite Sums with the Stochastic Average Gradient Algo\\nrithm by Mark Schmidt et al. from the University of British Columbia.\\n>>> ridge_reg.predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor(penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\" indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the  2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the  1 norm\\nof the weight vector instead of half the square of the 2 norm (see Equation 4-10).\\nEquation 4-10. Lasso Regression cost function\\nJ  = MSE  + i = 1\\nn i\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller  values.\\nRegularized Linear Models | 139',\n",
       "  'Figure 4-18. Lasso Regression\\nAn important characteristic of Lasso Regression is that it tends to completely elimi\\nnate the weights of the least important features (i.e., set them to zero). For example,\\nthe dashed line in the right plot on Figure 4-18 (with  = 10-7) looks quadratic, almost\\nlinear: all the weights for the high-degree polynomial features are equal to zero. In\\nother words, Lasso Regression automatically performs feature selection and outputs a\\nsparse model (i.e., with few nonzero feature weights).\\nY ou can get a sense of why this is the case by looking at Figure 4-19: on the top-left\\nplot, the background contours (ellipses) represent an unregularized MSE cost func\\ntion ( = 0), and the white circles show the Batch Gradient Descent path with that\\ncost function. The foreground contours (diamonds) represent the  1 penalty, and the\\ntriangles show the BGD path for this penalty only (   ). Notice how the path first\\nreaches 1 = 0, then rolls down a gutter until it reaches 2 = 0. On the top-right plot,\\nthe contours represent the same cost function plus an  1 penalty with  = 0.5. The\\nglobal minimum is on the 2 = 0 axis. BGD first reaches 2 = 0, then rolls down the\\ngutter until it reaches the global minimum. The two bottom plots show the same\\nthing but uses an 2 penalty instead. The regularized minimum is closer to  = 0 than\\nthe unregularized minimum, but the weights do not get fully eliminated.\\n140 | Chapter 4: Training Models',\n",
       "  '15 Y ou can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra\\ndient vectors around that point.\\nFigure 4-19. Lasso versus Ridge regularization\\nOn the Lasso cost function, the BGD path tends to bounce across\\nthe gutter toward the end. This is because the slope changes\\nabruptly at 2 = 0. Y ou need to gradually reduce the learning rate in\\norder to actually converge to the global minimum.\\nThe Lasso cost function is not differentiable at i = 0 (for i = 1, 2, , n), but Gradient\\nDescent still works fine if you use a subgradient vector g15 instead when any i = 0.\\nEquation 4-11 shows a subgradient vector equation you can use for Gradient Descent\\nwith the Lasso cost function.\\nEquation 4-11. Lasso Regression subgradient vector\\ng , J =  MSE  + \\nsign 1\\nsign 2\\n\\nsign n\\n where sign i =\\n1 if i < 0\\n0 if i = 0\\n+1 if i > 0\\nRegularized Linear Models | 141',\n",
       "  'Here is a small Scikit-Learn example using the Lasso class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\").\\n>>> from sklearn.linear_model import Lasso\\n>>> lasso_reg = Lasso(alpha=0.1)\\n>>> lasso_reg.fit(X, y)\\n>>> lasso_reg.predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lassos regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12).\\nEquation 4-12. Elastic Net cost function\\nJ  = MSE  + ri = 1\\nn i + 1  r\\n2 i = 1\\nn i\\n2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre\\nfer Lasso or Elastic Net since they tend to reduce the useless features weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learns ElasticNet (l1_ratio corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model import ElasticNet\\n>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\\n>>> elastic_net.fit(X, y)\\n>>> elastic_net.predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping. Figure 4-20 shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models',\n",
       "  'after a while the validation error stops decreasing and actually starts to go back up.\\nThis indicates that the model has started to overfit the training data. With early stop\\nping you just stop training as soon as the validation error reaches the minimum. It is\\nsuch a simple and efficient regularization technique that Geoffrey Hinton called it a\\nbeautiful free lunch. \\nFigure 4-20. Early stopping regularization\\nWith Stochastic and Mini-batch Gradient Descent, the curves are\\nnot so smooth, and it may be hard to know whether you have\\nreached the minimum or not. One solution is to stop only after the\\nvalidation error has been above the minimum for some time (when\\nyou are confident that the model will not do any better), then roll\\nback the model parameters to the point where the validation error\\nwas at a minimum.\\nHere is a basic implementation of early stopping:\\nfrom sklearn.base import clone\\n# prepare the data\\npoly_scaler = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\\n        (\"std_scaler\", StandardScaler())\\n    ])\\nX_train_poly_scaled = poly_scaler.fit_transform(X_train)\\nX_val_poly_scaled = poly_scaler.transform(X_val)\\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\\n                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\\nRegularized Linear Models | 143',\n",
       "  'minimum_val_error = float(\"inf\")\\nbest_epoch = None\\nbest_model = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\\n    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\\n    val_error = mean_squared_error(y_val, y_val_predict)\\n    if val_error < minimum_val_error:\\n        minimum_val_error = val_error\\n        best_epoch = epoch\\n        best_model = clone(sgd_reg)\\nNote that with warm_start=True, when the fit() method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1, some regression algorithms can be used for classifica\\ntion as well (and vice versa). Logistic Regression (also called Logit Regression) is com\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled 1), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled 0). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic of this result (see Equation 4-13).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np = h x =  xT\\nThe logisticnoted ()is a sigmoid function (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.\\nEquation 4-14. Logistic function\\n t = 1\\n1 + exp  t\\n144 | Chapter 4: Training Models',\n",
       "  'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = h(x) that an\\ninstance x belongs to the positive class, it can make its prediction  easily (see Equa\\ntion 4-15).\\nEquation 4-15. Logistic Regression model prediction\\ny = 0 if p < 0 . 5\\n1 if p  0 . 5\\nNotice that (t) < 0.5 when t < 0, and (t)  0.5 when t  0, so a Logistic Regression\\nmodel predicts 1 if xT  is positive, and 0 if it is negative.\\nThe score t is often called the logit: this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds, since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param\\neter vector  so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16 for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\nc  =\\nlog p ify = 1\\nlog 1  p ify = 0\\nLogistic Regression | 145',\n",
       "  'This cost function makes sense because  log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand,  log(t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss, shown in Equation 4-17.\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJ  =  1\\nm i = 1\\nm y i log p i + 1  y i log 1  p i\\nThe bad news is that there is no known closed-form equation to compute the value of\\n that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter j is given by Equation 4-18.\\nEquation 4-18. Logistic cost function partial derivatives\\n\\nj\\nJ  = 1\\nm \\ni = 1\\nm\\n Tx i  y i xj\\ni\\nThis equation looks very much like Equation 4-5: for each instance it computes the\\nprediction error and multiplies it by the j th feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. Thats\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLets use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22).\\n146 | Chapter 4: Training Models',\n",
       "  '16 Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield (Crea\\ntive Commons BY-SA 2.0), Iris-Versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0),\\nand Iris-Setosa photo is public domain.\\n17 NumPys reshape() function allows one dimension to be 1, which means unspecified: the value is inferred\\nfrom the length of the array and the remaining dimensions.\\nFigure 4-22. Flowers of three iris plant species16\\nLets try to build a classifier to detect the Iris-Virginica type based only on the petal\\nwidth feature. First lets load the data:\\n>>> from sklearn import datasets\\n>>> iris = datasets.load_iris()\\n>>> list(iris.keys())\\n[\\'data\\', \\'target\\', \\'target_names\\', \\'DESCR\\', \\'feature_names\\', \\'filename\\']\\n>>> X = iris[\"data\"][:, 3:]  # petal width\\n>>> y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\\nNow lets train a Logistic Regression model:\\nfrom sklearn.linear_model import LogisticRegression\\nlog_reg = LogisticRegression()\\nlog_reg.fit(X, y)\\nLets look at the models estimated probabilities for flowers with petal widths varying\\nfrom 0 to 3 cm (Figure 4-23)17:\\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\\ny_proba = log_reg.predict_proba(X_new)\\nplt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\\nLogistic Regression | 147',\n",
       "  '18 It is the the set of points x such that 0 + 1x1 + 2x2 = 0, which defines a straight line.\\nplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\\n# + more Matplotlib code to make the image look pretty\\nFigure 4-23. Estimated probabilities and decision boundary\\nThe petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4\\ncm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\\nsmaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over\\nlap. Above about 2 cm the classifier is highly confident that the flower is an Iris-\\nVirginica (it outputs a high probability to that class), while below 1 cm it is highly\\nconfident that it is not an Iris-Virginica (high probability for the Not Iris-Virginica\\nclass). In between these extremes, the classifier is unsure. However, if you ask it to\\npredict the class (using the predict() method rather than the predict_proba()\\nmethod), it will return whichever class is the most likely. Therefore, there is a decision\\nboundary at around 1.6 cm where both probabilities are equal to 50%: if the petal\\nwidth is higher than 1.6 cm, the classifier will predict that the flower is an Iris-\\nVirginica, or else it will predict that it is not (even if it is not very confident):\\n>>> log_reg.predict([[1.7], [1.5]])\\narray([1, 0])\\nFigure 4-24 shows the same dataset but this time displaying two features: petal width\\nand length. Once trained, the Logistic Regression classifier can estimate the probabil\\nity that a new flower is an Iris-Virginica based on these two features. The dashed line\\nrepresents the points where the model estimates a 50% probability: this is the models\\ndecision boundary. Note that it is a linear boundary.18 Each parallel line represents the\\npoints where the model outputs a specific probability, from 15% (bottom left) to 90%\\n(top right). All the flowers beyond the top-right line have an over 90% chance of\\nbeing Iris-Virginica according to the model.\\n148 | Chapter 4: Training Models',\n",
       "  'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\n1 or 2 penalties. Scitkit-Learn actually adds an 2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression model is not alpha (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3). This is called Softmax Regression, or Multinomial Logistic Regression.\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax function (also called the normalized exponential) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa\\ntion for Linear Regression prediction (see Equation 4-19).\\nEquation 4-19. Softmax score for class k\\nsk x = xT k\\nNote that each class has its own dedicated parameter vector (k). All these vectors are\\ntypically stored as rows in a parameter matrix .\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149',\n",
       "  'then normalizes them (dividing by the sum of all the exponentials). The scores are\\ngenerally called logits or log-odds (although they are actually unnormalized log-\\nodds).\\nEquation 4-20. Softmax function\\npk =  s x k =\\nexp sk x\\nj = 1\\nK exp sj x\\n K is the number of classes.\\n s(x) is a vector containing the scores of each class for the instance x.\\n (s(x))k is the estimated probability that the instance x belongs to class k given\\nthe scores of each class for that instance.\\nJust like the Logistic Regression classifier, the Softmax Regression classifier predicts\\nthe class with the highest estimated probability (which is simply the class with the\\nhighest score), as shown in Equation 4-21.\\nEquation 4-21. Softmax Regression classifier prediction\\ny = argmax\\nk\\n s x k = argmax\\nk\\nsk x = argmax\\nk\\n k T\\nx\\n The argmax operator returns the value of a variable that maximizes a function. In\\nthis equation, it returns the value of k that maximizes the estimated probability\\n(s(x))k.\\nThe Softmax Regression classifier predicts only one class at a time\\n(i.e., it is multiclass, not multioutput) so it should be used only with\\nmutually exclusive classes such as different types of plants. Y ou\\ncannot use it to recognize multiple people in one picture.\\nNow that you know how the model estimates probabilities and makes predictions,\\nlets take a look at training. The objective is to have a model that estimates a high\\nprobability for the target class (and consequently a low probability for the other\\nclasses). Minimizing the cost function shown in Equation 4-22 , called the cross\\nentropy, should lead to this objective because it penalizes the model when it estimates\\na low probability for a target class. Cross entropy is frequently used to measure how\\n150 | Chapter 4: Training Models',\n",
       "  'well a set of estimated class probabilities match the target classes (we will use it again\\nseveral times in the following chapters).\\nEquation 4-22. Cross entropy cost function\\nJ  =  1\\nm i = 1\\nm k = 1\\nK yk\\ni log pk\\ni\\n yki is the target probability that the i th instance belongs to class k. In general, it is\\neither equal to 1 or 0, depending on whether the instance belongs to the class or\\nnot.\\nNotice that when there are just two classes ( K = 2), this cost function is equivalent to\\nthe Logistic Regressions cost function (log loss; see Equation 4-17).\\nCross Entropy\\nCross entropy originated from information theory. Suppose you want to efficiently\\ntransmit information about the weather every day. If there are eight options (sunny,\\nrainy, etc.), you could encode each option using 3 bits since 2 3 = 8. However, if you\\nthink it will be sunny almost every day, it would be much more efficient to code\\nsunny on just one bit (0) and the other seven options on 4 bits (starting with a 1).\\nCross entropy measures the average number of bits you actually send per option. If\\nyour assumption about the weather is perfect, cross entropy will just be equal to the\\nentropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump\\ntions are wrong (e.g., if it rains often), cross entropy will be greater by an amount \\ncalled the KullbackLeibler divergence.\\nThe cross entropy between two probability distributions p and q is defined as\\nH p, q =   x p x log q x  (at least when the distributions are discrete). For more\\ndetails, check out this video.\\nThe gradient vector of this cost function with regards to (k) is given by Equation\\n4-23:\\nEquation 4-23. Cross entropy gradient vector for class k\\n\\n k J  = 1\\nm \\ni = 1\\nm\\npk\\ni  yk\\ni x i\\nNow you can compute the gradient vector for every class, then use Gradient Descent\\n(or any other optimization algorithm) to find the parameter matrix  that minimizes\\nthe cost function.\\nLogistic Regression | 151',\n",
       "  'Lets use Softmax Regression to classify the iris flowers into all three classes. Scikit-\\nLearns LogisticRegression uses one-versus-all by default when you train it on more\\nthan two classes, but you can set the multi_class hyperparameter to \"multinomial\"\\nto switch it to Softmax Regression instead. Y ou must also specify a solver that sup\\nports Softmax Regression, such as the \"lbfgs\" solver (see Scikit-Learns documenta\\ntion for more details). It also applies  2 regularization by default, which you can\\ncontrol using the hyperparameter C.\\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\\ny = iris[\"target\"]\\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\\nsoftmax_reg.fit(X, y)\\nSo the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\\nyour model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\\nwith 94.2% probability (or Iris-Versicolor with 5.8% probability):\\n>>> softmax_reg.predict([[5, 2]])\\narray([2])\\n>>> softmax_reg.predict_proba([[5, 2]])\\narray([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\\nFigure 4-25 shows the resulting decision boundaries, represented by the background\\ncolors. Notice that the decision boundaries between any two classes are linear. The\\nfigure also shows the probabilities for the Iris-Versicolor class, represented by the\\ncurved lines (e.g., the line labeled with 0.450 represents the 45% probability bound\\nary). Notice that the model can predict a class that has an estimated probability below\\n50%. For example, at the point where all decision boundaries meet, all classes have an\\nequal estimated probability of 33%.\\nFigure 4-25. Softmax Regression decision boundaries\\n152 | Chapter 4: Training Models',\n",
       "  'Exercises\\n1. What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2. Suppose the features in your training set have very different scales. What algo\\nrithms might suffer from this, and how? What can you do about it?\\n3. Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4. Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5. Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali\\ndation error goes up?\\n7. Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8. Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9. Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari\\nzation hyperparameter  or reduce it?\\n10. Why would you want to use:\\n Ridge Regression instead of plain Linear Regression (i.e., without any regulari\\nzation)?\\n Lasso instead of Ridge Regression?\\n Elastic Net instead of Lasso?\\n11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres\\nsion classifier?\\n12. Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153',\n",
       "  'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any\\none interested in Machine Learning should have it in their toolbox. SVMs are partic\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4. The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155',\n",
       "  'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification.\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances off the street will not affect the decision\\nboundary at all: it is fully determined (or supported) by the instances located on the\\nedge of the street. These instances are called the support vectors (they are circled in\\nFigure 5-1).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2: on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learns StandardScaler), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification. There are two main issues with hard margin classifi\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi\\ntive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1 without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines',\n",
       "  'Figure 5-3. Hard margin sensitivity to outliers\\nTo avoid these issues it is preferable to use a more flexible model. The objective is to\\nfind a good balance between keeping the street as large as possible and limiting the\\nmargin violations (i.e., instances that end up in the middle of the street or even on the\\nwrong side). This is called soft margin classification.\\nIn Scikit-Learns SVM classes, you can control this balance using the C hyperparame\\nter: a smaller C value leads to a wider street but more margin violations. Figure 5-4\\nshows the decision boundaries and margins of two soft margin SVM classifiers on a\\nnonlinearly separable dataset. On the left, using a low C value the margin is quite\\nlarge, but many instances end up on the street. On the right, using a high C value the\\nclassifier makes fewer margin violations but ends up with a smaller margin. However,\\nit seems likely that the first classifier will generalize better: in fact even on this train\\ning set it makes fewer prediction errors, since most of the margin violations are\\nactually on the correct side of the decision boundary.\\nFigure 5-4. Large margin (left) versus fewer margin violations (right)\\nIf your SVM model is overfitting, you can try regularizing it by\\nreducing C.\\nThe following Scikit-Learn code loads the iris dataset, scales the features, and then\\ntrains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss\\nfunction, described shortly) to detect Iris-Virginica flowers. The resulting model is\\nrepresented on the left of Figure 5-4.\\nLinear SVM Classification | 157',\n",
       "  'import numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import LinearSVC\\niris = datasets.load_iris()\\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\\nsvm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\\n    ])\\nsvm_clf.fit(X, y)\\nThen, as usual, you can use the model to make predictions:\\n>>> svm_clf.predict([[5.5, 1.7]])\\narray([1.])\\nUnlike Logistic Regression classifiers, SVM classifiers do not out\\nput probabilities for each class.\\nAlternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\\nis much slower, especially with large training sets, so it is not recommended. Another\\noption is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\\nalpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\\ntrain a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\\ncan be useful to handle huge datasets that do not fit in memory (out-of-core train\\ning), or to handle online classification tasks.\\nThe LinearSVC class regularizes the bias term, so you should center\\nthe training set first by subtracting its mean. This is automatic if\\nyou scale the data using the StandardScaler. Moreover, make sure\\nyou set the loss hyperparameter to \"hinge\", as it is not the default\\nvalue. Finally, for better performance you should set the dual\\nhyperparameter to False, unless there are more features than\\ntraining instances (we will discuss duality later in the chapter).\\n158 | Chapter 5: Support Vector Machines',\n",
       "  'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5: it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline containing a\\nPolynomialFeatures transformer (discussed in Polynomial Regression on page\\n130), followed by a StandardScaler and a LinearSVC. Lets test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons() function:\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import PolynomialFeatures\\npolynomial_svm_clf = Pipeline([\\n        (\"poly_features\", PolynomialFeatures(degree=3)),\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf.fit(X, y)\\nNonlinear SVM Classification | 159',\n",
       "  'Figure 5-6. Linear SVM classifier using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato\\nrial explosion of the number of features since you dont actually add any features. This\\ntrick is implemented by the SVC class. Lets test it on the moons dataset:\\nfrom sklearn.svm import SVC\\npoly_kernel_svm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf.fit(X, y)\\nThis code trains an SVM classifier using a 3 rd-degree polynomial kernel. It is repre\\nsented on the left of Figure 5-7. On the right is another SVM classifier using a 10 th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines',\n",
       "  'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0 controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark. For example, lets take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = 2 and x1 = 1 (see the left plot in Figure 5-8). Next,\\nlets define the similarity function to be the Gaussian Radial Basis Function  (RBF)\\nwith  = 0.3 (see Equation 5-1).\\nEquation 5-1. Gaussian RBF\\n x,  = exp  x   2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, lets look\\nat the instance x1 = 1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (0.3  1 2)  0.74\\nand x3 = exp (0.3  2 2)  0.30. The plot on the right of Figure 5-8 shows the trans\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification | 161',\n",
       "  'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Lets try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf = Pipeline([\\n        (\"scaler\", StandardScaler()),\\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf.fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma () and C. Increasing\\ngamma makes the bell-shape curve narrower (see the left plot of Figure 5-8), and as a\\nresult each instances range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ\\nence, and the decision boundary ends up smoother. So  acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines',\n",
       "  '1  A Dual Coordinate Descent Method for Large-scale Linear SVM,  Lin et al. (2008).\\nFigure 5-9. SVM classifiers using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels are sometimes used when classi\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel or\\nkernels based on the Levenshtein distance).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC is much faster than SVC(ker\\nnel=\"linear\")), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nsets data structure.\\nComputational Complexity\\nThe LinearSVC class is based on the liblinear library, which implements an optimized\\nalgorithm for linear SVMs. 1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification | 163',\n",
       "  '2 Sequential Minimal Optimization (SMO),  J. Platt (1998).\\nlinearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m  n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter  (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm library, which implements an algorithm that sup\\nports the kernel trick. 2 The training time complexity is usually between O(m2  n)\\nand O(m3  n). Unfortunately, this means that it gets dreadfully slow when the num\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1 compares Scikit-Learns\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m  n) No Yes No\\nSGDClassifier O(m  n) Yes Yes No\\nSVC O(m  n) to O(m  n) No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame\\nter . Figure 5-10 shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin (  = 1.5) and the other with a small margin (  =\\n0.5).\\n164 | Chapter 5: Support Vector Machines',\n",
       "  'Figure 5-10. SVM Regression\\nAdding more training instances within the margin does not affect the models predic\\ntions; thus, the model is said to be -insensitive.\\nY ou can use Scikit-Learns LinearSVR class to perform linear SVM Regression. The\\nfollowing code produces the model represented on the left of Figure 5-10 (the train\\ning data should be scaled and centered first):\\nfrom sklearn.svm import LinearSVR\\nsvm_reg = LinearSVR(epsilon=1.5)\\nsvm_reg.fit(X, y)\\nTo tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam\\nple, Figure 5-11 shows SVM Regression on a random quadratic training set, using a\\n2nd-degree polynomial kernel. There is little regularization on the left plot (i.e., a large\\nC value), and much more regularization on the right plot (i.e., a small C value).\\nFigure 5-11. SVM regression using a 2nd-degree polynomial kernel\\nSVM Regression | 165',\n",
       "  'The following code produces the model represented on the left of Figure 5-11 using\\nScikit-Learns SVR class (which supports the kernel trick). The SVR class is the regres\\nsion equivalent of the SVC class, and the LinearSVR class is the regression equivalent\\nof the LinearSVC class. The LinearSVR class scales linearly with the size of the train\\ning set (just like the LinearSVC class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm import SVR\\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg.fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learns doc\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4 we used the convention of putting all the \\nmodel parameters in one vector , including the bias term 0 and the input feature\\nweights 1 to n, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com\\nputing the decision function wT x + b = w1 x1 +  + wn xn + b: if the result is positive,\\nthe predicted class  is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2.\\nEquation 5-2. Linear SVM classifier prediction\\ny = 0 if wTx + b < 0,\\n1 if wTx + b  0\\n166 | Chapter 5: Support Vector Machines',\n",
       "  '3 More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the deci\\nsion boundary is an (n  1)-dimensional hyperplane.\\nFigure 5-12 shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or 1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec\\ntor,  w . If we divide this slope by 2, the points where the decision function is equal\\nto 1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual\\nize in 2D in Figure 5-13. The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167',\n",
       "  '4 Zeta () is the 6th letter of the Greek alphabet.\\nFigure 5-13. A smaller weight vector results in a larger margin\\nSo we want to minimize  w  to get a large margin. However, if we also want to avoid\\nany margin violation (hard margin), then we need the decision function to be greater\\nthan 1 for all positive training instances, and lower than 1 for negative training\\ninstances. If we define t(i) = 1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\\ninstances (if y(i) = 1), then we can express this constraint as t(i)(wT x(i) + b)  1 for all\\ninstances.\\nWe can therefore express the hard margin linear SVM classifier objective as the con\\nstrained optimization problem in Equation 5-3.\\nEquation 5-3. Hard margin linear SVM classifier objective\\nminimizew, b\\n1\\n2wTw\\nsubject to t i wTx i + b  1 for i = 1, 2, , m\\nWe are minimizing 12wT w, which is equal to 12 w 2, rather than\\nminimizing  w . Indeed, 12 w 2 has a nice and simple derivative\\n(it is just w) while  w  is not differentiable at w = 0. Optimization\\nalgorithms work much better on differentiable functions.\\nTo get the soft margin objective, we need to introduce a slack variable (i)  0 for each\\ninstance:4 (i) measures how much the ith instance is allowed to violate the margin. We\\nnow have two conflicting objectives: making the slack variables as small as possible to\\nreduce the margin violations, and making 12wT w as small as possible to increase the\\nmargin. This is where the C hyperparameter comes in: it allows us to define the trade\\n168 | Chapter 5: Support Vector Machines',\n",
       "  '5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden\\nberghe, Convex Optimization (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Browns\\nseries of video lectures.\\noff between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4.\\nEquation 5-4. Soft margin linear SVM classifier objective\\nminimizew, b, \\n1\\n2wTw + C \\ni = 1\\nm\\n i\\nsubject to t i wTx i + b  1   i and  i  0 for i = 1, 2, , m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program\\nming (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book. 5 The general\\nproblem formulation is given by Equation 5-5.\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np\\n1\\n2pTHp + fTp\\nsubject to Ap  b\\nwhere\\np is an npdimensional vector ( np = number of parameters),\\nH is an np  np matrix,\\nf is an npdimensional vector,\\nA is an nc  np matrix ( nc = number of constraints),\\nb is an ncdimensional vector.\\nNote that the expression A p  b actually defines nc constraints: pT a(i)  b(i) for i = 1,\\n2, , nc, where a(i) is the vector containing the elements of the i th row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169',\n",
       "  '6 The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.\\n nc = m, where m is the number of training instances.\\n H is the np  np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n f = 0, an np-dimensional vector full of 0s.\\n b = 1, an nc-dimensional vector full of 1s.\\n a(i) = t(i) x (i), where x (i) is equal to x(i) with an extra bias feature x 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, , n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem, it is possi\\nble to express a different but closely related problem, called its dual problem. The sol\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri\\nmal problem. Luckily, the SVM problem happens to meet these conditions, 6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6 shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimize\\n1\\n2 \\ni = 1\\nm\\n\\nj = 1\\nm\\n i  j t i t j x i T\\nx j  \\ni = 1\\nm\\n i\\nsubject to  i  0 for i = 1, 2, , m\\n170 | Chapter 5: Support Vector Machines',\n",
       "  '7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a  b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.\\nOnce you find the vector  that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7.\\nEquation 5-7. From the dual solution to the primal solution\\nw = \\ni = 1\\nm\\n i t i x i\\nb = 1\\nns\\n\\ni = 1\\n i > 0\\nm\\nt i  wTx i\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2 nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8 shows the 2nd-degree polyno\\nmial mapping function  that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\n x = \\nx1\\nx2\\n=\\nx1\\n2\\n2 x1x2\\nx2\\n2\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow lets look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product 7 of the\\ntransformed vectors (See Equation 5-9).\\nUnder the Hood | 171',\n",
       "  'Equation 5-9. Kernel trick for a 2nd-degree polynomial mapping\\n a T b =\\na1\\n2\\n2 a1a2\\na2\\n2\\nT\\nb1\\n2\\n2 b1b2\\nb2\\n2\\n= a1\\n2b1\\n2 + 2a1b1a2b2 + a2\\n2b2\\n2\\n= a1b1 + a2b2\\n2 =\\na1\\na2\\nT b1\\nb2\\n2\\n= aTb\\n2\\nHow about that? The dot product of the transformed vectors is equal to the square of\\nthe dot product of the original vectors: (a)T (b) = (aT b)2.\\nNow here is the key insight: if you apply the transformation  to all training instan\\nces, then the dual problem (see Equation 5-6 ) will contain the dot product (x(i))T\\n(x(j)). But if  is the 2 nd-degree polynomial transformation defined in Equation 5-8,\\nthen you can replace this dot product of transformed vectors simply by x i T\\nx j 2\\n. So\\nyou dont actually need to transform the training instances at all: just replace the dot\\nproduct by its square in Equation 5-6. The result will be strictly the same as if you\\nwent through the trouble of actually transforming the training set then fitting a linear\\nSVM algorithm, but this trick makes the whole process much more computationally\\nefficient. This is the essence of the kernel trick.\\nThe function K(a, b) = ( aT b)2 is called a 2 nd-degree polynomial kernel. In Machine\\nLearning, a kernel is a function capable of computing the dot product (a)T (b)\\nbased only on the original vectors a and b, without having to compute (or even to\\nknow about) the transformation . Equation 5-10 lists some of the most commonly\\nused kernels.\\nEquation 5-10. Common kernels\\nLinear: K a, b = aTb\\nPolynomial: K a, b = aTb + r\\nd\\nGaussian RBF: K a, b = exp  a  b 2\\nSigmoid: K a, b = tanh aTb + r\\n172 | Chapter 5: Support Vector Machines',\n",
       "  'Mercers Theorem\\nAccording to Mercers theorem, if a function K(a, b) respects a few mathematical con\\nditions called Mercers conditions (K must be continuous, symmetric in its arguments\\nso K(a, b) = K(b, a), etc.), then there exists a function  that maps a and b into\\nanother space (possibly with much higher dimensions) such that K(a, b) = (a)T (b).\\nSo you can use K as a kernel since you know  exists, even if you dont know what \\nis. In the case of the Gaussian RBF kernel, it can be shown that  actually maps each\\ntraining instance to an infinite-dimensional space, so its a good thing you dont need\\nto actually perform the mapping!\\nNote that some frequently used kernels (such as the Sigmoid kernel) dont respect all\\nof Mercers conditions, yet they generally work well in practice.\\nThere is still one loose end we must tie. Equation 5-7 shows how to go from the dual\\nsolution to the primal solution in the case of a linear SVM classifier, but if you apply\\nthe kernel trick you end up with equations that include (x(i)). In fact, w must have\\nthe same number of dimensions as (x(i)), which may be huge or even infinite, so you\\ncant compute it. But how can you make predictions without knowing w? Well, the\\ngood news is that you can plug in the formula for w from Equation 5-7 into the deci\\nsion function for a new instance x(n), and you get an equation with only dot products\\nbetween input vectors. This makes it possible to use the kernel trick, once again\\n(Equation 5-11).\\nEquation 5-11. Making predictions with a kernelized SVM\\nhw, b  x n = wT x n + b = \\ni = 1\\nm\\n i t i  x i\\nT\\n x n + b\\n= \\ni = 1\\nm\\n i t i  x i T\\n x n + b\\n= \\ni = 1\\n i > 0\\nm\\n i t i K x i , x n + b\\nNote that since (i)  0 only for support vectors, making predictions involves comput\\ning the dot product of the new input vector x(n) with only the support vectors, not all\\nthe training instances. Of course, you also need to compute the bias term b, using the\\nsame trick (Equation 5-12).\\nUnder the Hood | 173',\n",
       "  'Equation 5-12. Computing the bias term using the kernel trick\\nb = 1\\nns\\n\\ni = 1\\n i > 0\\nm\\nt i  wT x i = 1\\nns\\n\\ni = 1\\n i > 0\\nm\\nt i  \\nj = 1\\nm\\n j t j  x j\\nT\\n x i\\n= 1\\nns\\n\\ni = 1\\n i > 0\\nm\\nt i  \\nj = 1\\n j > 0\\nm\\n j t j K x i , x j\\nIf you are starting to get a headache, its perfectly normal: its an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, lets take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier cost function\\nJ w, b = 1\\n2wTw + C \\ni = 1\\nm\\nmax 0, 1  t i wTx i + b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola\\ntions. An instances margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max(0, 1  t) is called the hinge loss function (represented below). It is\\nequal to 0 when t  1. Its derivative (slope) is equal to 1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see Lasso Regression on\\npage 139) you can still use Gradient Descent using any subderivative at t = 1 (i.e., any\\nvalue between 1 and 0).\\n174 | Chapter 5: Support Vector Machines',\n",
       "  '8 Incremental and Decremental Support Vector Machine Learning,  G. Cauwenberghs, T. Poggio (2001).\\n9 Fast Kernel Classifiers with Online and Active Learning, A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMsfor example, using Incre\\nmental and Decremental SVM Learning8 or Fast Kernel Classifiers with Online and\\nActive Learning. 9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II).\\nExercises\\n1. What is the fundamental idea behind Support Vector Machines?\\n2. What is a support vector?\\n3. Why is it important to scale the inputs when using SVMs?\\n4. Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5. Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease  (gamma)? What about C?\\n7. How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175',\n",
       "  'want to tune the hyperparameters using small validation sets to speed up the pro\\ncess. What accuracy can you reach?\\n10. Train an SVM regressor on the California housing dataset.\\nSolutions to these exercises are available in ???.\\n176 | Chapter 5: Support Vector Machines',\n",
       "  'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees are versatile Machine Learning algorithms that can per\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap\\nter 2 you trained a DecisionTreeRegressor model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, lets just build one and take a look at how it makes pre\\ndictions. The following code trains a DecisionTreeClassifier on the iris dataset\\n(see Chapter 4):\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n177',\n",
       "  '1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/.\\niris = load_iris()\\nX = iris.data[:, 2:] # petal length and width\\ny = iris.target\\ntree_clf = DecisionTreeClassifier(max_depth=2)\\ntree_clf.fit(X, y)\\nY ou can visualize the trained Decision Tree by first using the export_graphviz() \\nmethod to output a graph definition file called iris_tree.dot:\\nfrom sklearn.tree import export_graphviz\\nexport_graphviz(\\n        tree_clf,\\n        out_file=image_path(\"iris_tree.dot\"),\\n        feature_names=iris.feature_names[2:],\\n        class_names=iris.target_names,\\n        rounded=True,\\n        filled=True\\n    )\\nThen you can convert this .dot file to a variety of formats such as PDF or PNG using\\nthe dot command-line tool from the graphviz package.1 This command line converts\\nthe .dot file to a .png image file:\\n$ dot -Tpng iris_tree.dot -o iris_tree.png\\nY our first decision tree looks like Figure 6-1.\\n178 | Chapter 6: Decision Trees',\n",
       "  'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLets see how the tree represented in Figure 6-1 makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node (depth 0, at the\\ntop): this node asks whether the flowers petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the roots left child node (depth 1, left). In this case, it is a leaf\\nnode (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa (class=setosa).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the roots right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). Its really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they dont require feature\\nscaling or centering at all.\\nMaking Predictions | 179',\n",
       "  'A nodes samples attribute counts how many training instances it applies to. For\\nexample, 100 training instances have a petal length greater than 2.45 cm (depth 1,\\nright), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A\\nnodes value attribute tells you how many training instances of each class this node\\napplies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\\nVersicolor, and 45 Iris-Virginica. Finally, a nodes gini attribute measures its impur\\nity: a node is pure ( gini=0) if all training instances it applies to belong to the same\\nclass. For example, since the depth-1 left node applies only to Iris-Setosa training\\ninstances, it is pure and its gini score is 0. Equation 6-1 shows how the training algo\\nrithm computes the gini score Gi of the i th node. For example, the depth-2 left node\\nhas a gini score equal to 1  (0/54) 2  (49/54) 2  (5/54) 2  0.168. Another impurity\\nmeasure is discussed shortly.\\nEquation 6-1. Gini impurity\\nGi = 1  \\nk = 1\\nn\\npi, k\\n2\\n pi,k is the ratio of class k instances among the training instances in the ith node.\\nScikit-Learn uses the CART algorithm, which produces only binary\\ntrees: nonleaf nodes always have two children (i.e., questions only\\nhave yes/no answers). However, other algorithms such as ID3 can\\nproduce Decision Trees with nodes that have more than two chil\\ndren.\\nFigure 6-2 shows this Decision Trees decision boundaries. The thick vertical line rep\\nresents the decision boundary of the root node (depth 0): petal length = 2.45 cm.\\nSince the left area is pure (only Iris-Setosa), it cannot be split any further. However,\\nthe right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm\\n(represented by the dashed line). Since max_depth was set to 2, the Decision Tree\\nstops right there. However, if you set max_depth to 3, then the two depth-2 nodes\\nwould each add another decision boundary (represented by the dotted lines).\\n180 | Chapter 6: Decision Trees',\n",
       "  'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter\\npret. Such models are often called white box models. In contrast, as we will see, Ran\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that persons eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Lets check\\nthis:\\n>>> tree_clf.predict_proba([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181',\n",
       "  '>>> tree_clf.predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called growing trees). The idea is really quite simple: the algo\\nrithm first splits the training set in two subsets using a single feature k and a thres\\nhold tk (e.g., petal length  2.45 cm). How does it choose k and tk? It searches for the\\npair (k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2.\\nEquation 6-2. CART cost function for classification\\nJ k, tk =\\nmleft\\nm Gleft +\\nmright\\nm Gright\\nwhere\\nGleft/right measures the impurity of the left/right subset,\\nmleft/right is the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea\\nches the maximum depth (defined by the max_depth hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split, min_sam\\nples_leaf, min_weight_fraction_leaf, and max_leaf_nodes).\\n182 | Chapter 6: Decision Trees',\n",
       "  '2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques\\ntion is whether or not P = NP . If P  NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4 A reduction of entropy is often called an information gain.\\nAs you can see, the CART algorithm is a greedy algorithm: it greed\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete problem:2 it requires O(exp(m)) time, making the prob\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a reasonably good solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes. 3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features is set)\\non all samples at each node. This results in a training complexity of O(n  m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True), but this slows down training con\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy impurity\\nmeasure instead by setting the criterion hyperparameter to \"entropy\". The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannons information theory, where it measures\\nthe average information content of a message: 4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a sets\\nComputational Complexity | 183',\n",
       "  '5 See Sebastian Raschkas interesting analysis for more details.\\nentropy is zero when it contains instances of only one class. Equation 6-3 shows the\\ndefinition of the entropy of the i th node. For example, the depth-2 left node in\\nFigure 6-1 has an entropy equal to  49\\n54 log2\\n49\\n54  5\\n54 log2\\n5\\n54   0.445.\\nEquation 6-3. Entropy\\nHi =  \\nk = 1\\npi, k  0\\nn\\npi, k log2 pi, k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel, not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Trees freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth hyperparameter (the default value is None, which means unlimited).\\nReducing max_depth will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split (the minimum number of sam\\nples a node must have before it can be split), min_samples_leaf (the minimum num\\nber of samples a leaf node must have), min_weight_fraction_leaf (same as\\nmin_samples_leaf but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees',\n",
       "  'instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas\\ning min_* hyperparameters or reducing max_* hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant. Stan\\ndard statistical tests, such as the 2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis). If this probability, called the p-\\nvalue, is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4. It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Lets build a regres\\nsion tree using Scikit-Learns DecisionTreeRegressor class, training it on a noisy\\nquadratic dataset with max_depth=2:\\nfrom sklearn.tree import DecisionTreeRegressor\\nRegression | 185',\n",
       "  'tree_reg = DecisionTreeRegressor(max_depth=2)\\ntree_reg.fit(X, y)\\nThe resulting tree is represented on Figure 6-4.\\nFigure 6-4. A Decision Tree for regression\\nThis tree looks very similar to the classification tree you built earlier. The main differ\\nence is that instead of predicting a class in each node, it predicts a value. For example,\\nsuppose you want to make a prediction for a new instance with x1 = 0.6. Y ou traverse\\nthe tree starting at the root, and you eventually reach the leaf node that predicts\\nvalue=0.1106. This prediction is simply the average target value of the 110 training\\ninstances associated to this leaf node. This prediction results in a Mean Squared Error\\n(MSE) equal to 0.0151 over these 110 instances.\\nThis models predictions are represented on the left of Figure 6-5 . If you set\\nmax_depth=3, you get the predictions represented on the right. Notice how the pre\\ndicted value for each region is always the average target value of the instances in that\\nregion. The algorithm splits each region in a way that makes most training instances\\nas close as possible to that predicted value.\\n186 | Chapter 6: Decision Trees',\n",
       "  'Figure 6-5. Predictions of two Decision Tree regression models\\nThe CART algorithm works mostly the same way as earlier, except that instead of try\\ning to split the training set in a way that minimizes impurity, it now tries to split the\\ntraining set in a way that minimizes the MSE. Equation 6-4 shows the cost function\\nthat the algorithm tries to minimize.\\nEquation 6-4. CART cost function for regression\\nJ k, tk =\\nmleft\\nm MSEleft +\\nmright\\nm MSEright where\\nMSEnode = \\ni  node\\nynode  y i 2\\nynode = 1\\nmnode\\n\\ni  node\\ny i\\nJust like for classification tasks, Decision Trees are prone to overfitting when dealing\\nwith regression tasks. Without any regularization (i.e., using the default hyperpara\\nmeters), you get the predictions on the left of Figure 6-6. It is obviously overfitting\\nthe training set very badly. Just setting min_samples_leaf=10 results in a much more\\nreasonable model, represented on the right of Figure 6-6.\\nFigure 6-6. Regularizing a Decision Tree regressor\\nRegression | 187',\n",
       "  '6 It randomly selects the set of features to evaluate at each node.\\nInstability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7 shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45, the decision boundary looks unneces\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob\\nlem is to use PCA (see Chapter 8), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8. As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic 6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state hyperparameter).\\n188 | Chapter 6: Decision Trees',\n",
       "  'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1. What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2. Is a nodes Gini impurity generally lower or greater than its parents? Is it gener\\nally lower/greater, or always lower/greater?\\n3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth?\\n4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5. If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6. If your training set contains 100,000 instances, will setting presort=True speed\\nup training?\\n7. Train and fine-tune a Decision Tree for the moons dataset.\\na. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\\nb. Split it into a training set and a test set using train_test_split().\\nExercises | 189',\n",
       "  'c. Use grid search with cross-validation (with the help of the GridSearchCV\\nclass) to find good hyperparameter values for a DecisionTreeClassifier. \\nHint: try various values for max_leaf_nodes.\\nd. Train it on the full training set using these hyperparameters, and measure\\nyour models performance on the test set. Y ou should get roughly 85% to 87%\\naccuracy.\\n8. Grow a forest.\\na. Continuing the previous exercise, generate 1,000 subsets of the training set,\\neach containing 100 instances selected randomly. Hint: you can use Scikit-\\nLearns ShuffleSplit class for this.\\nb. Train one Decision Tree on each subset, using the best hyperparameter values\\nfound above. Evaluate these 1,000 Decision Trees on the test set. Since they\\nwere trained on smaller sets, these Decision Trees will likely perform worse\\nthan the first Decision Tree, achieving only about 80% accuracy.\\nc. Now comes the magic. For each test set instance, generate the predictions of\\nthe 1,000 Decision Trees, and keep only the most frequent prediction (you can\\nuse SciPys mode() function for this). This gives you majority-vote predictions\\nover the test set.\\nd. Evaluate these predictions on the test set: you should obtain a slightly higher\\naccuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\\nyou have trained a Random Forest classifier!\\nSolutions to these exercises are available in ???.\\n190 | Chapter 6: Decision Trees',\n",
       "  'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan experts answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre\\ndictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\\nEnsemble Learning algorithm is called an Ensemble method.\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, \\nand despite its simplicity, this is one of the most powerful Machine Learning algo\\nrithms available today.\\nMoreover, as we discussed in Chapter 2, you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn\\ning competitions often involve several Ensemble methods (most famously in the Net\\nflix Prize competition).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag\\nging, boosting, stacking, and a few others. We will also explore Random Forests.\\n191',\n",
       "  'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi\\nfier is called a hard voting classifier (see Figure 7-2).\\nFigure 7-2. Hard voting classifier predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\\nbest classifier in the ensemble. In fact, even if each classifier is a weak learner (mean\\ning it does only slightly better than random guessing), the ensemble can still be a\\nstrong learner (achieving high accuracy), provided there are a sufficient number of\\nweak learners and they are sufficiently diverse.\\nHow is this possible? The following analogy can help shed some light on this mystery.\\nSuppose you have a slightly biased coin that has a 51% chance of coming up heads,\\nand 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\\nmore or less 510 heads and 490 tails, and hence a majority of heads. If you do the\\nmath, you will find that the probability of obtaining a majority of heads after 1,000\\ntosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\\nwith 10,000 tosses, the probability climbs over 97%). This is due to the law of large\\nnumbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the\\nprobability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. Y ou can\\nsee that as the number of tosses increases, the ratio of heads approaches 51%. Eventu\\nally all 10 series end up so close to 51% that they are consistently above 50%.\\nFigure 7-3. The law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are individ\\nually correct only 51% of the time (barely better than random guessing). If you pre\\ndict the majority voted class, you can hope for up to 75% accuracy! However, this is\\nonly true if all classifiers are perfectly independent, making uncorrelated errors,\\nwhich is clearly not the case since they are trained on the same data. They are likely to\\nmake the same types of errors, so there will be many majority votes for the wrong\\nclass, reducing the ensembles accuracy.\\nVoting Classifiers | 193',\n",
       "  'Ensemble methods work best when the predictors are as independ\\nent from one another as possible. One way to get diverse classifiers\\nis to train them using very different algorithms. This increases the\\nchance that they will make very different types of errors, improving\\nthe ensembles accuracy.\\nThe following code creates and trains a voting classifier in Scikit-Learn, composed of\\nthree diverse classifiers (the training set is the moons dataset, introduced in Chap\\nter 5):\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nlog_clf = LogisticRegression()\\nrnd_clf = RandomForestClassifier()\\nsvm_clf = SVC()\\nvoting_clf = VotingClassifier(\\n    estimators=[(\\'lr\\', log_clf), (\\'rf\\', rnd_clf), (\\'svc\\', svm_clf)],\\n    voting=\\'hard\\')\\nvoting_clf.fit(X_train, y_train)\\nLets look at each classifiers accuracy on the test set:\\n>>> from sklearn.metrics import accuracy_score\\n>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\\n...     clf.fit(X_train, y_train)\\n...     y_pred = clf.predict(X_test)\\n...     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\\n...\\nLogisticRegression 0.864\\nRandomForestClassifier 0.896\\nSVC 0.888\\nVotingClassifier 0.904\\nThere you have it! The voting classifier slightly outperforms all the individual classifi\\ners.\\nIf all classifiers are able to estimate class probabilities (i.e., they have a pre\\ndict_proba() method), then you can tell Scikit-Learn to predict the class with the\\nhighest class probability, averaged over all the individual classifiers. This is called soft\\nvoting. It often achieves higher performance than hard voting because it gives more\\nweight to highly confident votes. All you need to do is replace voting=\"hard\" with\\nvoting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\\nnot the case of the SVC class by default, so you need to set its probability hyperpara\\nmeter to True (this will make the SVC class use cross-validation to estimate class prob\\nabilities, slowing down training, and it will add a predict_proba() method). If you\\n194 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '1 Bagging Predictors,  L. Breiman (1996).\\n2 In statistics, resampling with replacement is called bootstrapping.\\n3 Pasting small votes for classification in large databases and on-line,  L. Breiman (1999).\\nmodify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without replacement, it is called\\npasting.3\\nIn other words, both bagging and pasting allow training instances to be sampled sev\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4.\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195',\n",
       "  '4 Bias and variance were introduced in Chapter 4.\\n5 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples.\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance. 4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier class (or BaggingRegressor for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False). The n_jobs param\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nbag_clf = BaggingClassifier(\\n    DecisionTreeClassifier(), n_estimators=500,\\n    max_samples=100, bootstrap=True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba\\nbilities (i.e., if it has a predict_proba() method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5 compares the decision boundary of a single Decision Tree with the deci\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensembles predictions will likely\\ngeneralize much better than the single Decision Trees predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '6 As m grows, this ratio approaches 1  exp(1)  63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensembles variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier samples m\\ntraining instances with replacement ( bootstrap=True), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_ variable:\\n>>> bag_clf = BaggingClassifier(\\n...     DecisionTreeClassifier(), n_estimators=500,\\n...     bootstrap=True, n_jobs=-1, oob_score=True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197',\n",
       "  '7 Ensembles on Random Patches,  G. Louppe and P . Geurts (2012).\\n8 The random subspace method for constructing decision forests,  Tin Kam Ho (1998).\\n>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier is likely to achieve about\\n90.1% accuracy on the test set. Lets verify this:\\n>>> from sklearn.metrics import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score(y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test setclose enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_ variable. In this case (since the base estimator has a pre\\ndict_proba() method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier class supports sampling the features as well. This is con\\ntrolled by two hyperparameters: max_features and bootstrap_features. They work\\nthe same way as max_samples and bootstrap, but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\\nples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\\ntures smaller than 1.0) is called the Random Subspaces method.8\\n198 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '9 Random Decision Forests,  T. Ho (1995).\\n10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\\n11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\\nFalse), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\\nfier with the provided hyperparameters).\\nSampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier and pass\\ning it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier has all the hyperparameters of a\\nDecisionTreeClassifier (to control how trees are grown), plus all the hyperpara\\nmeters of a BaggingClassifier to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier is\\nroughly equivalent to the previous RandomForestClassifier:\\nbag_clf = BaggingClassifier(\\n    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\\n    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\\nRandom Forests | 199',\n",
       "  '12 Extremely randomized trees,  P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learns ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\\nTreesRegressor class has the same API as the RandomForestRegressor class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier. Gen\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a features importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnodes weight is equal to the number of training samples that are associated with it\\n(see Chapter 6).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_ variable. For example, the following code\\ntrains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and\\noutputs each features importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '>>> from sklearn.datasets import load_iris\\n>>> iris = load_iris()\\n>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\\n>>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixels importance, you get the image represented in\\nFigure 7-6.\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201',\n",
       "  '13  A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,  Y oav Freund,\\nRobert E. Schapire (1997).\\n14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.\\nAdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Lets start with Ada\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic\\ntors focusing more and more on the hard cases. This is the technique used by Ada\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'get boosted. The second classifier therefore does a better job on these instances, and\\nso on. The plot on the right represents the same sequence of predictors except that\\nthe learning rate is halved (i.e., the misclassified instance weights are boosted half as\\nmuch at every iteration). As you can see, this sequential learning technique has some\\nsimilarities with Gradient Descent, except that instead of tweaking a single predictors\\nparameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\\ngradually making it better.\\nFigure 7-8. Decision boundaries of consecutive predictors\\nOnce all predictors are trained, the ensemble makes predictions very much like bag\\nging or pasting, except that predictors have different weights depending on their\\noverall accuracy on the weighted training set.\\nThere is one important drawback to this sequential learning techni\\nque: it cannot be parallelized (or only partially), since each predic\\ntor can only be trained after the previous predictor has been\\ntrained and evaluated. As a result, it does not scale as well as bag\\nging or pasting.\\nLets take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\\nset to 1m. A first predictor is trained and its weighted error rate r1 is computed on the\\ntraining set; see Equation 7-1.\\nEquation 7-1. Weighted error rate of the jth predictor\\nrj =\\n\\ni = 1\\ny j\\ni  y i\\nm\\nw i\\n\\ni = 1\\nm\\nw i\\nwhere y j\\ni is the jth predictors prediction for the ith instance.\\nBoosting | 203',\n",
       "  '15 The original AdaBoost algorithm does not use a learning rate hyperparameter.\\nThe predictors weight j is then computed using Equation 7-2, where  is the learn\\ning rate hyperparameter (defaults to 1). 15 The more accurate the predictor is, the\\nhigher its weight will be. If it is just guessing randomly, then its weight will be close to\\nzero. However, if it is most often wrong (i.e., less accurate than random guessing),\\nthen its weight will be negative.\\nEquation 7-2. Predictor weight\\nj =  log\\n1  rj\\nrj\\nNext the instance weights are updated using Equation 7-3: the misclassified instances\\nare boosted.\\nEquation 7-3. Weight update rule\\nfor i = 1, 2, , m\\nw i w i if yj\\ni = y i\\nw i exp j if yj\\ni  y i\\nThen all the instance weights are normalized (i.e., divided by i = 1\\nm w i ).\\nFinally, a new predictor is trained using the updated weights, and the whole process is\\nrepeated (the new predictors weight is computed, the instance weights are updated,\\nthen another predictor is trained, and so on). The algorithm stops when the desired\\nnumber of predictors is reached, or when a perfect predictor is found.\\nTo make predictions, AdaBoost simply computes the predictions of all the predictors\\nand weighs them using the predictor weights j. The predicted class is the one that\\nreceives the majority of weighted votes (see Equation 7-4).\\nEquation 7-4. AdaBoost predictions\\ny x = argmax\\nk\\n\\nj = 1\\ny j x = k\\nN\\nj where N is the number of predictors.\\n204 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '16 For more details, see Multi-Class AdaBoost,  J. Zhu et al. (2006).\\n17 First introduced in  Arcing the Edge,  L. Breiman (1997), and further developed in the paper Greedy Func\\ntion Approximation: A Gradient Boosting Machine,  Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands\\nfor Real), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps using\\nScikit-Learns AdaBoostClassifier class (as you might expect, there is also an Ada\\nBoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier class:\\nfrom sklearn.ensemble import AdaBoostClassifier\\nada_clf = AdaBoostClassifier(\\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\\n    algorithm=\"SAMME.R\", learning_rate=0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors made by the previous predictor.\\nLets go through a simple regression example using Decision Trees as the base predic\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, lets\\nfit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train\\ning set):\\nBoosting | 205',\n",
       "  'from sklearn.tree import DecisionTreeRegressor\\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\\ntree_reg1.fit(X, y)\\nNow train a second DecisionTreeRegressor on the residual errors made by the first\\npredictor:\\ny2 = y - tree_reg1.predict(X)\\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\\ntree_reg2.fit(X, y2)\\nThen we train a third regressor on the residual errors made by the second predictor:\\ny3 = y2 - tree_reg2.predict(X)\\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\\ntree_reg3.fit(X, y3)\\nNow we have an ensemble containing three trees. It can make predictions on a new\\ninstance simply by adding up the predictions of all the trees:\\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\\nFigure 7-9 represents the predictions of these three trees in the left column, and the\\nensembles predictions in the right column. In the first row, the ensemble has just one\\ntree, so its predictions are exactly the same as the first trees predictions. In the second\\nrow, a new tree is trained on the residual errors of the first tree. On the right you can\\nsee that the ensembles predictions are equal to the sum of the predictions of the first\\ntwo trees. Similarly, in the third row another tree is trained on the residual errors of\\nthe second tree. Y ou can see that the ensembles predictions gradually get better as\\ntrees are added to the ensemble.\\nA simpler way to train GBRT ensembles is to use Scikit-Learns GradientBoostingRe\\ngressor class. Much like the RandomForestRegressor class, it has hyperparameters to\\ncontrol the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\\nas well as hyperparameters to control the ensemble training, such as the number of\\ntrees (n_estimators). The following code creates the same ensemble as the previous\\none:\\nfrom sklearn.ensemble import GradientBoostingRegressor\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\\ngbrt.fit(X, y)\\n206 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'Figure 7-9. Gradient Boosting\\nThe learning_rate hyperparameter scales the contribution of each tree. If you set it\\nto a low value, such as 0.1, you will need more trees in the ensemble to fit the train\\ning set, but the predictions will usually generalize better. This is a regularization tech\\nnique called shrinkage. Figure 7-10  shows two GBRT ensembles trained with a low\\nlearning rate: the one on the left does not have enough trees to fit the training set,\\nwhile the one on the right has too many trees and overfits the training set.\\nBoosting | 207',\n",
       "  'Figure 7-10. GBRT ensembles with not enough predictors (left) and too many (right)\\nIn order to find the optimal number of trees, you can use early stopping (see Chap\\nter 4 ). A simple way to implement this is to use the staged_predict() method: it\\nreturns an iterator over the predictions made by the ensemble at each stage of train\\ning (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\\n120 trees, then measures the validation error at each stage of training to find the opti\\nmal number of trees, and finally trains another GBRT ensemble using the optimal\\nnumber of trees:\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import mean_squared_error\\nX_train, X_val, y_train, y_val = train_test_split(X, y)\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\\ngbrt.fit(X_train, y_train)\\nerrors = [mean_squared_error(y_val, y_pred)\\n          for y_pred in gbrt.staged_predict(X_val)]\\nbst_n_estimators = np.argmin(errors)\\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\\ngbrt_best.fit(X_train, y_train)\\nThe validation errors are represented on the left of Figure 7-11, and the best models\\npredictions are represented on the right.\\n208 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'Figure 7-11. Tuning the number of trees using early stopping\\nIt is also possible to implement early stopping by actually stopping training early\\n(instead of training a large number of trees first and then looking back to find the\\noptimal number). Y ou can do so by setting warm_start=True, which makes Scikit-\\nLearn keep existing trees when the fit() method is called, allowing incremental\\ntraining. The following code stops training when the validation error does not\\nimprove for five iterations in a row:\\ngbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\\nmin_val_error = float(\"inf\")\\nerror_going_up = 0\\nfor n_estimators in range(1, 120):\\n    gbrt.n_estimators = n_estimators\\n    gbrt.fit(X_train, y_train)\\n    y_pred = gbrt.predict(X_val)\\n    val_error = mean_squared_error(y_val, y_pred)\\n    if val_error < min_val_error:\\n        min_val_error = val_error\\n        error_going_up = 0\\n    else:\\n        error_going_up += 1\\n        if error_going_up == 5:\\n            break  # early stopping\\nThe GradientBoostingRegressor class also supports a subsample hyperparameter,\\nwhich specifies the fraction of training instances to be used for training each tree. For\\nexample, if subsample=0.25, then each tree is trained on 25% of the training instan\\nces, selected randomly. As you can probably guess by now, this trades a higher bias\\nfor a lower variance. It also speeds up training considerably. This technique is called\\nStochastic Gradient Boosting.\\nBoosting | 209',\n",
       "  '18 Stacked Generalization,  D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss hyperparameter (see Scikit-Learns\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost, which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community (DMLC), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoosts API is quite similar to Scikit-Learns:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking (short for\\nstacked generalization).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy dont we train a model to perform this aggregation? Figure 7-12 shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender, or a meta learner) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  '19 Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called stacking, while using a\\nhold-out set is called blending. However, for many people these terms are synonymous.\\nFigure 7-12. Aggregating predictions using a blending predictor\\nTo train the blender, a common approach is to use a hold-out set.19 Lets see how it\\nworks. First, the training set is split in two subsets. The first subset is used to train the\\npredictors in the first layer (see Figure 7-13).\\nFigure 7-13. Training the first layer\\nNext, the first layer predictors are used to make predictions on the second (held-out)\\nset (see Figure 7-14). This ensures that the predictions are clean,  since the predictors\\nnever saw these instances during training. Now for each instance in the hold-out set\\nStacking | 211',\n",
       "  'there are three predicted values. We can create a new training set using these predic\\nted values as input features (which makes this new training set three-dimensional),\\nand keeping the target values. The blender is trained on this new training set, so it\\nlearns to predict the target value given the first layers predictions.\\nFigure 7-14. Training the blender\\nIt is actually possible to train several different blenders this way (e.g., one using Lin\\near Regression, another using Random Forest Regression, and so on): we get a whole\\nlayer of blenders. The trick is to split the training set into three subsets: the first one is\\nused to train the first layer, the second one is used to create the training set used to\\ntrain the second layer (using predictions made by the predictors of the first layer),\\nand the third one is used to create the training set to train the third layer (using pre\\ndictions made by the predictors of the second layer). Once this is done, we can make\\na prediction for a new instance by going through each layer sequentially, as shown in\\nFigure 7-15.\\n212 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew (available at https://github.com/\\nviisar/brew).\\nExercises\\n1. If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2. What is the difference between hard and soft voting classifiers?\\n3. Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4. What is the benefit of out-of-bag evaluation?\\n5. What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran\\ndom Forests?\\n6. If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213',\n",
       "  '7. If your Gradient Boosting ensemble overfits the training set, should you increase\\nor decrease the learning rate?\\n8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\\nvalidation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val\\nidation, and 10,000 for testing). Then train various classifiers, such as a Random\\nForest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine\\nthem into an ensemble that outperforms them all on the validation set, using a\\nsoft or hard voting classifier. Once you have found one, try it on the test set. How\\nmuch better does it perform compared to the individual classifiers?\\n9. Run the individual classifiers from the previous exercise to make predictions on\\nthe validation set, and create a new training set with the resulting predictions:\\neach training instance is a vector containing the set of predictions from all your\\nclassifiers for an image, and the target is the images class. Train a classifier on\\nthis new training set. Congratulations, you have just trained a blender, and\\ntogether with the classifiers they form a stacking ensemble! Now lets evaluate the\\nensemble on the test set. For each image in the test set, make predictions with all\\nyour classifiers, then feed the predictions to the blender to get the ensembles pre\\ndictions. How does it compare to the voting classifier you trained earlier?\\nSolutions to these exercises are available in ???.\\n214 | Chapter 7: Ensemble Learning and Random Forests',\n",
       "  'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality.\\nFortunately, in real-world problems, it is often possible to reduce the number of fea\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3): the pixels on the image bor\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6 confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215',\n",
       "  '1 Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per\\nform slightly worse. It also makes your pipelines a bit more com\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per\\nformance (but in general it wont; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions 1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction',\n",
       "  '2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd\\nBoy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.\\n3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\\nin their coffee), if you consider enough dimensions.\\nFigure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\\nIt turns out that many things behave very differently in high-dimensional space. For\\nexample, if you pick a random point in a unit square (a 1  1 square), it will have only\\nabout a 0.4% chance of being located less than 0.001 from a border (in other words, it\\nis very unlikely that a random point will be extreme along any dimension). But in a\\n10,000-dimensional unit hypercube (a 1  1    1 cube, with ten thousand 1s), this\\nprobability is greater than 99.999999%. Most points in a high-dimensional hypercube\\nare very close to the border.3\\nHere is a more troublesome difference: if you pick two points randomly in a unit\\nsquare, the distance between these two points will be, on average, roughly 0.52. If you\\npick two random points in a unit 3D cube, the average distance will be roughly 0.66.\\nBut what about two points picked randomly in a 1,000,000-dimensional hypercube?\\nWell, the average distance, believe it or not, will be about 408.25 (roughly\\n1, 000, 000/6)! This is quite counterintuitive: how can two points be so far apart\\nwhen they both lie within the same unit hypercube? This fact implies that high-\\ndimensional datasets are at risk of being very sparse: most training instances are\\nlikely to be far away from each other. Of course, this also means that a new instance\\nwill likely be far away from any training instance, making predictions much less relia\\nble than in lower dimensions, since they will be based on much larger extrapolations.\\nIn short, the more dimensions the training set has, the greater the risk of overfitting\\nit.\\nIn theory, one solution to the curse of dimensionality could be to increase the size of\\nthe training set to reach a sufficient density of training instances. Unfortunately, in\\npractice, the number of training instances required to reach a given density grows\\nexponentially with the number of dimensions. With just 100 features (much less than\\nThe Curse of Dimensionality | 217',\n",
       "  'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, lets take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace of the high-dimensional space. This\\nsounds very abstract, so lets look at an example. In Figure 8-2 you can see a 3D data\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3.\\nTa-da! We have just reduced the datasets dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction',\n",
       "  'Figure 8-3. The new 2D dataset after projection\\nHowever, projection is not always the best approach to dimensionality reduction. In\\nmany cases the subspace may twist and turn, such as in the famous Swiss roll toy data\\nset represented in Figure 8-4.\\nFigure 8-4. Swiss roll dataset\\nMain Approaches for Dimensionality Reduction | 219',\n",
       "  'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5. However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5.\\nFigure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold on which\\nthe training instances lie; this is called Manifold Learning. It relies on the manifold\\nassumption, also called the manifold hypothesis, which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction',\n",
       "  'boundary would be fairly complex, but in the 2D unrolled manifold space (on the\\nright), the decision boundary is a simple straight line.\\nHowever, this assumption does not always hold. For example, in the bottom row of\\nFigure 8-6, the decision boundary is located at x1 = 5. This decision boundary looks\\nvery simple in the original 3D space (a vertical plane), but it looks more complex in\\nthe unrolled manifold (a collection of four independent line segments).\\nIn short, if you reduce the dimensionality of your training set before training a\\nmodel, it will usually speed up training, but it may not always lead to a better or sim\\npler solution; it all depends on the dataset.\\nHopefully you now have a good sense of what the curse of dimensionality is and how\\ndimensionality reduction algorithms can fight it, especially when the manifold\\nassumption holds. The rest of this chapter will go through some of the most popular\\nalgorithms.\\nFigure 8-6. The decision boundary may not always be simpler with lower dimensions\\nMain Approaches for Dimensionality Reduction | 221',\n",
       "  '4 On Lines and Planes of Closest Fit to Systems of Points in Space,  K. Pearson (1901).\\nPCA\\nPrincipal Component Analysis (PCA) is by far the most popular dimensionality reduc\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2.\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre\\nsented on the left of Figure 8-7, along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA.4\\n222 | Chapter 8: Dimensionality Reduction',\n",
       "  'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train\\ning set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so onas many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the i th axis is called the i th principal component (PC). In\\nFigure 8-7, the 1 st PC is c1 and the 2 nd PC is c2. In Figure 8-2 the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U  VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1.\\nEquation 8-1. Principal components matrix\\nV =\\n  \\nc1 c2  cn\\n  \\nThe following Python code uses NumPys svd() function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered)\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223',\n",
       "  'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learns PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre\\nceding example), or if you use other libraries, dont forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2 the\\n3D dataset is projected down to the 2D plane defined by the first two principal com\\nponents, preserving a large part of the datasets variance. As a result, the 2D projec\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2.\\nEquation 8-2. Projecting the training set down to d dimensions\\nXdproj = XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered.dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learns PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition import PCA\\npca = PCA(n_components = 2)\\nX2D = pca.fit_transform(X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo\\nnents using the components_ variable (note that it contains the PCs as horizontal vec\\n224 | Chapter 8: Dimensionality Reduction',\n",
       "  'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio of each prin\\ncipal component, available via the explained_variance_ratio_ variable. It indicates\\nthe proportion of the datasets variance that lies along the axis of each principal com\\nponent. For example, lets look at the explained variance ratios of the first two compo\\nnents of the 3D dataset represented in Figure 8-2:\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the datasets variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen\\nsionality for data visualizationin that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training sets\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_)\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components=0.95)\\nX_reduced = pca.fit_transform(X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225',\n",
       "  'dimensionality down to about 100 dimensions wouldnt lose too much explained var\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this wont give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error. For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform() method to decompress it back to 784 dimensions.\\nFigure 8-9 shows a few digits from the original training set (on the left), and the cor\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components = 154)\\nX_reduced = pca.fit_transform(X_train)\\nX_recovered = pca.inverse_transform(X_reduced)\\n226 | Chapter 8: Dimensionality Reduction',\n",
       "  'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3.\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered = XdprojWd\\nT\\nRandomized PCA\\nIf you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a sto\\nchastic algorithm called Randomized PCA that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m  d2) + O(d3),\\ninstead of O(m  n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\\nX_reduced = rnd_pca.fit_transform(X_train)\\nBy default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver hyperparameter to \"full\".\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227',\n",
       "  '5 Scikit-Learn uses the algorithm described in Incremental Learning for Robust Visual Tracking,  D. Ross et al.\\n(2007).\\nuseful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPys\\narray_split() function) and feeds them to Scikit-Learns IncrementalPCA class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit() method with each mini-batch\\nrather than the fit() method with the whole training set:\\nfrom sklearn.decomposition import IncrementalPCA\\nn_batches = 100\\ninc_pca = IncrementalPCA(n_components=154)\\nfor X_batch in np.array_split(X_train, n_batches):\\n    inc_pca.partial_fit(X_batch)\\nX_reduced = inc_pca.transform(X_train)\\nAlternatively, you can use NumPys memmap class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit() method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\\nbatch_size = m // n_batches\\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space.\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction',\n",
       "  '6 Kernel Principal Component Analysis,  B. Schlkopf, A. Smola, K. Mller (1999).\\nPCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learns KernelPCA class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition import KernelPCA\\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced = rbf_pca.fit_transform(X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.pipeline import Pipeline\\nKernel PCA | 229',\n",
       "  'clf = Pipeline([\\n        (\"kpca\", KernelPCA(n_components=2)),\\n        (\"log_reg\", LogisticRegression())\\n    ])\\nparam_grid = [{\\n        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\\n        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\\n    }]\\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\\ngrid_search.fit(X, y)\\nThe best kernel and hyperparameters are then available through the best_params_\\nvariable:\\n>>> print(grid_search.best_params_)\\n{\\'kpca__gamma\\': 0.043333333333333335, \\'kpca__kernel\\': \\'rbf\\'}\\nAnother approach, this time entirely unsupervised, is to select the kernel and hyper\\nparameters that yield the lowest reconstruction error. However, reconstruction is not\\nas easy as with linear PCA. Heres why. Figure 8-11 shows the original Swiss roll 3D\\ndataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF\\nkernel (top right). Thanks to the kernel trick, this is mathematically equivalent to\\nmapping the training set to an infinite-dimensional feature space (bottom right)\\nusing the feature map , then projecting the transformed training set down to 2D\\nusing linear PCA. Notice that if we could invert the linear PCA step for a given\\ninstance in the reduced space, the reconstructed point would lie in feature space, not\\nin the original space (e.g., like the one represented by an x in the diagram). Since the\\nfeature space is infinite-dimensional, we cannot compute the reconstructed point,\\nand therefore we cannot compute the true reconstruction error. Fortunately, it is pos\\nsible to find a point in the original space that would map close to the reconstructed\\npoint. This is called the reconstruction pre-image. Once you have this pre-image, you\\ncan measure its squared distance to the original instance. Y ou can then select the ker\\nnel and hyperparameters that minimize this reconstruction pre-image error.\\n230 | Chapter 8: Dimensionality Reduction',\n",
       "  '7 Scikit-Learn uses the algorithm based on Kernel Ridge Regression described in Gokhan H. Bakr, Jason\\nWeston, and Bernhard Scholkopf, Learning to Find Pre-images (Tubingen, Germany: Max Planck Institute\\nfor Biological Cybernetics, 2004).\\nFigure 8-11. Kernel PCA and the reconstruction pre-image error\\nY ou may be wondering how to perform this reconstruction. One solution is to train a\\nsupervised regression model, with the projected instances as the training set and the\\noriginal instances as the targets. Scikit-Learn will do this automatically if you set\\nfit_inverse_transform=True, as shown in the following code:7\\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\\n                    fit_inverse_transform=True)\\nX_reduced = rbf_pca.fit_transform(X)\\nX_preimage = rbf_pca.inverse_transform(X_reduced)\\nBy default, fit_inverse_transform=False and KernelPCA has no\\ninverse_transform() method. This method only gets created\\nwhen you set fit_inverse_transform=True.\\nKernel PCA | 231',\n",
       "  '8 Nonlinear Dimensionality Reduction by Locally Linear Embedding,  S. Roweis, L. Saul (2000).\\nY ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics import mean_squared_error\\n>>> mean_squared_error(X, X_preimage)\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding (LLE)8 is another very powerful nonlinear dimensionality\\nreduction (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learns LocallyLinearEmbedding class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12. As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\\nX_reduced = lle.fit_transform(X)\\n232 | Chapter 8: Dimensionality Reduction',\n",
       "  'Figure 8-12. Unrolled Swiss roll using LLE\\nHeres how LLE works: first, for each training instance x(i), the algorithm identifies its\\nk closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a\\nlinear function of these neighbors. More specifically, it finds the weights wi,j such that\\nthe squared distance between x(i) and j = 1\\nm wi, jx j  is as small as possible, assuming wi,j\\n= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the\\nconstrained optimization problem described in Equation 8-4, where W is the weight\\nmatrix containing all the weights wi,j. The second constraint simply normalizes the\\nweights for each training instance x(i).\\nLLE | 233',\n",
       "  'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW = argmin\\nW\\n\\ni = 1\\nm\\nx i  \\nj = 1\\nm\\nwi, jx j\\n2\\nsubject to\\nwi, j = 0 if x j is not one of the k c.n. of x i\\n\\nj = 1\\nm\\nwi, j = 1 for i = 1, 2, , m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and j = 1\\nm wi, jz j  to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5. It looks very similar to the first step, but instead of keeping the instan\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ = argmin\\nZ\\n\\ni = 1\\nm\\nz i  \\nj = 1\\nm\\nwi, jz j\\n2\\nScikit-Learns LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13).\\n234 | Chapter 8: Dimensionality Reduction',\n",
       "  '9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.\\n Isomap creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1. What are the main motivations for reducing a datasets dimensionality? What are\\nthe main drawbacks?\\n2. What is the curse of dimensionality?\\n3. Once a datasets dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235',\n",
       "  '6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\\nor Kernel PCA?\\n7. How can you evaluate the performance of a dimensionality reduction algorithm\\non your dataset?\\n8. Does it make any sense to chain two different dimensionality reduction algo\\nrithms?\\n9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set\\nand a test set (take the first 60,000 instances for training, and the remaining\\n10,000 for testing). Train a Random Forest classifier on the dataset and time how\\nlong it takes, then evaluate the resulting model on the test set. Next, use PCA to\\nreduce the datasets dimensionality, with an explained variance ratio of 95%.\\nTrain a new Random Forest classifier on the reduced dataset and see how long it\\ntakes. Was training much faster? Next evaluate the classifier on the test set: how\\ndoes it compare to the previous classifier?\\n10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the\\nresult using Matplotlib. Y ou can use a scatterplot using 10 different colors to rep\\nresent each images target class. Alternatively, you can write colored digits at the\\nlocation of each instance, or even plot scaled-down versions of the digit images\\nthemselves (if you plot all digits, the visualization will be too cluttered, so you\\nshould either draw a random sample or plot an instance only if no other instance\\nhas already been plotted at a close distance). Y ou should get a nice visualization\\nwith well-separated clusters of digits. Try using other dimensionality reduction\\nalgorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\\nSolutions to these exercises are available in ???.\\n236 | Chapter 8: Dimensionality Reduction',\n",
       "  'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as defective or normal . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic\\ntures. As a result, the labeled dataset will be quite small, and the classifiers perfor\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldnt it\\n237',\n",
       "  'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8, we looked at the most common unsupervised learning task: dimension\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n Clustering: the goal is to group similar instances together into clusters. This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n Anomaly detection: the objective is to learn what normal data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n Density estimation: this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly dont need an expert to identify groups of similar-\\nlooking objects. This is called clustering: it is the task of identifying similar instances\\nand assigning them to clusters, i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1: on the left is the iris dataset (introduced in\\nChapter 4), where each instances species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'represented here, and clustering algorithms can make good use of all features, so in\\nfact they identify the three clusters fairly well (e.g., using a Gaussian mixture model,\\nonly 5 instances out of 150 are assigned to the wrong cluster).\\nFigure 9-1. Classification (left) versus clustering (right)\\nClustering is used in a wide variety of applications, including:\\n For customer segmentation: you can cluster your customers based on their pur\\nchases, their activity on your website, and so on. This is useful to understand who\\nyour customers are and what they need, so you can adapt your products and\\nmarketing campaigns to each segment. For example, this can be useful in recom\\nmender systems to suggest content that other users in the same cluster enjoyed.\\n For data analysis: when analyzing a new dataset, it is often useful to first discover\\nclusters of similar instances, as it is often easier to analyze clusters separately.\\n As a dimensionality reduction technique: once a dataset has been clustered, it is\\nusually possible to measure each instances affinity with each cluster (affinity is\\nany measure of how well an instance fits into a cluster). Each instances feature\\nvector x can then be replaced with the vector of its cluster affinities. If there are k\\nclusters, then this vector is k dimensional. This is typically much lower dimen\\nsional than the original feature vector, but it can preserve enough information for\\nfurther processing.\\n For anomaly detection (also called outlier detection): any instance that has a low\\naffinity to all the clusters is likely to be an anomaly. For example, if you have clus\\ntered the users of your website based on their behavior, you can detect users with\\nunusual behavior, such as an unusual number of requests per second, and so on.\\nAnomaly detection is particularly useful in detecting defects in manufacturing, or\\nfor fraud detection.\\n For semi-supervised learning: if you only have a few labels, you could perform\\nclustering and propagate the labels to all the instances in the same cluster. This\\ncan greatly increase the amount of labels available for a subsequent supervised\\nlearning algorithm, and thus improve its performance.\\nClustering | 239',\n",
       "  '1 Least square quantization in PCM,  Stuart P . Lloyd. (1982).\\n For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this images cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n To segment an image: by clustering pixels according to their color, then replacing\\neach pixels color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid.\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2: you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula\\ntion, but it was only published outside of the company in 1982, in a paper titled\\nLeast square quantization in PCM .1 By then, in 1965, Edward W . Forgy had pub\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-2. An unlabeled dataset composed of five blobs of instances\\nLets train a K-Means clusterer on this dataset. It will try to find each blobs center and\\nassign each instance to the closest blob:\\nfrom sklearn.cluster import KMeans\\nk = 5\\nkmeans = KMeans(n_clusters=k)\\ny_pred = kmeans.fit_predict(X)\\nNote that you have to specify the number of clusters k that the algorithm must find.\\nIn this example, it is pretty obvious from looking at the data that k should be set to 5,\\nbut in general it is not that easy. We will discuss this shortly.\\nEach instance was assigned to one of the 5 clusters. In the context of clustering, an\\ninstances label is the index of the cluster that this instance gets assigned to by the\\nalgorithm: this is not to be confused with the class labels in classification (remember\\nthat clustering is an unsupervised learning task). The KMeans instance preserves a\\ncopy of the labels of the instances it was trained on, available via the labels_ instance\\nvariable:\\n>>> y_pred\\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\\n>>> y_pred is kmeans.labels_\\nTrue\\nWe can also take a look at the 5 centroids that the algorithm found:\\n>>> kmeans.cluster_centers_\\narray([[-2.80389616,  1.80117999],\\n       [ 0.20876306,  2.25551336],\\n       [-2.79290307,  2.79641063],\\n       [-1.46679593,  2.28585348],\\n       [-2.80037642,  1.30082566]])\\nOf course, you can easily assign new instances to the cluster whose centroid is closest:\\nClustering | 241',\n",
       "  '>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\\n>>> kmeans.predict(X_new)\\narray([1, 1, 2, 2], dtype=int32)\\nIf you plot the clusters decision boundaries, you get a Voronoi tessellation (see\\nFigure 9-3, where each centroid is represented with an X):\\nFigure 9-3. K-Means decision boundaries (Voronoi tessellation)\\nThe vast majority of the instances were clearly assigned to the appropriate cluster, but\\na few instances were probably mislabeled (especially near the boundary between the\\ntop left cluster and the central cluster). Indeed, the K-Means algorithm does not\\nbehave very well when the blobs have very different diameters since all it cares about\\nwhen assigning an instance to a cluster is the distance to the centroid.\\nInstead of assigning each instance to a single cluster, which is called hard clustering, it\\ncan be useful to just give each instance a score per cluster: this is called soft clustering.\\nFor example, the score can be the distance between the instance and the centroid, or\\nconversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis\\nFunction (introduced in Chapter 5 ). In the KMeans class, the transform() method\\nmeasures the distance from each instance to every centroid:\\n>>> kmeans.transform(X_new)\\narray([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\\n       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\\n       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\\n       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\\nIn this example, the first instance in X_new is located at a distance of 2.81 from the\\nfirst centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from\\nthe fourth centroid and 2.87 from the fifth centroid. If you have a high-dimensional\\ndataset and you transform it this way, you end up with a k-dimensional dataset: this\\ncan be a very efficient non-linear dimensionality reduction technique.\\n242 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  '2 This can be proven by pointing out that the mean squared distance between the instances and their closest\\ncentroid can only go down at each step.\\nThe K-Means Algorithm\\nSo how does the algorithm work? Well it is really quite simple. Suppose you were\\ngiven the centroids: you could easily label all the instances in the dataset by assigning\\neach of them to the cluster whose centroid is closest. Conversely, if you were given all\\nthe instance labels, you could easily locate all the centroids by computing the mean of\\nthe instances for each cluster. But you are given neither the labels nor the centroids,\\nso how can you proceed? Well, just start by placing the centroids randomly (e.g., by\\npicking k instances at random and using their locations as centroids). Then label the\\ninstances, update the centroids, label the instances, update the centroids, and so on\\nuntil the centroids stop moving. The algorithm is guaranteed to converge in a finite\\nnumber of steps (usually quite small), it will not oscillate forever 2. Y ou can see the\\nalgorithm in action in Figure 9-4 : the centroids are initialized randomly (top left),\\nthen the instances are labeled (top right), then the centroids are updated (center left),\\nthe instances are relabeled (center right), and so on. As you can see, in just 3 itera\\ntions the algorithm has reached a clustering that seems close to optimal.\\nFigure 9-4. The K-Means algorithm\\nClustering | 243',\n",
       "  'The computational complexity of the algorithm is generally linear\\nwith regards to the number of instances m, the number of clusters\\nk and the number of dimensions n. However, this is only true when\\nthe data has a clustering structure. If it does not, then in the worst\\ncase scenario the complexity can increase exponentially with the\\nnumber of instances. In practice, however, this rarely happens, and\\nK-Means is generally one of the fastest clustering algorithms.\\nUnfortunately, although the algorithm is guaranteed to converge, it may not converge\\nto the right solution (i.e., it may converge to a local optimum): this depends on the\\ncentroid initialization. For example, Figure 9-5 shows two sub-optimal solutions that\\nthe algorithm can converge to if you are not lucky with the random initialization step:\\nFigure 9-5. Sub-optimal solutions due to unlucky centroid initializations\\nLets look at a few ways you can mitigate this risk by improving the centroid initializa\\ntion.\\nCentroid Initialization Methods\\nIf you happen to know approximately where the centroids should be (e.g., if you ran\\nanother clustering algorithm earlier), then you can set the init hyperparameter to a\\nNumPy array containing the list of centroids, and set n_init to 1:\\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\\nAnother solution is to run the algorithm multiple times with different random initial\\nizations and keep the best solution. This is controlled by the n_init hyperparameter:\\nby default, it is equal to 10, which means that the whole algorithm described earlier\\nactually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution.\\nBut how exactly does it know which solution is the best? Well of course it uses a per\\nformance metric! It is called the models inertia: this is the mean squared distance\\nbetween each instance and its closest centroid. It is roughly equal to 223.3 for the\\nmodel on the left of Figure 9-5, 237.5 for the model on the right of Figure 9-5, and\\n211.6 for the model in Figure 9-3. The KMeans class runs the algorithm n_init times\\nand keeps the model with the lowest inertia: in this example, the model in Figure 9-3\\nwill be selected (unless we are very unlucky with n_init consecutive random initiali\\n244 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  '3 k-means\\\\++: The advantages of careful seeding,  David Arthur and Sergei Vassilvitskii (2006).\\n4 Using the Triangle Inequality to Accelerate k-Means,  Charles Elkan (2003).\\nzations). If you are curious, a models inertia is accessible via the inertia_ instance\\nvariable:\\n>>> kmeans.inertia_\\n211.59853725816856\\nThe score() method returns the negative inertia. Why negative? Well, it is because a\\npredictors score() method must always respect the \"great is better\" rule.\\n>>> kmeans.score(X)\\n-211.59853725816856\\nAn important improvement to the K-Means algorithm, called K-Means+\\\\+, was pro\\nposed in a 2006 paper by David Arthur and Sergei Vassilvitskii: 3 they introduced a\\nsmarter initialization step that tends to select centroids that are distant from one\\nanother, and this makes the K-Means algorithm much less likely to converge to a sub-\\noptimal solution. They showed that the additional computation required for the\\nsmarter initialization step is well worth it since it makes it possible to drastically\\nreduce the number of times the algorithm needs to be run to find the optimal solu\\ntion. Here is the K-Means++ initialization algorithm:\\n Take one centroid c(1), chosen uniformly at random from the dataset.\\n Take a new centroid c(i), choosing an instance x(i) with probability: Di2\\nj = 1\\nm D  j 2\\n where D(x(i)) is the distance between the instance x(i) and the closest\\ncentroid that was already chosen. This probability distribution ensures that\\ninstances further away from already chosen centroids are much more likely be\\nselected as centroids.\\n Repeat the previous step until all k centroids have been chosen.\\nThe KMeans class actually uses this initialization method by default. If you want to\\nforce it to use the original method (i.e., picking k instances randomly to define the\\ninitial centroids), then you can set the init hyperparameter to \"random\". Y ou will\\nrarely need to do this.\\nAccelerated K-Means and Mini-batch K-Means\\nAnother important improvement to the K-Means algorithm was proposed in a 2003\\npaper by Charles Elkan. 4 It considerably accelerates the algorithm by avoiding many\\nunnecessary distance calculations: this is achieved by exploiting the triangle inequal\\nClustering | 245',\n",
       "  '5 The triangle inequality is AC  AB + BC where A, B and C are three points, and AB, AC and BC are the\\ndistances between these points.\\n6 Web-Scale K-Means Clustering,  David Sculley (2010).\\nity (i.e., the straight line is always the shortest 5) and by keeping track of lower and\\nupper bounds for distances between instances and centroids. This is the algorithm\\nused by default by the KMeans class (but you can force it to use the original algorithm\\nby setting the algorithm hyperparameter to \"full\", although you probably will\\nnever need to).\\nY et another important variant of the K-Means algorithm was proposed in a 2010\\npaper by David Sculley. 6 Instead of using the full dataset at each iteration, the algo\\nrithm is capable of using mini-batches, moving the centroids just slightly at each iter\\nation. This speeds up the algorithm typically by a factor of 3 or 4 and makes it\\npossible to cluster huge datasets that do not fit in memory. Scikit-Learn implements\\nthis algorithm in the MiniBatchKMeans class. Y ou can just use this class like the\\nKMeans class:\\nfrom sklearn.cluster import MiniBatchKMeans\\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5)\\nminibatch_kmeans.fit(X)\\nIf the dataset does not fit in memory, the simplest option is to use the memmap class, as\\nwe did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch\\nat a time to the partial_fit() method, but this will require much more work, since\\nyou will need to perform multiple initializations and select the best one yourself (see\\nthe notebook for an example).\\nAlthough the Mini-batch K-Means algorithm is much faster than the regular K-\\nMeans algorithm, its inertia is generally slightly worse, especially as the number of\\nclusters increases. Y ou can see this in Figure 9-6 : the plot on the left compares the\\ninertias of Mini-batch K-Means and regular K-Means models trained on the previous\\ndataset using various numbers of clusters k. The difference between the two curves\\nremains fairly constant, but this difference becomes more and more significant as k\\nincreases, since the inertia becomes smaller and smaller. However, in the plot on the\\nright, you can see that Mini-batch K-Means is much faster than regular K-Means, and\\nthis difference increases with k.\\n246 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-6. Mini-batch K-Means vs K-Means: worse inertia as k increases (left) but\\nmuch faster (right)\\nFinding the Optimal Number of Clusters\\nSo far, we have set the number of clusters k to 5 because it was obvious by looking at\\nthe data that this is the correct number of clusters. But in general, it will not be so\\neasy to know how to set k, and the result might be quite bad if you set it to the wrong\\nvalue. For example, as you can see in Figure 9-7, setting k to 3 or 8 results in fairly\\nbad models:\\nFigure 9-7. Bad choices for the number of clusters\\nY ou might be thinking that we could just pick the model with the lowest inertia,\\nright? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much\\nhigher than for k=5 (which was 211.6), but with k=8, the inertia is just 119.1. The\\ninertia is not a good performance metric when trying to choose k since it keeps get\\nting lower as we increase k. Indeed, the more clusters there are, the closer each\\ninstance will be to its closest centroid, and therefore the lower the inertia will be. Lets\\nplot the inertia as a function of k (see Figure 9-8):\\nClustering | 247',\n",
       "  'Figure 9-8. Selecting the number of clusters k using the elbow rule\\nAs you can see, the inertia drops very quickly as we increase k up to 4, but then it\\ndecreases much more slowly as we keep increasing k. This curve has roughly the\\nshape of an arm, and there is an elbow at k=4 so if we did not know better, it would\\nbe a good choice: any lower value would be dramatic, while any higher value would\\nnot help much, and we might just be splitting perfectly good clusters in half for no\\ngood reason.\\nThis technique for choosing the best value for the number of clusters is rather coarse.\\nA more precise approach (but also more computationally expensive) is to use the sil\\nhouette score, which is the mean silhouette coefficient over all the instances. An instan\\nces silhouette coefficient is equal to ( b  a) / max(a, b) where a is the mean distance\\nto the other instances in the same cluster (it is the mean intra-cluster distance), and b\\nis the mean nearest-cluster distance, that is the mean distance to the instances of the\\nnext closest cluster (defined as the one that minimizes b, excluding the instances own\\ncluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to\\n+1 means that the instance is well inside its own cluster and far from other clusters,\\nwhile a coefficient close to 0 means that it is close to a cluster boundary, and finally a\\ncoefficient close to -1 means that the instance may have been assigned to the wrong\\ncluster. To compute the silhouette score, you can use Scikit-Learns silhou\\nette_score() function, giving it all the instances in the dataset, and the labels they\\nwere assigned:\\n>>> from sklearn.metrics import silhouette_score\\n>>> silhouette_score(X, kmeans.labels_)\\n0.655517642572828\\nLets compare the silhouette scores for different numbers of clusters (see Figure 9-9):\\n248 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-9. Selecting the number of clusters k using the silhouette score\\nAs you can see, this visualization is much richer than the previous one: in particular,\\nalthough it confirms that k=4 is a very good choice, it also underlines the fact that\\nk=5 is quite good as well, and much better than k=6 or 7. This was not visible when\\ncomparing inertias.\\nAn even more informative visualization is obtained when you plot every instances\\nsilhouette coefficient, sorted by the cluster they are assigned to and by the value of the\\ncoefficient. This is called a silhouette diagram (see Figure 9-10):\\nFigure 9-10. Silouhette analysis: comparing the silhouette diagrams for various values of\\nk\\nThe vertical dashed lines represent the silhouette score for each number of clusters.\\nWhen most of the instances in a cluster have a lower coefficient than this score (i.e., if\\nmany of the instances stop short of the dashed line, ending to the left of it), then the\\ncluster is rather bad since this means its instances are much too close to other clus\\nClustering | 249',\n",
       "  'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good  most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11 shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow lets look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Using clustering for image segmentation\\nImage segmentation is the task of partitioning an image into multiple segments. In\\nsemantic segmentation, all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving cars vision system, all pixels that are\\npart of a pedestrians image might be assigned to the pedestrian segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec\\ntures based on convolutional neural networks (see Chapter 14). Here, we are going to\\ndo something much simpler: color segmentation. We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, lets load the image (see the upper left image in Figure 9-12) using Matplotlibs\\nimread() function:\\n>>> from matplotlib.image import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\",\"clustering\",\"ladybug.png\"))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimensions size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread()). Some images may have less channels, such as gray\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixels color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And were done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters=8).fit(X)\\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\\nsegmented_img = segmented_img.reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12. Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybugs flashy red color fails to get a cluster of its own: it\\nClustering | 251',\n",
       "  'gets merged with colors from the environment. This is due to the fact that the lady\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now lets look at another application of clustering: pre\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, lets tackle\\nthe digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 88\\nimages representing digits 0 to 9. First, lets load the dataset:\\nfrom sklearn.datasets import load_digits\\nX_digits, y_digits = load_digits(return_X_y=True)\\nNow, lets split it into a training set and a test set:\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)\\nNext, lets fit a Logistic Regression model:\\nfrom sklearn.linear_model import LogisticRegression\\nlog_reg = LogisticRegression(random_state=42)\\nlog_reg.fit(X_train, y_train)\\nLets evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Okay, thats our baseline: 96.7% accuracy. Lets see if we can do better by using K-\\nMeans as a preprocessing step. We will create a pipeline that will first cluster the\\ntraining set into 50 clusters and replace the images with their distances to these 50\\nclusters, then apply a logistic regression model.\\nAlthough it is tempting to define the number of clusters to 10,\\nsince there are 10 different digits, it is unlikely to perform well,\\nbecause there are several different ways to write each digit.\\nfrom sklearn.pipeline import Pipeline\\npipeline = Pipeline([\\n    (\"kmeans\", KMeans(n_clusters=50)),\\n    (\"log_reg\", LogisticRegression()),\\n])\\npipeline.fit(X_train, y_train)\\nNow lets evaluate this classification pipeline:\\n>>> pipeline.score(X_test, y_test)\\n0.9822222222222222\\nHow about that? We almost divided the error rate by a factor of 2!\\nBut we chose the number of clusters k completely arbitrarily, we can surely do better.\\nSince K-Means is just a preprocessing step in a classification pipeline, finding a good\\nvalue for k is much simpler than earlier: theres no need to perform silhouette analysis\\nor minimize the inertia, the best value of k is simply the one that results in the best\\nclassification performance during cross-validation. Lets use GridSearchCV to find the\\noptimal number of clusters:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = dict(kmeans__n_clusters=range(2, 100))\\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\\ngrid_clf.fit(X_train, y_train)\\nLets look at best value for k, and the performance of the resulting pipeline:\\n>>> grid_clf.best_params_\\n{\\'kmeans__n_clusters\\': 90}\\n>>> grid_clf.score(X_test, y_test)\\n0.9844444444444445\\nWith k=90 clusters, we get a small accuracy boost, reaching 98.4% accuracy on the\\ntest set. Cool!\\nClustering | 253',\n",
       "  'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Lets train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled = 50\\nlog_reg = LogisticRegression()\\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Lets see how we can do\\nbetter. First, lets cluster the training set into 50 clusters, then for each cluster lets find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters=k)\\nX_digits_dist = kmeans.fit_transform(X_train)\\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\\nX_representative_digits = X_train[representative_digit_idx]\\nFigure 9-13 shows these 50 representative images:\\nFigure 9-13. Fifty representative digit images (one per cluster)\\nNow lets look at each image and manually label it:\\ny_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Lets see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_representative_digits, y_representative_digits)\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe\\n254 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'cially when it has to be done manually by experts, it is a good idea to label representa\\ntive instances rather than just random instances.\\nBut perhaps we can go one step further: what if we propagated the labels to all the\\nother instances in the same cluster? This is called label propagation:\\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\\nfor i in range(k):\\n    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\\nNow lets train the model again and look at its performance:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_train, y_train_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.9288888888888889\\nWe got a tiny little accuracy boost. Better than nothing, but not astounding. The\\nproblem is that we propagated each representative instances label to all the instances\\nin the same cluster, including the instances located close to the cluster boundaries,\\nwhich are more likely to be mislabeled. Lets see what happens if we only propagate\\nthe labels to the 20% of the instances that are closest to the centroids:\\npercentile_closest = 20\\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\\nfor i in range(k):\\n    in_cluster = (kmeans.labels_ == i)\\n    cluster_dist = X_cluster_dist[in_cluster]\\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\\n    above_cutoff = (X_cluster_dist > cutoff_distance)\\n    X_cluster_dist[in_cluster & above_cutoff] = -1\\npartially_propagated = (X_cluster_dist != -1)\\nX_train_partially_propagated = X_train[partially_propagated]\\ny_train_partially_propagated = y_train_propagated[partially_propagated]\\nNow lets train the model again on this partially propagated dataset:\\n>>> log_reg = LogisticRegression()\\n>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\\n>>> log_reg.score(X_test, y_test)\\n0.9422222222222222\\nNice! With just 50 labeled instances (only 5 examples per class on average!), we got\\n94.2% performance, which is pretty close to the performance of logistic regression on\\nthe fully labeled digits dataset (which was 96.7%). This is because the propagated\\nlabels are actually pretty good, their accuracy is very close to 99%:\\nClustering | 255',\n",
       "  '>>> np.mean(y_train_partially_propagated == y_train[partially_propagated])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning: this is when a human expert interacts with the learn\\ning algorithm, providing labels when the algorithm needs them. There are many dif\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling:\\n The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the models validation error, or the instances that differ\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, lets take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n For each instance, the algorithm counts how many instances are located within a\\nsmall distance  (epsilon) from it. This region is called the instances -\\nneighborhood.\\n If an instance has at least min_samples instances in its -neighborhood (includ\\ning itself), then it is considered a core instance. In other words, core instances are\\nthose that are located in dense regions.\\n All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  ' Any instance that is not a core instance and does not have one in its neighbor\\nhood is considered an anomaly.\\nThis algorithm works well if all the clusters are dense enough, and they are well sepa\\nrated by low-density regions. The DBSCAN class in Scikit-Learn is as simple to use as\\nyou might expect. Lets test it on the moons dataset, introduced in Chapter 5:\\nfrom sklearn.cluster import DBSCAN\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons(n_samples=1000, noise=0.05)\\ndbscan = DBSCAN(eps=0.05, min_samples=5)\\ndbscan.fit(X)\\nThe labels of all the instances are now available in the labels_ instance variable:\\n>>> dbscan.labels_\\narray([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])\\nNotice that some instances have a cluster index equal to -1: this means that they are\\nconsidered as anomalies by the algorithm. The indices of the core instances are avail\\nable in the core_sample_indices_ instance variable, and the core instances them\\nselves are available in the components_ instance variable:\\n>>> len(dbscan.core_sample_indices_)\\n808\\n>>> dbscan.core_sample_indices_\\narray([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\\n>>> dbscan.components_\\narray([[-0.02137124,  0.40618608],\\n       [-0.84192557,  0.53058695],\\n                  ...\\n       [-0.94355873,  0.3278936 ],\\n       [ 0.79419406,  0.60777171]])\\nThis clustering is represented in the left plot of Figure 9-14. As you can see, it identi\\nfied quite a lot of anomalies, plus 7 different clusters. How disappointing! Fortunately,\\nif we widen each instances neighborhood by increasing eps to 0.2, we get the cluster\\ning on the right, which looks perfect. Lets continue with this model.\\nFigure 9-14. DBSCAN clustering using two different neighborhood radiuses\\nClustering | 257',\n",
       "  'Somewhat surprisingly, the DBSCAN class does not have a predict() method,\\nalthough it has a fit_predict() method. In other words, it cannot predict which\\ncluster a new instance belongs to. The rationale for this decision is that several classi\\nfication algorithms could make sense here, and it is easy enough to train one, for\\nexample a KNeighborsClassifier:\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=50)\\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\\nNow, given a few new instances, we can predict which cluster they most likely belong\\nto, and even estimate a probability for each cluster. Note that we only trained them on\\nthe core instances, but we could also have chosen to train them on all the instances,\\nor all but the anomalies: this choice depends on the final task.\\n>>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\\n>>> knn.predict(X_new)\\narray([1, 0, 1, 0])\\n>>> knn.predict_proba(X_new)\\narray([[0.18, 0.82],\\n       [1.  , 0.  ],\\n       [0.12, 0.88],\\n       [1.  , 0.  ]])\\nThe decision boundary is represented on Figure 9-15  (the crosses represent the 4\\ninstances in X_new). Notice that since there is no anomaly in the KNNs training set,\\nthe classifier always chooses a cluster, even when that cluster is far away. However, it\\nis fairly straightforward to introduce a maximum distance, in which case the two\\ninstances that are far away from both clusters are classified as anomalies. To do this,\\nwe can use the kneighbors() method of the KNeighborsClassifier: given a set of\\ninstances, it returns the distances and the indices of the k nearest neighbors in the\\ntraining set (two matrices, each with k columns):\\n>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\\n>>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\\n>>> y_pred[y_dist > 0.2] = -1\\n>>> y_pred.ravel()\\narray([-1,  0,  1, -1])\\n258 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper\\nparameters (eps and min_samples). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learns implementation can\\nrequire up to O(m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n Agglomerative clustering: a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until theres just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus\\nters, it can capture clusters of various shapes, it produces a flexible and informa\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph()). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n Birch: this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259',\n",
       "  'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n Mean-shift: this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O(m2), so it is not suited for large datasets.\\n Affinity propagation: this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu\\ntational complexity of O(m2), so it is not suited for large datasets.\\n Spectral clustering: this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learns implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif\\nferent sizes.\\nNow lets dive into Gaussian mixture models, which can be used for density estima\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11. When\\nyou observe an instance, you know it was generated from one of the Gaussian distri\\n260 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  '7 Phi ( or ) is the 21st letter of the Greek alphabet.\\n8 Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on\\nplate notation.\\nbutions, but you are not told which one, and you do not know what the parameters of\\nthese distributions are.\\nThere are several GMM variants: in the simplest variant, implemented in the Gaus\\nsianMixture class, you must know in advance the number k of Gaussian distribu\\ntions. The dataset X is assumed to have been generated through the following\\nprobabilistic process:\\n For each instance, a cluster is picked randomly among k clusters. The probability\\nof choosing the jth cluster is defined by the clusters weight (j).7 The index of the\\ncluster chosen for the ith instance is noted z(i).\\n If z(i)=j, meaning the ith instance has been assigned to the jth cluster, the location\\nx(i) of this instance is sampled randomly from the Gaussian distribution with\\nmean (j) and covariance matrix (j). This is noted  i   j ,  j .\\nThis generative process can be represented as a graphical model (see Figure 9-16 ).\\nThis is a graph which represents the structure of the conditional dependencies\\nbetween random variables.\\nFigure 9-16. Gaussian mixture model\\nHere is how to interpret it:8\\n The circles represent random variables.\\n The squares represent fixed values (i.e., parameters of the model).\\nGaussian Mixtures | 261',\n",
       "  ' The large rectangles are called plates: they indicate that their content is repeated\\nseveral times.\\n The number indicated at the bottom right hand side of each plate indicates how\\nmany times its content is repeated, so there are m random variables z(i) (from z(1)\\nto z(m)) and m random variables x(i), and k means (j) and k covariance matrices\\n(j), but just one weight vector  (containing all the weights (1) to (k)).\\n Each variable z(i) is drawn from the categorical distribution with weights . Each\\nvariable x(i) is drawn from the normal distribution with the mean and covariance\\nmatrix defined by its cluster z(i).\\n The solid arrows represent conditional dependencies. For example, the probabil\\nity distribution for each random variable z(i) depends on the weight vector .\\nNote that when an arrow crosses a plate boundary, it means that it applies to all\\nthe repetitions of that plate, so for example the weight vector  conditions the\\nprobability distributions of all the random variables x(1) to x(m).\\n The squiggly arrow from z(i) to x(i) represents a switch: depending on the value of\\nz(i), the instance x(i) will be sampled from a different Gaussian distribution. For\\nexample, if z(i)=j, then  i   j ,  j .\\n Shaded nodes indicate that the value is known, so in this case only the random\\nvariables x(i) have known values: they are called observed variables. The unknown\\nrandom variables z(i) are called latent variables.\\nSo what can you do with such a model? Well, given the dataset X, you typically want\\nto start by estimating the weights  and all the distribution parameters (1) to (k) and\\n(1) to (k). Scikit-Learns GaussianMixture class makes this trivial:\\nfrom sklearn.mixture import GaussianMixture\\ngm = GaussianMixture(n_components=3, n_init=10)\\ngm.fit(X)\\nLets look at the parameters that the algorithm estimated:\\n>>> gm.weights_\\narray([0.20965228, 0.4000662 , 0.39028152])\\n>>> gm.means_\\narray([[ 3.39909717,  1.05933727],\\n       [-1.40763984,  1.42710194],\\n       [ 0.05135313,  0.07524095]])\\n>>> gm.covariances_\\narray([[[ 1.14807234, -0.03270354],\\n        [-0.03270354,  0.95496237]],\\n       [[ 0.63478101,  0.72969804],\\n        [ 0.72969804,  1.1609872 ]],\\n262 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  '[[ 0.68809572,  0.79608475],\\n        [ 0.79608475,  1.21234145]]])\\nGreat, it worked fine! Indeed, the weights that were used to generate the data were\\n0.2, 0.4 and 0.4, and similarly, the means and covariance matrices were very close to\\nthose found by the algorithm. But how? This class relies on the Expectation-\\nMaximization (EM) algorithm, which has many similarities with the K-Means algo\\nrithm: it also initializes the cluster parameters randomly, then it repeats two steps\\nuntil convergence, first assigning instances to clusters (this is called the expectation\\nstep) then updating the clusters (this is called the maximization step). Sounds famil\\niar? Indeed, in the context of clustering you can think of EM as a generalization of K-\\nMeans which not only finds the cluster centers ( (1) to (k)), but also their size, shape\\nand orientation ( (1) to (k)), as well as their relative weights ( (1) to (k)). Unlike K-\\nMeans, EM uses soft cluster assignments rather than hard assignments: for each\\ninstance during the expectation step, the algorithm estimates the probability that it\\nbelongs to each cluster (based on the current cluster parameters). Then, during the\\nmaximization step, each cluster is updated using all the instances in the dataset, with\\neach instance weighted by the estimated probability that it belongs to that cluster.\\nThese probabilities are called the responsibilities of the clusters for the instances. Dur\\ning the maximization step, each clusters update will mostly be impacted by the\\ninstances it is most responsible for.\\nUnfortunately, just like K-Means, EM can end up converging to\\npoor solutions, so it needs to be run several times, keeping only the\\nbest solution. This is why we set n_init to 10. Be careful: by default\\nn_init is only set to 1.\\nY ou can check whether or not the algorithm converged and how many iterations it\\ntook:\\n>>> gm.converged_\\nTrue\\n>>> gm.n_iter_\\n3\\nOkay, now that you have an estimate of the location, size, shape, orientation and rela\\ntive weight of each cluster, the model can easily assign each instance to the most likely\\ncluster (hard clustering) or estimate the probability that it belongs to a particular\\ncluster (soft clustering). For this, just use the predict() method for hard clustering,\\nor the predict_proba() method for soft clustering:\\n>>> gm.predict(X)\\narray([2, 2, 1, ..., 0, 0, 0])\\n>>> gm.predict_proba(X)\\narray([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],\\n       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],\\nGaussian Mixtures | 263',\n",
       "  '[2.01535333e-06, 9.99923053e-01, 7.49319577e-05],\\n       ...,\\n       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],\\n       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],\\n       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\\nIt is a generative model, meaning you can actually sample new instances from it (note\\nthat they are ordered by cluster index):\\n>>> X_new, y_new = gm.sample(6)\\n>>> X_new\\narray([[ 2.95400315,  2.63680992],\\n       [-1.16654575,  1.62792705],\\n       [-1.39477712, -1.48511338],\\n       [ 0.27221525,  0.690366  ],\\n       [ 0.54095936,  0.48591934],\\n       [ 0.38064009, -0.56240465]])\\n>>> y_new\\narray([0, 1, 2, 2, 2, 2])\\nIt is also possible to estimate the density of the model at any given location. This is\\nachieved using the score_samples() method: for each instance it is given, this\\nmethod estimates the log of the probability density function (PDF) at that location.\\nThe greater the score, the higher the density:\\n>>> gm.score_samples(X)\\narray([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,\\n       -4.39802535, -3.80743859])\\nIf you compute the exponential of these scores, you get the value of the PDF at the\\nlocation of the given instances. These are not probabilities, but probability densities:\\nthey can take on any positive value, not just between 0 and 1. To estimate the proba\\nbility that an instance will fall within a particular region, you would have to integrate\\nthe PDF over that region (if you do so over the entire space of possible instance loca\\ntions, the result will be 1).\\nFigure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the\\ndensity contours of this model:\\n264 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-17. Cluster means, decision boundaries and density contours of a trained Gaus\\nsian mixture model\\nNice! The algorithm clearly found an excellent solution. Of course, we made its task\\neasy by actually generating the data using a set of 2D Gaussian distributions (unfortu\\nnately, real life data is not always so Gaussian and low-dimensional), and we also gave\\nthe algorithm the correct number of clusters. When there are many dimensions, or\\nmany clusters, or few instances, EM can struggle to converge to the optimal solution.\\nY ou might need to reduce the difficulty of the task by limiting the number of parame\\nters that the algorithm has to learn: one way to do this is to limit the range of shapes\\nand orientations that the clusters can have. This can be achieved by imposing con\\nstraints on the covariance matrices. To do this, just set the covariance_type hyper\\nparameter to one of the following values:\\n \"spherical\": all clusters must be spherical, but they can have different diameters\\n(i.e., different variances).\\n \"diag\": clusters can take on any ellipsoidal shape of any size, but the ellipsoids\\naxes must be parallel to the coordinate axes (i.e., the covariance matrices must be\\ndiagonal).\\n \"tied\": all clusters must have the same ellipsoidal shape, size and orientation\\n(i.e., all clusters share the same covariance matrix).\\nBy default, covariance_type is equal to \"full\", which means that each cluster can\\ntake on any shape, size and orientation (it has its own unconstrained covariance\\nmatrix). Figure 9-18  plots the solutions found by the EM algorithm when cova\\nriance_type is set to \"tied\" or \"spherical .\\nGaussian Mixtures | 265',\n",
       "  'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type is \"spherical or \"diag\",\\nit is O( kmn), assuming the data has a clustering structure. If cova\\nriance_type is \"tied\" or \"full\", it is O(kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Lets see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection (also called outlier detection) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies or\\noutliers, while the normal instances are called inliers. Anomaly detection is very use\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3). Here is how you would identify the outliers using the 4th percentile low\\n266 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities = gm.score_samples(X)\\ndensity_threshold = np.percentile(densities, 4)\\nanomalies = X[densities < density_threshold]\\nThese anomalies are represented as stars on Figure 9-19:\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection: it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a clean dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli\\ners, so if you have too many of them, this will bias the models view\\nof normality: some outliers may wrongly be considered as nor\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope class).\\nJust like K-Means, the GaussianMixture algorithm requires you to specify the num\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor\\nGaussian Mixtures | 267',\n",
       "  'mation criterion  such as the Bayesian information criterion  (BIC) or the Akaike\\ninformation criterion (AIC), defined in Equation 9-1.\\nEquation 9-1. Bayesian information criterion (BIC) and Akaike information\\ncriterion (AIC)\\nBIC = log m p  2 log L\\nAIC = 2 p  2 log L\\n m is the number of instances, as always.\\n p is the number of parameters learned by the model.\\n L is the maximized value of the likelihood function of the model.\\nBoth the BIC and the AIC penalize models that have more parameters to learn (e.g.,\\nmore clusters), and reward models that fit the data well. They often end up selecting\\nthe same model, but when they differ, the model selected by the BIC tends to be sim\\npler (fewer parameters) than the one selected by the AIC, but it does not fit the data\\nquite as well (this is especially true for larger datasets).\\nLikelihood function\\nThe terms probability and likelihood are often used interchangeably in the\\nEnglish language, but they have very different meanings in statistics: given a statistical\\nmodel with some parameters , the word probability is used to describe how plausi\\nble a future outcome x is (knowing the parameter values ), while the word likeli\\nhood is used to describe how plausible a particular set of parameter values  are,\\nafter the outcome x is known.\\nConsider a one-dimensional mixture model of two Gaussian distributions centered at\\n-4 and +1. For simplicity, this toy model has a single parameter  that controls the\\nstandard deviations of both distributions. The top left contour plot in Figure 9-20\\nshows the entire model f(x; ) as a function of both x and . To estimate the probabil\\nity distribution of a future outcome x, you need to set the model parameter . For\\nexample, if you set it to =1.3 (the horizontal line), you get the probability density\\nfunction f(x; =1.3) shown in the lower left plot. Say you want to estimate the proba\\nbility that x will fall between -2 and +2, you must calculate the integral of the PDF on\\nthis range (i.e., the surface of the shaded region). On the other hand, if you have\\nobserved a single instance x=2.5 (the vertical line in the upper left plot), you get the\\nlikelihood function noted (|x=2.5)=f(x=2.5; ) represented in the upper right plot.\\nIn short, the PDF is a function of x (with  fixed) while the likelihood function is a\\nfunction of  (with x fixed). It is important to understand that the likelihood function\\nis not a probability distribution: if you integrate a probability distribution over all\\n268 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'possible values of x, you always get 1, but if you integrate the likelihood function over\\nall possible values of , the result can be any positive value.\\nFigure 9-20. A models parametric function (top left), and some derived functions: a PDF\\n(lower left), a likelihood function (top right) and a log likelihood function (lower right)\\nGiven a dataset X, a common task is to try to estimate the most likely values for the\\nmodel parameters. To do this, you must find the values that maximize the likelihood\\nfunction, given X. In this example, if you have observed a single instance x=2.5, the\\nmaximum likelihood estimate (MLE) of  is =1.5. If a prior probability distribution g\\nover  exists, it is possible to take it into account by maximizing (|x)g() rather\\nthan just maximizing (|x). This is called maximum a-posteriori (MAP) estimation.\\nSince MAP constrains the parameter values, you can think of it as a regularized ver\\nsion of MLE.\\nNotice that it is equivalent to maximize the likelihood function or to maximize its\\nlogarithm (represented in the lower right hand side of Figure 9-20): indeed, the loga\\nrithm is a strictly increasing function, so if  maximizes the log likelihood, it also\\nmaximizes the likelihood. It turns out that it is generally easier to maximize the log\\nlikelihood. For example, if you observed several independent instances x(1) to x(m), you\\nwould need to find the value of  that maximizes the product of the individual likeli\\nhood functions. But it is equivalent, and much simpler, to maximize the sum (not the\\nproduct) of the log likelihood functions, thanks to the magic of the logarithm which\\nconverts products into sums: log(ab)=log(a)+log(b).\\nOnce you have estimated , the value of  that maximizes the likelihood function,\\nthen you are ready to compute L =  ,  . This is the value which is used to com\\npute the AIC and BIC: you can think of it as a measure of how well the model fits the\\ndata.\\nTo compute the BIC and AIC, just call the bic() or aic() methods:\\nGaussian Mixtures | 269',\n",
       "  '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21 shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type hyperparameter.\\nFor example, if it is \"spherical\" rather than \"full\", then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, lets set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_, 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17.\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'Figure 9-22. Bayesian Gaussian mixture model\\nPrior knowledge about the latent variables z can be encoded in a probability distribu\\ntion p(z) called the prior. For example, we may have a prior belief that the clusters are\\nlikely to be few (low concentration), or conversely, that they are more likely to be\\nplentiful (high concentration). This can be adjusted using the weight_concentra\\ntion_prior hyperparameter. Setting it to 0.01 or 1000 gives very different clusterings\\n(see Figure 9-23). However, the more data we have, the less the priors matter. In fact,\\nto plot diagrams with such large differences, you must use very strong priors and lit\\ntle data.\\nFigure 9-23. Using different concentration priors\\nThe fact that you see only 3 regions in the right plot although there\\nare 4 centroids is not a bug: the weight of the top-right cluster is\\nmuch larger than the weight of the lower-right cluster, so the prob\\nability that any given point in this region belongs to the top-right\\ncluster is greater than the probability that it belongs to the lower-\\nright cluster, even near the lower-right cluster.\\nGaussian Mixtures | 271',\n",
       "  'Bayes theorem (Equation 9-2) tells us how to update the probability distribution over\\nthe latent variables after we observe some data X. It computes the posterior distribu\\ntion p(z|X), which is the conditional probability of z given X.\\nEquation 9-2. Bayes theorem\\np z X = Posterior = LikelihoodPrior\\nEvidence = p X z p z\\np X\\nUnfortunately, in a Gaussian mixture model (and many other problems), the denomi\\nnator p(x) is intractable, as it requires integrating over all the possible values of z\\n(Equation 9-3). This means considering all possible combinations of cluster parame\\nters and cluster assignments.\\nEquation 9-3. The evidence p(X) is often intractable\\np X = p X z p z dz\\nThis is one of the central problems in Bayesian statistics, and there are several\\napproaches to solving it. One of them is variational inference, which picks a family of\\ndistributions q(z; ) with its own variational parameters  (lambda), then it optimizes\\nthese parameters to make q(z) a good approximation of p(z|X). This is achieved by\\nfinding the value of  that minimizes the KL divergence from q(z) to p(z|X), noted\\nDKL(qp). The KL divergence equation is shown in (see Equation 9-4), and it can be\\nrewritten as the log of the evidence (log p(X)) minus the evidence lower bound\\n(ELBO). Since the log of the evidence does not depend on q, it is a constant term, so\\nminimizing the KL divergence just requires maximizing the ELBO.\\nEquation 9-4. KL divergence from q(z) to p(z|X)\\nDKL q  p = q log q z\\np z X\\n= q log q z  log p z X\\n= q log q z  log p z, X\\np X\\n= q log q z  log p z, X + log p X\\n= q log q z  q log p z, X + q log p X\\n= q log p X  q log p z, X  q log q z\\n= log p X  ELBO\\nwhere ELBO = q log p z, X  q log q z\\n272 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'In practice, there are different techniques to maximize the ELBO. In mean field varia\\ntional inference, it is necessary to pick the family of distributions q(z; ) and the prior\\np(z) very carefully to ensure that the equation for the ELBO simplifies to a form that\\ncan actually be computed. Unfortunately, there is no general way to do this, it\\ndepends on the task and requires some mathematical skills. For example, the distribu\\ntions and lower bound equations used in Scikit-Learns BayesianGaussianMixture\\nclass are presented in the documentation. From these equations it is possible to derive\\nupdate equations for the cluster parameters and assignment variables: these are then\\nused very much like in the Expectation-Maximization algorithm. In fact, the compu\\ntational complexity of the BayesianGaussianMixture class is similar to that of the\\nGaussianMixture class (but generally significantly slower). A simpler approach to\\nmaximizing the ELBO is called black box stochastic variational inference (BBSVI): at\\neach iteration, a few samples are drawn from q and they are used to estimate the gra\\ndients of the ELBO with regards to the variational parameters , which are then used\\nin a gradient ascent step. This approach makes it possible to use Bayesian inference\\nwith any kind of model (provided it is differentiable), even deep neural networks: this\\nis called Bayesian deep learning.\\nIf you want to dive deeper into Bayesian statistics, check out the\\nBayesian Data Analysis book by Andrew Gelman, John Carlin, Hal\\nStern, David Dunson, Aki Vehtari, and Donald Rubin.\\nGaussian mixture models work great on clusters with ellipsoidal shapes, but if you try\\nto fit a dataset with different shapes, you may have bad surprises. For example, lets\\nsee what happens if we use a Bayesian Gaussian mixture model to cluster the moons\\ndataset (see Figure 9-24):\\nFigure 9-24. moons_vs_bgm_diagram\\nOops, the algorithm desperately searched for ellipsoids, so it found 8 different clus\\nters instead of 2. The density estimation is not too bad, so this model could perhaps\\nbe used for anomaly detection, but it failed to identify the two moons. Lets now look\\nat a few clustering algorithms capable of dealing with arbitrarily shaped clusters.\\nGaussian Mixtures | 273',\n",
       "  'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n Fast-MCD (minimum covariance determinant), implemented by the EllipticEn\\nvelope class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n Isolation forest: this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n One-class SVM: this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques',\n",
       "  'PART II\\nNeural Networks and Deep Learning',\n",
       "  '1 Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven\\ntions were inspired by nature. It seems only logical, then, to look at the brains archi\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial neural networks  (ANNs). However, although planes were\\ninspired by birds, they dont have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying units rather\\nthan neurons), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni\\ntion services (e.g., Apples Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMinds Alpha\\nZero).\\n277',\n",
       "  '2  A Logical Calculus of Ideas Immanent in Nervous Activity,  W . McCulloch and W . Pitts (1943).\\nIn the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per\\nceptrons (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But dont be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap\\nter 12.\\nBut first, lets go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2  A Logical Calculus of Ideas Immanent in\\nNervous Activity,  McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic. This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5). These techniques seemed to offer better results and stron\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  ' There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n The tremendous increase in computing power since the 1990s now makes it pos\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moores Law, but also thanks to the gaming industry, which has pro\\nduced powerful GPU cards by the millions.\\n The training algorithms have been improved. To be fair they are only slightly dif\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, lets take a quick look at a biological neuron (rep\\nresented in Figure 10-1). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body containing the nucleus and most\\nof the cells complex components, and many branching extensions called dendrites,\\nplus one very long extension called the axon. The axons length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria, and at the tip of\\nthese branches are minuscule structures called synaptic terminals (or simply synap\\nses), which are connected to the dendrites (or directly to the cell body) of other neu\\nrons. Biological neurons receive short electrical impulses called signals from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial Neurons | 279',\n",
       "  '3 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from https://en.wikipedia.org/wiki/Neuron.\\n4 In the context of Machine Learning, the phrase neural networks generally refers to ANNs, not BNNs.\\n5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe\\ndia.org/wiki/Cerebral_cortex.\\nFigure 10-1. Biological neuron3\\nThus, individual biological neurons seem to behave in a rather simple way, but they\\nare organized in a vast network of billions of neurons, each neuron typically connec\\nted to thousands of other neurons. Highly complex computations can be performed\\nby a vast network of fairly simple neurons, much like a complex anthill can emerge\\nfrom the combined efforts of simple ants. The architecture of biological neural net\\nworks (BNN)4 is still the subject of active research, but some parts of the brain have\\nbeen mapped, and it seems that neurons are often organized in consecutive layers, as \\nshown in Figure 10-2.\\nFigure 10-2. Multiple layers in a biological neural network (human cortex)5\\n280 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial neuron: it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, lets\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n The first network on the left is simply the identity function: if neuron A is activa\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti\\nvate neuron C).\\n The third network performs a logical OR: neuron C gets activated if either neu\\nron A or neuron B is activated (or both).\\n Finally, if we suppose that an input connection can inhibit the neurons activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called \\nFrom Biological to Artificial Neurons | 281',\n",
       "  '6 The name Perceptron is sometimes used to mean a tiny network with a single TLU.\\na threshold logic unit (TLU), or sometimes a linear threshold unit (LTU): the inputs\\nand output are now numbers (instead of binary on/off values) and each input con\\nnection is associated with a weight. The TLU computes a weighted sum of its inputs\\n(z = w1 x1 + w2 x2 +  + wn xn = xT w), then applies a step function to that sum and\\noutputs the result: hw(x) = step(z), where z = xT w.\\nFigure 10-4. Threshold logic unit\\nThe most common step function used in Perceptrons is the Heaviside step function\\n(see Equation 10-1). Sometimes the sign function is used instead.\\nEquation 10-1. Common step functions used in Perceptrons\\nheaviside z = 0 if z < 0\\n1 if z  0 sgn z =\\n1 if z < 0\\n0 if z = 0\\n+1 if z > 0\\nA single TLU can be used for simple linear binary classification. It computes a linear\\ncombination of the inputs and if the result exceeds a threshold, it outputs the positive\\nclass or else outputs the negative class (just like a Logistic Regression classifier or a\\nlinear SVM). For example, you could use a single TLU to classify iris flowers based on\\nthe petal length and width (also adding an extra bias feature x0 = 1, just like we did in\\nprevious chapters). Training a TLU in this case means finding the right values for w0,\\nw1, and w2 (the training algorithm is discussed shortly).\\nA Perceptron is simply composed of a single layer of TLUs,6 with each TLU connected\\nto all the inputs. When all the neurons in a layer are connected to every neuron in the\\nprevious layer (i.e., its input neurons), it is called a fully connected layer  or a dense\\nlayer. To represent the fact that each input is sent to every TLU, it is common to draw\\nspecial passthrough neurons called input neurons: they just output whatever input\\nthey are fed. All the input neurons form the input layer. Moreover, an extra bias fea\\n282 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'ture is generally added ( x0 = 1): it is typically represented using a special type of neu\\nron called a bias neuron, which just outputs 1 all the time. A Perceptron with two\\ninputs and three outputs is represented in Figure 10-5. This Perceptron can classify\\ninstances simultaneously into three different binary classes, which makes it a multi\\noutput classifier.\\nFigure 10-5. Perceptron diagram\\nThanks to the magic of linear algebra, it is possible to efficiently compute the outputs\\nof a layer of artificial neurons for several instances at once, by using Equation 10-2:\\nEquation 10-2. Computing the outputs of a fully connected layer\\nhW, b X =  XW + b\\n As always, X represents the matrix of input features. It has one row per instance,\\none column per feature.\\n The weight matrix W contains all the connection weights except for the ones\\nfrom the bias neuron. It has one row per input neuron and one column per artifi\\ncial neuron in the layer.\\n The bias vector b contains all the connection weights between the bias neuron\\nand the artificial neurons. It has one bias term per artificial neuron.\\n The function  is called the activation function: when the artificial neurons are\\nTLUs, it is a step function (but we will discuss other activation functions shortly).\\nSo how is a Perceptron trained? The Perceptron training algorithm proposed by\\nFrank Rosenblatt was largely inspired by Hebbs rule. In his book The Organization of\\nBehavior, published in 1949, Donald Hebb suggested that when a biological neuron\\noften triggers another neuron, the connection between these two neurons grows\\nstronger. This idea was later summarized by Siegrid Lwel in this catchy phrase:\\nCells that fire together, wire together.  This rule later became known as Hebbs rule \\nFrom Biological to Artificial Neurons | 283',\n",
       "  '7 Note that this solution is generally not unique: in general when the data are linearly separable, there is an\\ninfinity of hyperplanes that can separate them.\\n(or Hebbian learning ); that is, the connection weight between two neurons is\\nincreased whenever they have the same output. Perceptrons are trained using a var\\niant of this rule that takes into account the error made by the network; it reinforces\\nconnections that help reduce the error. More specifically, the Perceptron is fed one\\ntraining instance at a time, and for each instance it makes its predictions. For every\\noutput neuron that produced a wrong prediction, it reinforces the connection\\nweights from the inputs that would have contributed to the correct prediction. The\\nrule is shown in Equation 10-3.\\nEquation 10-3. Perceptron learning rule (weight update)\\nwi, j\\nnext step = wi, j +  yj  y j xi\\n wi, j is the connection weight between the ith input neuron and the jth output neu\\nron.\\n xi is the ith input value of the current training instance.\\n yj is the output of the jth output neuron for the current training instance.\\n yj is the target output of the jth output neuron for the current training instance.\\n  is the learning rate.\\nThe decision boundary of each output neuron is linear, so Perceptrons are incapable\\nof learning complex patterns (just like Logistic Regression classifiers). However, if the\\ntraining instances are linearly separable, Rosenblatt demonstrated that this algorithm\\nwould converge to a solution.7 This is called the Perceptron convergence theorem.\\nScikit-Learn provides a Perceptron class that implements a single TLU network. It\\ncan be used pretty much as you would expectfor example, on the iris dataset (intro\\nduced in Chapter 4):\\nimport numpy as np\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import Perceptron\\niris = load_iris()\\nX = iris.data[:, (2, 3)]  # petal length, petal width\\ny = (iris.target == 0).astype(np.int)  # Iris Setosa?\\nper_clf = Perceptron()\\nper_clf.fit(X, y)\\n284 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'y_pred = per_clf.predict([[2, 0.5]])\\nY ou may have noticed the fact that the Perceptron learning algorithm strongly resem\\nbles Stochastic Gradient Descent. In fact, Scikit-Learns Perceptron class is equivalent\\nto using an SGDClassifier with the following hyperparameters: loss=\"perceptron\",\\nlearning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regu\\nlarization).\\nNote that contrary to Logistic Regression classifiers, Perceptrons do not output a class\\nprobability; rather, they just make predictions based on a hard threshold. This is one\\nof the good reasons to prefer Logistic Regression over Perceptrons.\\nIn their 1969 monograph titled Perceptrons, Marvin Minsky and Seymour Papert\\nhighlighted a number of serious weaknesses of Perceptrons, in particular the fact that\\nthey are incapable of solving some trivial problems (e.g., the Exclusive OR  (XOR)\\nclassification problem; see the left side of Figure 10-6). Of course this is true of any\\nother linear classification model as well (such as Logistic Regression classifiers), but\\nresearchers had expected much more from Perceptrons, and their disappointment\\nwas great, and many researchers dropped neural networks altogether in favor of\\nhigher-level problems such as logic, problem solving, and search.\\nHowever, it turns out that some of the limitations of Perceptrons can be eliminated by\\nstacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron\\n(MLP). In particular, an MLP can solve the XOR problem, as you can verify by com\\nputing the output of the MLP represented on the right of Figure 10-6: with inputs (0,\\n0) or (1, 1) the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All\\nconnections have a weight equal to 1, except the four connections where the weight is\\nshown. Try verifying that this network indeed solves the XOR problem!\\nFigure 10-6. XOR classification problem and an MLP that solves it\\nFrom Biological to Artificial Neurons | 285',\n",
       "  '8 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of deep is quite fuzzy.\\n9 Learning Internal Representations by Error Propagation,  D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer, one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out\\nputs), so this architecture is an example of a feedforward neural net\\nwork (FNN).\\nWhen an ANN contains a deep stack of hidden layers 8, it is called a deep neural net\\nwork (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  '10 This technique was actually independently invented several times by various researchers in different fields,\\nstarting with P . Werbos in 1974.\\nusing an efficient technique for computing the gradients automatically 10: in just two\\npasses through the network (one forward, one backward), the backpropagation algo\\nrithm is able to compute the gradient of the networks error with regards to every sin\\ngle model parameter. In other words, it can find out how each connection weight and\\neach bias term should be tweaked in order to reduce the error. Once it has these gra\\ndients, it just performs a regular Gradient Descent step, and the whole process is\\nrepeated until the network converges to the solution.\\nAutomatically computing gradients is called automatic differentia\\ntion, or autodiff. There are various autodiff techniques, with differ\\nent pros and cons. The one used by backpropagation is called\\nreverse-mode autodiff. It is fast and precise, and is well suited when\\nthe function to differentiate has many variables (e.g., connection\\nweights) and few outputs (e.g., one loss). If you want to learn more\\nabout autodiff, check out ???.\\nLets run through this algorithm in a bit more detail:\\n It handles one mini-batch at a time (for example containing 32 instances each),\\nand it goes through the full training set multiple times. Each pass is called an\\nepoch, as we saw in Chapter 4.\\n Each mini-batch is passed to the networks input layer, which just sends it to the\\nfirst hidden layer. The algorithm then computes the output of all the neurons in\\nthis layer (for every instance in the mini-batch). The result is passed on to the\\nnext layer, its output is computed and passed to the next layer, and so on until we\\nget the output of the last layer, the output layer. This is the forward pass: it is\\nexactly like making predictions, except all intermediate results are preserved\\nsince they are needed for the backward pass.\\n Next, the algorithm measures the networks output error (i.e., it uses a loss func\\ntion that compares the desired output and the actual output of the network, and\\nreturns some measure of the error).\\n Then it computes how much each output connection contributed to the error.\\nThis is done analytically by simply applying the chain rule (perhaps the most fun\\ndamental rule in calculus), which makes this step fast and precise.\\n The algorithm then measures how much of these error contributions came from\\neach connection in the layer below, again using the chain ruleand so on until\\nthe algorithm reaches the input layer. As we explained earlier, this reverse pass\\nefficiently measures the error gradient across all the connection weights in the\\nFrom Biological to Artificial Neurons | 287',\n",
       "  'network by propagating the error gradient backward through the network (hence\\nthe name of the algorithm).\\n Finally, the algorithm performs a Gradient Descent step to tweak all the connec\\ntion weights in the network, using the error gradients it just computed.\\nThis algorithm is so important, its worth summarizing it again: for each training\\ninstance the backpropagation algorithm first makes a prediction (forward pass),\\nmeasures the error, then goes through each layer in reverse to measure the error con\\ntribution from each connection (reverse pass), and finally slightly tweaks the connec\\ntion weights to reduce the error (Gradient Descent step).\\nIt is important to initialize all the hidden layers connection weights\\nrandomly, or else training will fail. For example, if you initialize all\\nweights and biases to zero, then all neurons in a given layer will be\\nperfectly identical, and thus backpropagation will affect them in\\nexactly the same way, so they will remain identical. In other words,\\ndespite having hundreds of neurons per layer, your model will act\\nas if it had only one neuron per layer: it wont be too smart. If\\ninstead you randomly initialize the weights, you break the symme\\ntry and allow backpropagation to train a diverse team of neurons.\\nIn order for this algorithm to work properly, the authors made a key change to the\\nMLPs architecture: they replaced the step function with the logistic function, (z) =\\n1 / (1 + exp( z)). This was essential because the step function contains only flat seg\\nments, so there is no gradient to work with (Gradient Descent cannot move on a flat\\nsurface), while the logistic function has a well-defined nonzero derivative every\\nwhere, allowing Gradient Descent to make some progress at every step. In fact, the\\nbackpropagation algorithm works well with many other activation functions, not just\\nthe logistic function. Two other popular activation functions are:\\nThe hyperbolic tangent function tanh(z) = 2(2z)  1\\nJust like the logistic function it is S-shaped, continuous, and differentiable, but its\\noutput value ranges from 1 to 1 (instead of 0 to 1 in the case of the logistic func\\ntion), which tends to make each layers output more or less centered around 0 at\\nthe beginning of training. This often helps speed up convergence.\\nThe Rectified Linear Unit function: ReLU(z) = max(0, z)\\nIt is continuous but unfortunately not differentiable at z = 0 (the slope changes\\nabruptly, which can make Gradient Descent bounce around), and its derivative is\\n0 for z < 0. However, in practice it works very well and has the advantage of being\\n288 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  '11 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.\\nfast to compute 11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8. But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f(x) = 2 x + 3 and g(x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you dont\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial Neurons | 289',\n",
       "  'In general, when building an MLP for regression, you do not want to use any activa\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or 1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres\\nhold  (typically 1), but linear when the error is larger than . This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso\\nlute error.\\nTable 10-1 summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11)\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'or non-urgent email. In this case, you would need two output neurons, both using\\nthe logistic activation function: the first would output the probability that the email is\\nspam and the second would output the probability that it is urgent. More generally,\\nyou would dedicate one output neuron for each positive class. Note that the output\\nprobabilities do not necessarily add up to one. This lets the model output any combi\\nnation of labels: you can have non-urgent ham, urgent ham, non-urgent spam, and\\nperhaps even urgent spam (although that would probably be an error).\\nIf each instance can belong only to a single class, out of 3 or more possible classes\\n(e.g., classes 0 through 9 for digit image classification), then you need to have one\\noutput neuron per class, and you should use the softmax activation function for the\\nwhole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)\\nwill ensure that all the estimated probabilities are between 0 and 1 and that they add\\nup to one (which is required if the classes are exclusive). This is called multiclass clas\\nsification.\\nFigure 10-9. A modern MLP (including ReLU and softmax) for classification\\nRegarding the loss function, since we are predicting probability distributions, the\\ncross-entropy (also called the log loss, see Chapter 4) is generally a good choice.\\nTable 10-2 summarizes the typical architecture of a classification MLP .\\nTable 10-2. Typical Classification MLP Architecture\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nInput and hidden layers Same as regression Same as regression Same as regression\\n# output neurons 1 1 per label 1 per class\\nOutput layer activation Logistic Logistic Softmax\\nFrom Biological to Artificial Neurons | 291',\n",
       "  '12 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play\\nground. This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail\\nable at https://keras.io. The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras). It was developed by Franois Chollet as part of a\\nresearch project 12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apples Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlows Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  \"Figure 10-10. Two Keras implementations: keras-team (left) and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, lets install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc\\ntions in Chapter 2, you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis\\ntrator rights, or to add the --user option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu instead of\\ntensorflow, and there are other libraries to install. See https://\\ntensorflow.org/install/gpu for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow as tf\\n>>> from tensorflow import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\",\n",
       "  \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow lets use tf.keras! Lets start by building a simple image classifier.\\nBuilding an Image Classifier Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST, which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 2828 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Lets load\\nFashion MNIST:\\nfashion_mnist = keras.datasets.fashion_mnist\\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 2828 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full.shape\\n(60000, 28, 28)\\n>>> X_train_full.dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so lets create one. Moreover, since we are going to train the neural net\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial Neural Networks with Keras\",\n",
       "  'class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\\nFor example, the first image in the training set represents a coat:\\n>>> class_names[y_train[0]]\\n\\'Coat\\'\\nFigure 10-11 shows a few samples from the Fashion MNIST dataset:\\nFigure 10-11. Samples from Fashion MNIST\\nCreating the Model Using the Sequential API\\nNow lets build the neural network! Here is a classification MLP with two hidden lay\\ners:\\nmodel = keras.models.Sequential()\\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\\nLets go through this code line by line:\\n The first line creates a Sequential model. This is the simplest kind of Keras\\nmodel, for neural networks that are just composed of a single stack of layers, con\\nnected sequentially. This is called the sequential API.\\n Next, we build the first layer and add it to the model. It is a Flatten layer whose\\nrole is simply to convert each input image into a 1D array: if it receives input data\\nX, it computes X.reshape(-1, 1). This layer does not have any parameters, it is\\njust there to do some simple preprocessing. Since it is the first layer in the model,\\nyou should specify the input_shape: this does not include the batch size, only the\\nshape of the instances. Alternatively, you could add a keras.layers.InputLayer\\nas the first layer, setting shape=[28,28].\\n Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa\\ntion function. Each Dense layer manages its own weight matrix, containing all the\\nconnection weights between the neurons and their inputs. It also manages a vec\\nImplementing MLPs with Keras | 295',\n",
       "  'tor of bias terms (one per neuron). When it receives some input data, it computes\\nEquation 10-2.\\n Next we add a second Dense hidden layer with 100 neurons, also using the ReLU\\nactivation function.\\n Finally, we add a Dense output layer with 10 neurons (one per class), using the\\nsoftmax activation function (because the classes are exclusive).\\nSpecifying activation=\"relu\" is equivalent to activa\\ntion=keras.activations.relu. Other activation functions are\\navailable in the keras.activations package, we will use many of\\nthem in this book. See https://keras.io/activations/ for the full list.\\nInstead of adding the layers one by one as we just did, you can pass a list of layers\\nwhen creating the Sequential model:\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.Dense(300, activation=\"relu\"),\\n    keras.layers.Dense(100, activation=\"relu\"),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nUsing Code Examples From keras.io\\nCode examples documented on keras.io will work fine with tf.keras, but you need to\\nchange the imports. For example, consider this keras.io code:\\nfrom keras.layers import Dense\\noutput_layer = Dense(10)\\nY ou must change the imports like this:\\nfrom tensorflow.keras.layers import Dense\\noutput_layer = Dense(10)\\nOr simply use full paths, if you prefer:\\nfrom tensorflow import keras\\noutput_layer = keras.layers.Dense(10)\\nThis is more verbose, but I use this approach in this book so you can easily see which\\npackages to use, and to avoid confusion between standard classes and custom classes.\\nIn production code, I use the previous approach, as do most people.\\n296 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  \"13 Y ou can also generate an image of your model using keras.utils.plot_model().\\nThe models summary() method displays all the models layers13, including each layers\\nname (which is automatically generated unless you set it when creating the layer), its\\noutput shape (None means the batch size can be anything), and its number of parame\\nters. The summary ends with the total number of parameters, including trainable and\\nnon-trainable parameters. Here we only have trainable parameters (we will see exam\\nples of non-trainable parameters in Chapter 11):\\n>>> model.summary()\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\nflatten_1 (Flatten)          (None, 784)               0\\n_________________________________________________________________\\ndense_3 (Dense)              (None, 300)               235500\\n_________________________________________________________________\\ndense_4 (Dense)              (None, 100)               30100\\n_________________________________________________________________\\ndense_5 (Dense)              (None, 10)                1010\\n=================================================================\\nTotal params: 266,610\\nTrainable params: 266,610\\nNon-trainable params: 0\\nNote that Dense layers often have a lot of parameters. For example, the first hidden\\nlayer has 784  300 connection weights, plus 300 bias terms, which adds up to\\n235,500 parameters! This gives the model quite a lot of flexibility to fit the training\\ndata, but it also means that the model runs the risk of overfitting, especially when you\\ndo not have a lot of training data. We will come back to this later.\\nY ou can easily get a models list of layers, to fetch a layer by its index, or you can fetch\\nit by name:\\n>>> model.layers\\n[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\\n <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\\n <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\\n <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\\n>>> model.layers[1].name\\n'dense_3'\\n>>> model.get_layer('dense_3').name\\n'dense_3'\\nAll the parameters of a layer can be accessed using its get_weights() and\\nset_weights() method. For a Dense layer, this includes both the connection weights\\nand the bias terms:\\nImplementing MLPs with Keras | 297\",\n",
       "  '>>> weights, biases = hidden1.get_weights()\\n>>> weights\\narray([[ 0.03854964, -0.04054524,  0.00599282, ...,  0.02566582,\\n         0.01032123,  0.06914985],\\n       ...,\\n       [ 0.02632413, -0.05105981, -0.00332005, ...,  0.04175945,\\n         0.0443138 , -0.05558084]], dtype=float32)\\n>>> weights.shape\\n(784, 300)\\n>>> biases\\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\\n>>> biases.shape\\n(300,)\\nNotice that the Dense layer initialized the connection weights randomly (which is\\nneeded to break symmetry, as we discussed earlier), and the biases were just initial\\nized to zeros, which is fine. If you ever want to use a different initialization method,\\nyou can set kernel_initializer (kernel is another name for the matrix of connec\\ntion weights) or bias_initializer when creating the layer. We will discuss initializ\\ners further in Chapter 11, but if you want the full list, see https://keras.io/initializers/.\\nThe shape of the weight matrix depends on the number of inputs.\\nThis is why it is recommended to specify the input_shape when\\ncreating the first layer in a Sequential model. However, if you do\\nnot specify the input shape, its okay: Keras will simply wait until it\\nknows the input shape before it actually builds the model. This will\\nhappen either when you feed it actual data (e.g., during training),\\nor when you call its build() method. Until the model is really\\nbuilt, the layers will not have any weights, and you will not be able\\nto do certain things (such as print the model summary or save the\\nmodel), so if you know the input shape when creating the model, it\\nis best to specify it.\\nCompiling the Model\\nAfter a model is created, you must call its compile() method to specify the loss func\\ntion and the optimizer to use. Optionally, you can also specify a list of extra metrics to\\ncompute during training and evaluation:\\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\\n              optimizer=\"sgd\",\\n              metrics=[\"accuracy\"])\\n298 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Using loss=\"sparse_categorical_crossentropy\" is equivalent to\\nloss=keras.losses.sparse_categorical_crossentropy. Simi\\nlarly, optimizer=\"sgd\" is equivalent to optimizer=keras.optimiz\\ners.SGD() and metrics=[\"accuracy\"] is equivalent to\\nmetrics=[keras.metrics.sparse_categorical_accuracy] (when\\nusing this loss). We will use many other losses, optimizers and met\\nrics in this book, but for the full lists see https://keras.io/losses/,\\nhttps://keras.io/optimizers/ and https://keras.io/metrics/.\\nThis requires some explanation. First, we use the \"sparse_categorical_crossen\\ntropy\" loss because we have sparse labels (i.e., for each instance there is just a target\\nclass index, from 0 to 9 in this case), and the classes are exclusive. If instead we had\\none target probability per class for each instance (such as one-hot vectors, e.g. [0.,\\n0., 0., 1., 0., 0., 0., 0., 0., 0.]  to represent class 3), then we would need\\nto use the \"categorical_crossentropy\" loss instead. If we were doing binary classi\\nfication (with one or more binary labels), then we would use the \"sigmoid\" (i.e.,\\nlogistic) activation function in the output layer instead of the \"softmax\" activation\\nfunction, and we would use the \"binary_crossentropy\" loss.\\nIf you want to convert sparse labels (i.e., class indices) to one-hot\\nvector labels, you can use the keras.utils.to_categorical()\\nfunction. To go the other way round, you can just use the np.arg\\nmax() function with axis=1.\\nSecondly, regarding the optimizer, \"sgd\" simply means that we will train the model\\nusing simple Stochastic Gradient Descent. In other words, Keras will perform the\\nbackpropagation algorithm described earlier (i.e., reverse-mode autodiff + Gradient\\nDescent). We will discuss more efficient optimizers in Chapter 11 (they improve the\\nGradient Descent part, not the autodiff).\\nFinally, since this is a classifier, its useful to measure its \"accuracy\" during training\\nand evaluation.\\nTraining and Evaluating the Model\\nNow the model is ready to be trained. For this we simply need to call its fit()\\nmethod. We pass it the input features ( X_train) and the target classes ( y_train), as\\nwell as the number of epochs to train (or else it would default to just 1, which would\\ndefinitely not be enough to converge to a good solution). We also pass a validation set\\n(this is optional): Keras will measure the loss and the extra metrics on this set at the\\nend of each epoch, which is very useful to see how well the model really performs: if\\nthe performance on the training set is much better than on the validation set, your\\nImplementing MLPs with Keras | 299',\n",
       "  'model is probably overfitting the training set (or there is a bug, such as a data mis\\nmatch between the training set and the validation set):\\n>>> history = model.fit(X_train, y_train, epochs=30,\\n...                     validation_data=(X_valid, y_valid))\\n...\\nTrain on 55000 samples, validate on 5000 samples\\nEpoch 1/30\\n55000/55000 [==========] - 3s 55us/sample - loss: 1.4948     - acc: 0.5757\\n                                          - val_loss: 1.0042 - val_acc: 0.7166\\nEpoch 2/30\\n55000/55000 [==========] - 3s 55us/sample - loss: 0.8690     - acc: 0.7318\\n                                          - val_loss: 0.7549 - val_acc: 0.7616\\n[...]\\nEpoch 50/50\\n55000/55000 [==========] - 4s 72us/sample - loss: 0.3607     - acc: 0.8752\\n                                          - val_loss: 0.3706 - val_acc: 0.8728\\nAnd thats it! The neural network is trained. At each epoch during training, Keras dis\\nplays the number of instances processed so far (along with a progress bar), the mean\\ntraining time per sample, the loss and accuracy (or any other extra metrics you asked\\nfor), both on the training set and the validation set. Y ou can see that the training loss\\nwent down, which is a good sign, and the validation accuracy reached 87.28% after 50\\nepochs, not too far from the training accuracy, so there does not seem to be much\\noverfitting going on.\\nInstead of passing a validation set using the validation_data\\nargument, you could instead set validation_split to the ratio of\\nthe training set that you want Keras to use for validation (e.g., 0.1).\\nIf the training set was very skewed, with some classes being overrepresented and oth\\ners underrepresented, it would be useful to set the class_weight argument when\\ncalling the fit() method, giving a larger weight to underrepresented classes, and a\\nlower weight to overrepresented classes. These weights would be used by Keras when\\ncomputing the loss. If you need per-instance weights instead, you can set the sam\\nple_weight argument (it supersedes class_weight). This could be useful for exam\\nple if some instances were labeled by experts while others were labeled using a\\ncrowdsourcing platform: you might want to give more weight to the former. Y ou can\\nalso provide sample weights (but not class weights) for the validation set by adding\\nthem as a third item in the validation_data tuple.\\nThe fit() method returns a History object containing the training parameters ( his\\ntory.params), the list of epochs it went through ( history.epoch), and most impor\\ntantly a dictionary ( history.history) containing the loss and extra metrics it\\nmeasured at the end of each epoch on the training set and on the validation set (if\\n300 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'any). If you create a Pandas DataFrame using this dictionary and call its plot()\\nmethod, you get the learning curves shown in Figure 10-12:\\nimport pandas as pd\\npd.DataFrame(history.history).plot(figsize=(8, 5))\\nplt.grid(True)\\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\\nplt.show()\\nFigure 10-12. Learning Curves\\nY ou can see that both the training and validation accuracy steadily increase during\\ntraining, while the training and validation loss decrease. Good! Moreover, the valida\\ntion curves are quite close to the training curves, which means that there is not too\\nmuch overfitting. In this particular case, the model performed better on the valida\\ntion set than on the training set at the beginning of training: this sometimes happens\\nby chance (especially when the validation set is fairly small). However, the training set\\nperformance ends up beating the validation performance, as is generally the case\\nwhen you train for long enough. Y ou can tell that the model has not quite converged\\nyet, as the validation loss is still going down, so you should probably continue train\\ning. Its as simple as calling the fit() method again, since Keras just continues train\\ning where it left off (you should be able to reach close to 89% validation accuracy).\\nIf you are not satisfied with the performance of your model, you should go back and\\ntune the models hyperparameters, for example the number of layers, the number of\\nneurons per layer, the types of activation functions we use for each hidden layer, the\\nImplementing MLPs with Keras | 301',\n",
       "  \"number of training epochs, the batch size (it can be set in the fit() method using the\\nbatch_size argument, which defaults to 32). We will get back to hyperparameter\\ntuning at the end of this chapter. Once you are satisfied with your models validation\\naccuracy, you should evaluate it on the test set to estimate the generalization error\\nbefore you deploy the model to production. Y ou can easily do this using the evalu\\nate() method (it also supports several other arguments, such as batch_size or sam\\nple_weight, please check the documentation for more details):\\n>>> model.evaluate(X_test, y_test)\\n8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: 0.8540\\n[0.40738476498126985, 0.854]\\nAs we saw in Chapter 2, it is common to get slightly lower performance on the test set\\nthan on the validation set, because the hyperparameters are tuned on the validation\\nset, not the test set (however, in this example, we did not do any hyperparameter tun\\ning, so the lower accuracy is just bad luck). Remember to resist the temptation to\\ntweak the hyperparameters on the test set, or else your estimate of the generalization\\nerror will be too optimistic.\\nUsing the Model to Make Predictions\\nNext, we can use the models predict() method to make predictions on new instan\\nces. Since we dont have actual new instances, we will just use the first 3 instances of\\nthe test set:\\n>>> X_new = X_test[:3]\\n>>> y_proba = model.predict(X_new)\\n>>> y_proba.round(2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.79],\\n       [0.  , 0.  , 0.94, 0.  , 0.02, 0.  , 0.04, 0.  , 0.  , 0.  ],\\n       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\\n      dtype=float32)\\nAs you can see, for each instance the model estimates one probability per class, from\\nclass 0 to class 9. For example, for the first image it estimates that the probability of\\nclass 9 (ankle boot) is 79%, the probability of class 7 (sneaker) is 12%, the probability\\nof class 5 (sandal) is 9%, and the other classes are negligible. In other words, it\\nbelieves its footwear, probably ankle boots, but its not entirely sure, it might be\\nsneakers or sandals instead. If you only care about the class with the highest estima\\nted probability (even if that probability is quite low) then you can use the pre\\ndict_classes() method instead:\\n>>> y_pred = model.predict_classes(X_new)\\n>>> y_pred\\narray([9, 2, 1])\\n>>> np.array(class_names)[y_pred]\\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\\nAnd the classifier actually classified all three images correctly:\\n302 | Chapter 10: Introduction to Artificial Neural Networks with Keras\",\n",
       "  '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLets switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learns fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2,\\nsince it contains only numerical features (there is no ocean_proximity feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nhousing = fetch_california_housing()\\nX_train_full, X_test, y_train_full, y_test = train_test_split(\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split(\\n    X_train_full, y_train_full)\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_valid_scaled = scaler.transform(X_valid)\\nX_test_scaled = scaler.transform(X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential([\\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data=(X_valid, y_valid))\\nmse_test = model.evaluate(X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303',\n",
       "  '14 Wide & Deep Learning for Recommender Systems,  Heng-Tze Cheng et al. (2016).\\nAs you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep neural network.\\nThis neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13. This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor\\nmations.\\n304 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Figure 10-13. Wide and Deep Neural Network\\nLets build such a neural network to tackle the California housing problem:\\ninput = keras.layers.Input(shape=X_train.shape[1:])\\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\\nconcat = keras.layers.Concatenate()[input, hidden2])\\noutput = keras.layers.Dense(1)(concat)\\nmodel = keras.models.Model(inputs=[input], outputs=[output])\\nLets go through each line of this code:\\n First, we need to create an Input object. This is needed because we may have\\nmultiple inputs, as we will see later.\\n Next, we create a Dense layer with 30 neurons and using the ReLU activation\\nfunction. As soon as it is created, notice that we call it like a function, passing it\\nthe input. This is why this is called the Functional API. Note that we are just tell\\nImplementing MLPs with Keras | 305',\n",
       "  'ing Keras how it should connect the layers together, no actual data is being pro\\ncessed yet.\\n We then create a second hidden layer, and again we use it as a function. Note\\nhowever that we pass it the output of the first hidden layer.\\n Next, we create a Concatenate() layer, and once again we immediately use it like\\na function, to concatenate the input and the output of the second hidden layer\\n(you may prefer the keras.layers.concatenate() function, which creates a Con\\ncatenate layer and immediately calls it with the given inputs).\\n Then we create the output layer, with a single neuron and no activation function,\\nand we call it like a function, passing it the result of the concatenation.\\n Lastly, we create a Keras Model, specifying which inputs and outputs to use.\\nOnce you have built the Keras model, everything is exactly like earlier, so no need to\\nrepeat it here: you must compile the model, train it, evaluate it and use it to make\\npredictions.\\nBut what if you want to send a subset of the features through the wide path, and a\\ndifferent subset (possibly overlapping) through the deep path (see Figure 10-14)? In\\nthis case, one solution is to use multiple inputs. For example, suppose we want to\\nsend 5 features through the deep path (features 0 to 4), and 6 features through the\\nwide path (features 2 to 7):\\ninput_A = keras.layers.Input(shape=[5])\\ninput_B = keras.layers.Input(shape=[6])\\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\\nconcat = keras.layers.concatenate([input_A, hidden2])\\noutput = keras.layers.Dense(1)(concat)\\nmodel = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\\n306 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Figure 10-14. Handling Multiple Inputs\\nThe code is self-explanatory. Note that we specified inputs=[input_A, input_B]\\nwhen creating the model. Now we can compile the model as usual, but when we call\\nthe fit() method, instead of passing a single input matrix X_train, we must pass a\\npair of matrices (X_train_A, X_train_B) : one per input. The same is true for\\nX_valid, and also for X_test and X_new when you call evaluate() or predict():\\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\\nX_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\\nX_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\\nX_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\\nX_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\\nhistory = model.fit((X_train_A, X_train_B), y_train, epochs=20,\\n                    validation_data=((X_valid_A, X_valid_B), y_valid))\\nmse_test = model.evaluate((X_test_A, X_test_B), y_test)\\ny_pred = model.predict((X_new_A, X_new_B))\\nImplementing MLPs with Keras | 307',\n",
       "  'There are also many use cases in which you may want to have multiple outputs:\\n The task may demand it, for example you may want to locate and classify the\\nmain object in a picture. This is both a regression task (finding the coordinates of\\nthe objects center, as well as its width and height) and a classification task.\\n Similarly, you may have multiple independent tasks to perform based on the\\nsame data. Sure, you could train one neural network per task, but in many cases\\nyou will get better results on all tasks by training a single neural network with\\none output per task. This is because the neural network can learn features in the\\ndata that are useful across tasks.\\n Another use case is as a regularization technique (i.e., a training constraint whose\\nobjective is to reduce overfitting and thus improve the models ability to general\\nize). For example, you may want to add some auxiliary outputs in a neural net\\nwork architecture (see Figure 10-15 ) to ensure that the underlying part of the\\nnetwork learns something useful on its own, without relying on the rest of the\\nnetwork.\\nFigure 10-15. Handling Multiple Outputs  Auxiliary Output for Regularization\\nAdding extra outputs is quite easy: just connect them to the appropriate layers and\\nadd them to your models list of outputs. For example, the following code builds the\\nnetwork represented in Figure 10-15:\\n308 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg\\nularization), so we want to give the main outputs loss a much greater weight. Fortu\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train, we just need\\nto pass (y_train, y_train) (and the same goes for y_valid and y_test):\\nhistory = model.fit(\\n    [X_train_A, X_train_B], [y_train, y_train], epochs=20,\\n    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ\\nual losses:\\ntotal_loss, main_loss, aux_loss = model.evaluate(\\n    [X_test_A, X_test_B], [y_test, y_test])\\nSimilarly, the predict() method will return predictions for each output:\\ny_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Lets look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). Its also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: its static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309',\n",
       "  '15 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why\\nwe renamed it to main_output.\\nSimply subclass the Model class, create the layers you need in the constructor, and use\\nthem to perform the computations you want in the call() method. For example, cre\\nating an instance of the following WideAndDeepModel class gives us an equivalent\\nmodel to the one we just built with the Functional API. Y ou can then compile it, eval\\nuate it and use it to make predictions, exactly like we just did.\\nclass WideAndDeepModel(keras.models.Model):\\n    def __init__(self, units=30, activation=\"relu\", **kwargs):\\n        super().__init__(**kwargs) # handles standard args (e.g., name)\\n        self.hidden1 = keras.layers.Dense(units, activation=activation)\\n        self.hidden2 = keras.layers.Dense(units, activation=activation)\\n        self.main_output = keras.layers.Dense(1)\\n        self.aux_output = keras.layers.Dense(1)\\n    def call(self, inputs):\\n        input_A, input_B = inputs\\n        hidden1 = self.hidden1(input_B)\\n        hidden2 = self.hidden2(hidden1)\\n        concat = keras.layers.concatenate([input_A, hidden2])\\n        main_output = self.main_output(concat)\\n        aux_output = self.aux_output(hidden2)\\n        return main_output, aux_output\\nmodel = WideAndDeepModel()\\nThis example looks very much like the Functional API, except we do not need to cre\\nate the inputs, we just use the input argument to the call() method, and we separate\\nthe creation of the layers15 in the constructor from their usage in the call() method.\\nHowever, the big difference is that you can do pretty much anything you want in the\\ncall() method: for loops, if statements, low-level TensorFlow operations, your\\nimagination is the limit (see Chapter 12)! This makes it a great API for researchers\\nexperimenting with new ideas.\\nHowever, this extra flexibility comes at a cost: your models architecture is hidden\\nwithin the call() method, so Keras cannot easily inspect it, it cannot save or clone it,\\nand when you call the summary() method, you only get a list of layers, without any\\ninformation on how they are connected to each other. Moreover, Keras cannot check\\ntypes and shapes ahead of time, and it is easier to make mistakes. So unless you really\\nneed that extra flexibility, you should probably stick to the Sequential API or the\\nFunctional API.\\n310 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\")\\nKeras will save both the models architecture (including every layers hyperparame\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model(\"my_keras_model.h5\")\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How\\never, you can use save_weights() and load_weights() to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit() method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit() method accepts a callbacks argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311',\n",
       "  '[...] # build and compile the model\\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\\nhistory = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\\nMoreover, if you use a validation set during training, you can set\\nsave_best_only=True when creating the ModelCheckpoint. In this case, it will only\\nsave your model when its performance on the validation set is the best so far. This\\nway, you do not need to worry about training for too long and overfitting the training\\nset: simply restore the last model saved after training, and this will be the best model\\non the validation set. This is a simple way to implement early stopping (introduced in\\nChapter 4):\\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\\n                                                save_best_only=True)\\nhistory = model.fit(X_train, y_train, epochs=10,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[checkpoint_cb])\\nmodel = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\\nAnother way to implement early stopping is to simply use the EarlyStopping call\\nback. It will interrupt training when it measures no progress on the validation set for\\na number of epochs (defined by the patience argument), and it will optionally roll\\nback to the best model. Y ou can combine both callbacks to both save checkpoints of\\nyour model (in case your computer crashes), and actually interrupt training early\\nwhen there is no more progress (to avoid wasting time and resources):\\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\\n                                                  restore_best_weights=True)\\nhistory = model.fit(X_train, y_train, epochs=100,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[checkpoint_cb, early_stopping_cb])\\nThe number of epochs can be set to a large value since training will stop automati\\ncally when there is no more progress. Moreover, there is no need to restore the best\\nmodel saved in this case since the EarlyStopping callback will keep track of the best\\nweights and restore them for us at the end of training.\\nThere are many other callbacks available in the keras.callbacks\\npackage. See https://keras.io/callbacks/.\\nIf you need extra control, you can easily write your own custom callbacks. For exam\\nple, the following custom callback will display the ratio between the validation loss\\nand the training loss during training (e.g., to detect overfitting):\\n312 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'class PrintValTrainRatioCallback(keras.callbacks.Callback):\\n    def on_epoch_end(self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin(), on_train_end(),\\non_epoch_begin(), on_epoch_begin(), on_batch_end() and on_batch_end().\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin(), on_test_end(), on_test_batch_begin(), or\\non_test_batch_end() (called by evaluate()), or on_predict_begin(), on_pre\\ndict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by\\npredict()).\\nNow lets take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary. The TensorBoard server will monitor the log directory, and it will automat\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo lets start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir = os.path.join(os.curdir, \"my_logs\")\\ndef get_run_logdir():\\n    import time\\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir, run_id)\\nImplementing MLPs with Keras | 313',\n",
       "  'run_logdir = get_run_logdir() # e.g., \\'./my_logs/run_2019_01_16-11_28_43\\'\\nNext, the good news is that Keras provides a nice TensorBoard callback:\\n[...] # Build and compile your model\\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\\nhistory = model.fit(X_train, y_train, epochs=30,\\n                    validation_data=(X_valid, y_valid),\\n                    callbacks=[tensorboard_cb])\\nAnd thats all there is to it! It could hardly be easier to use. If you run this code, the\\nTensorBoard callback will take care of creating the log directory for you (along with\\nits parent directories if needed), and during training it will create event files and write\\nsummaries to them. After running the program a second time (perhaps changing\\nsome hyperparameter value), you will end up with a directory structure similar to\\nthis one:\\nmy_logs\\n run_2019_01_16-16_51_02\\n    events.out.tfevents.1547628669.mycomputer.local.v2\\n run_2019_01_16-16_56_50\\n     events.out.tfevents.1547629020.mycomputer.local.v2\\nNext you need to start the TensorBoard server. If you installed TensorFlow within a\\nvirtualenv, you should activate it. Next, run the following command at the root of the\\nproject (or from anywhere else as long as you point to the appropriate log directory).\\nIf your shell cannot find the tensorboard script, then you must update your PATH\\nenvironment variable so that it contains the directory in which the script was\\ninstalled (alternatively, you can just replace tensorboard with python3 -m tensor\\nboard.main).\\n$ tensorboard --logdir=./my_logs --port=6006\\nTensorBoard 2.0.0 at http://mycomputer.local:6006 (Press CTRL+C to quit)\\nFinally, open up a web browser to http://localhost:6006. Y ou should see TensorBoards\\nweb interface. Click on the SCALARS tab to view the learning curves (see\\nFigure 10-16). Notice that the training loss went down nicely during both runs, but\\nthe second run went down much faster. Indeed, we used a larger learning rate by set\\nting optimizer=keras.optimizers.SGD(lr=0.05) instead of optimizer=\"sgd\",\\nwhich defaults to a learning rate of 0.001.\\n314 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard) to see all the options.\\nLets summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.kerass Sequential API, or more complex architectures\\nusing the Functional API or Model Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame\\nters. Lets look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi\\nFine-Tuning Neural Network Hyperparameters | 315',\n",
       "  'alization logic, and much more. How do you know what combination of hyperpara\\nmeters is the best for your task?\\nOne option is to simply try many combinations of hyperparameters and see which\\none works best on the validation set (or using K-fold cross-validation). For this, one\\napproach is simply use GridSearchCV or RandomizedSearchCV to explore the hyper\\nparameter space, as we did in Chapter 2. For this, we need to wrap our Keras models\\nin objects that mimic regular Scikit-Learn regressors. The first step is to create a func\\ntion that will build and compile a Keras model, given a set of hyperparameters:\\ndef build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\\n    model = keras.models.Sequential()\\n    options = {\"input_shape\": input_shape}\\n    for layer in range(n_hidden):\\n        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\\n        options = {}\\n    model.add(keras.layers.Dense(1, **options))\\n    optimizer = keras.optimizers.SGD(learning_rate)\\n    model.compile(loss=\"mse\", optimizer=optimizer)\\n    return model\\nThis function creates a simple Sequential model for univariate regression (only one\\noutput neuron), with the given input shape and the given number of hidden layers\\nand neurons, and it compiles it using an SGD optimizer configured with the given\\nlearning rate. The options dict is used to ensure that the first layer is properly given\\nthe input shape (note that if n_hidden=0, the first layer will be the output layer). It is\\ngood practice to provide reasonable defaults to as many hyperparameters as you can,\\nas Scikit-Learn does.\\nNext, lets create a KerasRegressor based on this build_model() function:\\nkeras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\\nThe KerasRegressor object is a thin wrapper around the Keras model built using\\nbuild_model(). Since we did not specify any hyperparameter when creating it, it will\\njust use the default hyperparameters we defined in build_model(). Now we can use\\nthis object like a regular Scikit-Learn regressor: we can train it using its fit()\\nmethod, then evaluate it using its score() method, and use it to make predictions\\nusing its predict() method. Note that any extra parameter you pass to the fit()\\nmethod will simply get passed to the underlying Keras model. Also note that the\\nscore will be the opposite of the MSE because Scikit-Learn wants scores, not losses\\n(i.e., higher should be better).\\nkeras_reg.fit(X_train, y_train, epochs=100,\\n              validation_data=(X_valid, y_valid),\\n              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\\nmse_test = keras_reg.score(X_test, y_test)\\ny_pred = keras_reg.predict(X_new)\\n316 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'However, we do not actually want to train and evaluate a single model like this, we\\nwant to train hundreds of variants and see which one performs best on the validation\\nset. Since there are many hyperparameters, it is preferable to use a randomized search\\nrather than grid search (as we discussed in Chapter 2). Lets try to explore the number\\nof hidden layers, the number of neurons and the learning rate:\\nfrom scipy.stats import reciprocal\\nfrom sklearn.model_selection import RandomizedSearchCV\\nparam_distribs = {\\n    \"n_hidden\": [0, 1, 2, 3],\\n    \"n_neurons\": np.arange(1, 100),\\n    \"learning_rate\": reciprocal(3e-4, 3e-2),\\n}\\nrnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\\nrnd_search_cv.fit(X_train, y_train, epochs=100,\\n                  validation_data=(X_valid, y_valid),\\n                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\\nAs you can see, this is identical to what we did in Chapter 2, with the exception that\\nwe pass extra parameters to the fit() method: they simply get relayed to the under\\nlying Keras models. Note that RandomizedSearchCV uses K-fold cross-validation, so it\\ndoes not use X_valid and y_valid. These are just used for early stopping.\\nThe exploration may last many hours depending on the hardware, the size of the\\ndataset, the complexity of the model and the value of n_iter and cv. When it is over,\\nyou can access the best parameters found, the best score, and the trained Keras model\\nlike this:\\n>>> rnd_search_cv.best_params_\\n{\\'learning_rate\\': 0.0033625641252688094, \\'n_hidden\\': 2, \\'n_neurons\\': 42}\\n>>> rnd_search_cv.best_score_\\n-0.3189529188278931\\n>>> model = rnd_search_cv.best_estimator_.model\\nY ou can now save this model, evaluate it on the test set, and if you are satisfied with\\nits performance, deploy it to production. Using randomized search is not too hard,\\nand it works well for many fairly simple problems. However, when training is slow\\n(e.g., for more complex problems with larger datasets), this approach will only\\nexplore a tiny portion of the hyperparameter space. Y ou can partially alleviate this\\nproblem by assisting the search process manually: first run a quick random search\\nusing wide ranges of hyperparameter values, then run another search using smaller\\nranges of values centered on the best ones found during the first run, and so on. This\\nwill hopefully zoom in to a good set of hyperparameters. However, this is very time\\nconsuming, and probably not the best use of your time.\\nFortunately, there are many techniques to explore a search space much more effi\\nciently than randomly. Their core idea is simple: when a region of the space turns out\\nFine-Tuning Neural Network Hyperparameters | 317',\n",
       "  '16 Population Based Training of Neural Networks,  Max Jaderberg et al. (2017).\\nto be good, it should be explored more. This takes care of the zooming process for\\nyou and leads to much better solutions in much less time. Here are a few Python\\nlibraries you can use to optimize hyperparameters:\\n Hyperopt: a popular Python library for optimizing over all sorts of complex\\nsearch spaces (including real values such as the learning rate, or discrete values\\nsuch as the number of layers).\\n Hyperas, kopt or Talos: optimizing hyperparameters for Keras model (the first\\ntwo are based on Hyperopt).\\n Scikit-Optimize (skopt): a general-purpose optimization library. The Bayes\\nSearchCV class performs Bayesian optimization using an interface similar to Grid\\nSearchCV.\\n Spearmint: a Bayesian optimization library.\\n Sklearn-Deap: a hyperparameter optimization library based on evolutionary\\nalgorithms, also with a GridSearchCV-like interface.\\n And many more!\\nMoreover, many companies offer services for hyperparameter optimization. For\\nexample Google Cloud ML Engine has a hyperparameter tuning service. Other com\\npanies provide APIs for hyperparameter optimization, such as Arimo, SigOpt, Oscar\\nand many more.\\nHyperparameter tuning is still an active area of research. Evolutionary algorithms are\\nmaking a comeback lately. For example, check out DeepMinds excellent 2017 paper16,\\nwhere they jointly optimize a population of models and their hyperparameters. Goo\\ngle also used an evolutionary approach, not just to search for hyperparameters, but\\nalso to look for the best neural network architecture for the problem. They call this\\nAutoML, and it is already available as a cloud service . Perhaps the days of building\\nneural networks manually will soon be over? Check out Googles post on this topic. In\\nfact, evolutionary algorithms have also been used successfully to train individual neu\\nral networks, replacing the ubiquitous Gradient Descent! See this 2017 post by Uber\\nwhere they introduce their Deep Neuroevolution technique.\\nDespite all this exciting progress, and all these tools and services, it still helps to have\\nan idea of what values are reasonable for each hyperparameter, so you can build a\\nquick prototype, and restrict the search space. Here are a few guidelines for choosing\\nthe number of hidden layers and neurons in an MLP , and selecting good values for\\nsome of the main hyperparameters.\\n318 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning.\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319',\n",
       "  '17 By Vincent Vanhoucke in his Deep Learning class on Udacity.com.\\nreuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap\\nter 11).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra\\nmid, with fewer and fewer neurons at each layerthe rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layerfor example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu\\nlarization techniques, such as dropout, as we will see in Chapter 11). This has been\\ndubbed the stretch pants approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn\\n320 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  '18 Practical recommendations for gradient-based training of deep architectures,  Y oshua Bengio (2012).\\ning rate above which the training algorithm diverges, as we saw in Chapter 4). So\\na simple approach for tuning the learning rate is to start with a large value that\\nmakes the training algorithm diverge, then divide this value by 3 and try again,\\nand repeat until the training algorithm stops diverging. At that point, you gener\\nally wont be too far from the optimal learning rate. That said, it is sometimes\\nuseful to reduce the learning rate during training: we will discuss this in Chap\\nter 11.\\n Choosing a better optimizer than plain old Mini-batch Gradient Descent (and\\ntuning its hyperparameters) is also quite important. We will discuss this in Chap\\nter 11.\\n The batch size can also have a significant impact on your models performance\\nand the training time. In general the optimal batch size will be lower than 32 (in\\nApril 2018, Y ann Lecun even tweeted \" Friends dont let friends use mini-batches\\nlarger than 32 ). A small batch size ensures that each training iteration is very\\nfast, and although a large batch size will give a more precise estimate of the gradi\\nents, in practice this does not matter much since the optimization landscape is\\nquite complex and the direction of the true gradients do not point precisely in\\nthe direction of the optimum. However, having a batch size greater than 10 helps\\ntake advantage of hardware and software optimizations, in particular for matrix\\nmultiplications, so it will speed up training. Moreover, if you use Batch Normal\\nization (see Chapter 11), the batch size should not be too small (in general no less\\nthan 20).\\n We discussed the choice of the activation function earlier in this chapter: in gen\\neral, the ReLU activation function will be a good default for all hidden layers. For\\nthe output layer, it really depends on your task.\\n In most cases, the number of training iterations does not actually need to be\\ntweaked: just use early stopping instead.\\nFor more best practices, make sure to read Y oshua Bengios great 2012 paper18, which\\npresents many practical recommendations for deep networks.\\nThis concludes this introduction to artificial neural networks and their implementa\\ntion with Keras. In the next few chapters, we will discuss techniques to train very\\ndeep nets, we will see how to customize your models using TensorFlows lower-level\\nAPI and how to load and preprocess data efficiently using the Data API, and we will\\ndive into other popular neural network architectures: convolutional neural networks\\nfor image processing, recurrent neural networks for sequential data, autoencoders for\\nFine-Tuning Neural Network Hyperparameters | 321',\n",
       "  '19 A few extra ANN architectures are presented in ???.\\nrepresentation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1. Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n Too small: now remove one neuron to keep just 2. Notice that the neural net\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n Large enough: next, set the number of neurons to 8 and train the network sev\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n Deep net and vanishing gradients: now change the dataset to be the spiral (bot\\ntom right dataset under DATA ). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu\\nrons in the lowest layers (i.e. on the left). This problem, called the vanishing\\ngradients problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'other techniques, better optimizers (such as AdaGrad or Adam), or using\\nBatch Normalization.\\n More: go ahead and play with the other parameters to get a feel of what they\\ndo. In fact, you should definitely play with this UI for at least one hour, it will\\ngrow your intuitions about neural networks significantly.\\n2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\\nthat computes A  B (where  represents the XOR operation). Hint: A  B = (A\\n  B)  ( A  B).\\n3. Why is it generally preferable to use a Logistic Regression classifier rather than a\\nclassical Perceptron (i.e., a single layer of threshold logic units trained using the\\nPerceptron training algorithm)? How can you tweak a Perceptron to make it\\nequivalent to a Logistic Regression classifier?\\n4. Why was the logistic activation function a key ingredient in training the first\\nMLPs?\\n5. Name three popular activation functions. Can you draw them?\\n6. Suppose you have an MLP composed of one input layer with 10 passthrough\\nneurons, followed by one hidden layer with 50 artificial neurons, and finally one\\noutput layer with 3 artificial neurons. All artificial neurons use the ReLU activa\\ntion function.\\n What is the shape of the input matrix X?\\n What about the shape of the hidden layers weight vector Wh, and the shape of\\nits bias vector bh?\\n What is the shape of the output layers weight vector Wo, and its bias vector bo?\\n What is the shape of the networks output matrix Y?\\n Write the equation that computes the networks output matrix Y as a function\\nof X, Wh, bh, Wo and bo.\\n7. How many neurons do you need in the output layer if you want to classify email\\ninto spam or ham? What activation function should you use in the output layer?\\nIf instead you want to tackle MNIST, how many neurons do you need in the out\\nput layer, using what activation function? Answer the same questions for getting\\nyour network to predict housing prices as in Chapter 2.\\n8. What is backpropagation and how does it work? What is the difference between\\nbackpropagation and reverse-mode autodiff?\\n9. Can you list all the hyperparameters you can tweak in an MLP? If the MLP over\\nfits the training data, how could you tweak these hyperparameters to try to solve\\nthe problem?\\nExercises | 323',\n",
       "  '10. Train a deep MLP on the MNIST dataset and see if you can get over 98% preci\\nsion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop\\nping, plot learning curves using TensorBoard, and so on).\\nSolutions to these exercises are available in ???.\\n324 | Chapter 10: Introduction to Artificial Neural Networks with Keras',\n",
       "  'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n First, you would be faced with the tricky vanishing gradients problem (or the\\nrelated exploding gradients problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n Third, training may be extremely slow.\\n Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325',\n",
       "  '1 Understanding the Difficulty of Training Deep Feedforward Neural Networks,  X. Glorot, Y Bengio (2010).\\ntasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10, the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients prob\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand\\ning it. A paper titled Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks',\n",
       "  '2 Heres an analogy: if you set a microphone amplifiers knob too close to zero, people wont hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people wont understand what you are say\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.\\nLooking at the logistic activation function (see Figure 11-1), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi\\nents. We dont want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs, 2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in and fan-out of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327',\n",
       "  '3 Such as Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,  K.\\nHe et al. (2015).\\ndescribed in Equation 11-1 , where f anavg = f anin + f anout /2. This initialization\\nstrategy is called Xavier initialization (after the authors first name) or Glorot initiali\\nzation (after his last name).\\nEquation 11-1. Glorot initialization (when using the logistic activation function)\\nNormal distribution with mean 0 and variance2 = 1\\nfanavg\\nOr a uniform distribution betweenrand + r, withr = 3\\nfanavg\\nIf you just replace fanavg with fanin in Equation 11-1, you get an initialization strategy\\nthat was actually already proposed by Y ann LeCun in the 1990s, called LeCun initiali\\nzation, which was even recommended in the 1998 book Neural Networks: Tricks of the\\nTrade by Genevieve Orr and Klaus-Robert Mller (Springer). It is equivalent to\\nGlorot initialization when fanin = fanout. It took over a decade for researchers to realize\\njust how important this trick really is. Using Glorot initialization can speed up train\\ning considerably, and it is one of the tricks that led to the current success of Deep\\nLearning.\\nSome papers3 have provided similar strategies for different activation functions.\\nThese strategies differ only by the scale of the variance and whether they use fanavg or\\nfanin, as shown in Table 11-1 (for the uniform distribution, just compute r = 32).\\nThe initialization strategy for the ReLU activation function (and its variants, includ\\ning the ELU activation described shortly) is sometimes called He initialization (after\\nthe last name of its author). The SELU activation function will be explained later in\\nthis chapter. It should be used with LeCun initialization (preferably with a normal\\ndistribution, as we will see).\\nTable 11-1. Initialization parameters for each type of activation function\\nInitialization Activation functions  (Normal)\\nGlorot None, Tanh, Logistic, Softmax 1 / fanavg\\nHe ReLU & variants 2 / fanin\\nLeCun SELU 1 / fanin\\nBy default, Keras uses Glorot initialization with a uniform distribution. Y ou can\\nchange this to He initialization by setting kernel_initializer=\"he_uniform\" or ker\\nnel_initializer=\"he_normal\" when creating a layer, like this:\\n328 | Chapter 11: Training Deep Neural Networks',\n",
       "  '4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neurons\\ninputs is positive again.\\n5 Empirical Evaluation of Rectified Activations in Convolution Network,  B. Xu et al. (2015).\\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling initializer like this:\\nhe_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\\'fan_avg\\',\\n                                                 distribution=\\'uniform\\')\\nkeras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs: during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour networks neurons are dead, especially if you used a large learning rate. A neu\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLU (z) = max( z, z) (see\\nFigure 11-2). The hyperparameter  defines how much the function leaks: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting  = 0.2 (huge leak) seemed to\\nresult in better performance than  = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU (RReLU), where  is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329',\n",
       "  '6 Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),  D. Clevert, T. Unterthiner,\\nS. Hochreiter (2015).\\nFinally, they also evaluated the parametric leaky ReLU (PReLU), where  is authorized\\nto be learned during training (instead of being a hyperparameter, it becomes a\\nparameter that can be modified by backpropagation like any other parameter). This\\nwas reported to strongly outperform ReLU on large image datasets, but on smaller\\ndatasets it runs the risk of overfitting the training set.\\nFigure 11-2. Leaky ReLU\\nLast but not least, a 2015 paper by Djork-Arn Clevert et al. 6 proposed a new activa\\ntion function called the exponential linear unit (ELU) that outperformed all the ReLU\\nvariants in their experiments: training time was reduced and the neural network per\\nformed better on the test set. It is represented in Figure 11-3 , and Equation 11-2\\nshows its definition.\\nEquation 11-2. ELU activation function\\nELU z =  exp z  1 if z < 0\\nz if z  0\\n330 | Chapter 11: Training Deep Neural Networks',\n",
       "  '7 Self-Normalizing Neural Networks, \" G. Klambauer, T. Unterthiner and A. Mayr (2017).\\nFigure 11-3. ELU activation function\\nIt looks a lot like the ReLU function, with a few major differences:\\n First it takes on negative values when z < 0, which allows the unit to have an\\naverage output closer to 0. This helps alleviate the vanishing gradients problem,\\nas discussed earlier. The hyperparameter  defines the value that the ELU func\\ntion approaches when z is a large negative number. It is usually set to 1, but you\\ncan tweak it like any other hyperparameter if you want.\\n Second, it has a nonzero gradient for z < 0, which avoids the dead neurons prob\\nlem.\\n Third, if  is equal to 1 then the function is smooth everywhere, including\\naround z = 0, which helps speed up Gradient Descent, since it does not bounce as\\nmuch left and right of z = 0.\\nThe main drawback of the ELU activation function is that it is slower to compute\\nthan the ReLU and its variants (due to the use of the exponential function), but dur\\ning training this is compensated by the faster convergence rate. However, at test time\\nan ELU network will be slower than a ReLU network.\\nMoreover, in a 2017 paper 7 by Gnter Klambauer et al., called Self-Normalizing\\nNeural Networks , the authors showed that if you build a neural network composed\\nexclusively of a stack of dense layers, and if all hidden layers use the SELU activation\\nfunction (which is just a scaled version of the ELU activation function, as its name\\nsuggests), then the network will self-normalize: the output of each layer will tend to\\npreserve mean 0 and standard deviation 1 during training, which solves the vanish\\ning/exploding gradients problem. As a result, this activation function often outper\\nVanishing/Exploding Gradients Problems | 331',\n",
       "  'forms other activation functions very significantly for such neural nets (especially\\ndeep ones). However, there are a few conditions for self-normalization to happen:\\n The input features must be standardized (mean 0 and standard deviation 1).\\n Every hidden layers weights must also be initialized using LeCun normal initiali\\nzation. In Keras, this means setting kernel_initializer=\"lecun_normal\".\\n The networks architecture must be sequential. Unfortunately, if you try to use\\nSELU in non-sequential architectures, such as recurrent networks (see ???) or\\nnetworks with skip connections (i.e., connections that skip layers, such as in wide\\n& deep nets), self-normalization will not be guaranteed, so SELU will not neces\\nsarily outperform other activation functions.\\n The paper only guarantees self-normalization if all layers are dense. However, in\\npractice the SELU activation function seems to work great with convolutional\\nneural nets as well (see Chapter 14).\\nSo which activation function should you use for the hidden layers\\nof your deep neural networks? Although your mileage will vary, in\\ngeneral SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\\n> logistic. If the networks architecture prevents it from self-\\nnormalizing, then ELU may perform better than SELU (since SELU\\nis not smooth at z = 0). If you care a lot about runtime latency, then\\nyou may prefer leaky ReLU. If you dont want to tweak yet another\\nhyperparameter, you may just use the default  values used by\\nKeras (e.g., 0.3 for the leaky ReLU). If you have spare time and\\ncomputing power, you can use cross-validation to evaluate other\\nactivation functions, in particular RReLU if your network is over\\nfitting, or PReLU if you have a huge training set.\\nTo use the leaky ReLU activation function, you must create a LeakyReLU instance like\\nthis:\\nleaky_relu = keras.layers.LeakyReLU(alpha=0.2)\\nlayer = keras.layers.Dense(10, activation=leaky_relu,\\n                           kernel_initializer=\"he_normal\")\\nFor PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\\nofficial implementation of RReLU in Keras, but you can fairly easily implement your\\nown (see the exercises at the end of Chapter 12).\\nFor SELU activation, just set activation=\"selu\" and kernel_initial\\nizer=\"lecun_normal\" when creating a layer:\\nlayer = keras.layers.Dense(10, activation=\"selu\",\\n                           kernel_initializer=\"lecun_normal\")\\n332 | Chapter 11: Training Deep Neural Networks',\n",
       "  '8 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,  S. Ioffe\\nand C. Szegedy (2015).\\nBatch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train\\ning, it doesnt guarantee that they wont come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layers inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach inputs mean and standard deviation. It does so by evaluating the mean and stan\\ndard deviation of each input over the current mini-batch (hence the name Batch\\nNormalization). The whole operation is summarized in Equation 11-3.\\nEquation 11-3. Batch Normalization algorithm\\n1 . B = 1\\nmB\\n\\ni = 1\\nmB\\nx i\\n2 . B\\n2 = 1\\nmB\\n\\ni = 1\\nmB\\nx i  B\\n2\\n3 . x i =\\nx i  B\\nB\\n2 + \\n4 . z i =   x i + \\n B is the vector of input means, evaluated over the whole mini-batch B (it con\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333',\n",
       "  ' B is the vector of input standard deviations, also evaluated over the whole mini-\\nbatch (it contains one standard deviation per input).\\n mB is the number of instances in the mini-batch.\\n x(i) is the vector of zero-centered and normalized inputs for instance i.\\n  is the output scale parameter vector for the layer (it contains one scale parame\\nter per input).\\n  represents element-wise multiplication (each input is multiplied by its corre\\nsponding output scale parameter).\\n  is the output shift (offset) parameter vector for the layer (it contains one offset\\nparameter per input). Each input is offset by its corresponding shift parameter.\\n  is a tiny number to avoid division by zero (typically 10 5). This is called a\\nsmoothing term.\\n z(i) is the output of the BN operation: it is a rescaled and shifted version of the\\ninputs.\\nSo during training, BN just standardizes its inputs then rescales and offsets them.\\nGood! What about at test time? Well it is not that simple. Indeed, we may need to\\nmake predictions for individual instances rather than for batches of instances: in this\\ncase, we will have no way to compute each inputs mean and standard deviation.\\nMoreover, even if we do have a batch of instances, it may be too small, or the instan\\nces may not be independent and identically distributed (IID), so computing statistics\\nover the batch instances would be unreliable (during training, the batches should not\\nbe too small, if possible more than 30 instances, and all instances should be IID, as we\\nsaw in Chapter 4). One solution could be to wait until the end of training, then run\\nthe whole training set through the neural network, and compute the mean and stan\\ndard deviation of each input of the BN layer. These final input means and standard\\ndeviations can then be used instead of the batch input means and standard deviations\\nwhen making predictions. However, it is often preferred to estimate these final statis\\ntics during training using a moving average of the layers input means and standard\\ndeviations. To sum up, four parameter vectors are learned in each batch-normalized\\nlayer:  (the ouput scale vector) and  (the output offset vector) are learned through\\nregular backpropagation, and  (the final input mean vector), and  (the final input\\nstandard deviation vector) are estimated using an exponential moving average. Note\\nthat  and  are estimated during training, but they are not used at all during train\\ning, only after training (to replace the batch input means and standard deviations in\\nEquation 11-3).\\nThe authors demonstrated that this technique considerably improved all the deep\\nneural networks they experimented with, leading to a huge improvement in the\\nImageNet classification task (ImageNet is a large database of images classified into\\nmany classes and commonly used to evaluate computer vision systems). The vanish\\n334 | Chapter 11: Training Deep Neural Networks',\n",
       "  'ing gradients problem was strongly reduced, to the point that they could use saturat\\ning activation functions such as the tanh and even the logistic activation function.\\nThe networks were also much less sensitive to the weight initialization. They were\\nable to use much larger learning rates, significantly speeding up the learning process.\\nSpecifically, they note that  Applied to a state-of-the-art image classification model,\\nBatch Normalization achieves the same accuracy with 14 times fewer training steps,\\nand beats the original model by a significant margin. [] Using an ensemble of\\nbatch-normalized networks, we improve upon the best published result on ImageNet\\nclassification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding\\nthe accuracy of human raters.  Finally, like a gift that keeps on giving, Batch Normal\\nization also acts like a regularizer, reducing the need for other regularization techni\\nques (such as dropout, described later in this chapter).\\nBatch Normalization does, however, add some complexity to the model (although it\\ncan remove the need for normalizing the input data, as we discussed earlier). More\\nover, there is a runtime penalty: the neural network makes slower predictions due to\\nthe extra computations required at each layer. So if you need predictions to be\\nlightning-fast, you may want to check how well plain ELU + He initialization perform\\nbefore playing with Batch Normalization.\\nY ou may find that training is rather slow, because each epoch takes\\nmuch more time when you use batch normalization. However, this\\nis usually counterbalanced by the fact that convergence is much\\nfaster with BN, so it will take fewer epochs to reach the same per\\nformance. All in all, wall time will usually be smaller (this is the\\ntime measured by the clock on your wall).\\nImplementing Batch Normalization with Keras\\nAs with most things with Keras, implementing Batch Normalization is quite simple.\\nJust add a BatchNormalization layer before or after each hidden layers activation\\nfunction, and optionally add a BN layer as well as the first layer in your model. For\\nexample, this model applies BN after every hidden layer and as the first layer in the\\nmodel (after flattening the input images):\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nVanishing/Exploding Gradients Problems | 335',\n",
       "  '9 However, they are estimated during training, based on the training data, so arguably they are trainable. In\\nKeras, Non-trainable really means untouched by backpropagation .\\nThats all! In this tiny example with just two hidden layers, its unlikely that Batch\\nNormalization will have a very positive impact, but for deeper networks it can make a\\ntremendous difference.\\nLets zoom in a bit. If you display the model summary, you can see that each BN layer\\nadds 4 parameters per input: , ,  and  (for example, the first BN layer adds 3136\\nparameters, which is 4 times 784). The last two parameters,  and , are the moving\\naverages, they are not affected by backpropagation, so Keras calls them Non-\\ntrainable9 (if you count the total number of BN parameters, 3136 + 1200 + 400, and\\ndivide by two, you get 2,368, which is the total number of non-trainable params in\\nthis model).\\n>>> model.summary()\\nModel: \"sequential_3\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #\\n=================================================================\\nflatten_3 (Flatten)          (None, 784)               0\\n_________________________________________________________________\\nbatch_normalization_v2 (Batc (None, 784)               3136\\n_________________________________________________________________\\ndense_50 (Dense)             (None, 300)               235500\\n_________________________________________________________________\\nbatch_normalization_v2_1 (Ba (None, 300)               1200\\n_________________________________________________________________\\ndense_51 (Dense)             (None, 100)               30100\\n_________________________________________________________________\\nbatch_normalization_v2_2 (Ba (None, 100)               400\\n_________________________________________________________________\\ndense_52 (Dense)             (None, 10)                1010\\n=================================================================\\nTotal params: 271,346\\nTrainable params: 268,978\\nNon-trainable params: 2,368\\nLets look at the parameters of the first BN layer. Two are trainable (by backprop), and\\ntwo are not:\\n>>> [(var.name, var.trainable) for var in model.layers[1].variables]\\n[(\\'batch_normalization_v2/gamma:0\\', True),\\n (\\'batch_normalization_v2/beta:0\\', True),\\n (\\'batch_normalization_v2/moving_mean:0\\', False),\\n (\\'batch_normalization_v2/moving_variance:0\\', False)]\\nNow when you create a BN layer in Keras, it also creates two operations that will be\\ncalled by Keras at each iteration during training. These operations will update the\\n336 | Chapter 11: Training Deep Neural Networks',\n",
       "  'moving averages. Since we are using the TensorFlow backend, these operations are\\nTensorFlow operations (we will discuss TF operations in Chapter 12).\\n>>> model.layers[1].updates\\n[<tf.Operation \\'cond_2/Identity\\' type=Identity>,\\n <tf.Operation \\'cond_3/Identity\\' type=Identity>]\\nThe authors of the BN paper argued in favor of adding the BN layers before the acti\\nvation functions, rather than after (as we just did). There is some debate about this, as\\nit seems to depend on the task. So thats one more thing you can experiment with to\\nsee which option works best on your dataset. To add the BN layers before the activa\\ntion functions, we must remove the activation function from the hidden layers, and\\nadd them as separate layers after the BN layers. Moreover, since a Batch Normaliza\\ntion layer includes one offset parameter per input, you can remove the bias term from\\nthe previous layer (just pass use_bias=False when creating it):\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Activation(\"elu\"),\\n    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\\n    keras.layers.Activation(\"elu\"),\\n    keras.layers.BatchNormalization(),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nThe BatchNormalization class has quite a few hyperparameters you can tweak. The\\ndefaults will usually be fine, but you may occasionally need to tweak the momentum.\\nThis hyperparameter is used when updating the exponential moving averages: given a\\nnew value v (i.e., a new vector of input means or standard deviations computed over\\nthe current batch), the running average  is updated using the following equation:\\nv v  momentum + v  1  momentum\\nA good momentum value is typically close to 1for example, 0.9, 0.99, or 0.999 (you\\nwant more 9s for larger datasets and smaller mini-batches).\\nAnother important hyperparameter is axis: it determines which axis should be nor\\nmalized. It defaults to 1, meaning that by default it will normalize the last axis (using\\nthe means and standard deviations computed across the other axes). For example,\\nwhen the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\\nthat each input feature will be normalized based on the mean and standard deviation\\ncomputed across all the instances in the batch. For example, the first BN layer in the\\nprevious code example will independently normalize (and rescale and shift) each of\\nthe 784 input features. However, if we move the first BN layer before the Flatten\\nVanishing/Exploding Gradients Problems | 337',\n",
       "  '10 Fixup Initialization: Residual Learning Without Normalization,  Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11 On the difficulty of training recurrent neural networks,  R. Pascanu et al. (2013).\\nlayer, then the input batches will be 3D, with shape [batch size, height, width], there\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2].\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the final statistics after\\ntraining (i.e., the final value of the moving averages). Lets take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization(Layer):\\n    [...]\\n    def call(self, inputs, training=None):\\n        if training is None:\\n            training = keras.backend.learning_phase()\\n        [...]\\nThe call() method is the one that actually performs the computations, and as you\\ncan see it has an extra training argument: if it is None it falls back to keras.back\\nend.learning_phase(), which returns 1 during training (the fit() method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12).\\nBatch Normalization has become one of the most used layers in deep neural net\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay\\ners!) without BN, achieving state-of-the-art performance on complex image classifi\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping.11 This technique is most often used in recurrent neu\\n338 | Chapter 11: Training Deep Neural Networks',\n",
       "  'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\\nclipnorm argument when creating an optimizer. For example:\\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\\nmodel.compile(loss=\"mse\", optimizer=optimizer)\\nThis will clip every component of the gradient vector to a value between 1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between 1.0 and 1.0. The threshold is a hyper\\nparameter you can tune. Note that it may change the orientation of the gradient vec\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue. This will clip the whole gradient if its  2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14),\\nthen just reuse the lower layers of this network: this is called transfer learning. It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4).\\nReusing Pretrained Layers | 339',\n",
       "  'Figure 11-4. Reusing pretrained layers\\nIf the input pictures of your new task dont have the same size as\\nthe ones used in the original task, you will usually have to add a\\npreprocessing step to resize them to the size expected by the origi\\nnal model. More generally, transfer learning will work best when\\nthe inputs have similar low-level features.\\nThe output layer of the original model should usually be replaced since it is most\\nlikely not useful at all for the new task, and it may not even have the right number of\\noutputs for the new task.\\nSimilarly, the upper hidden layers of the original model are less likely to be as useful\\nas the lower layers, since the high-level features that are most useful for the new task\\nmay differ significantly from the ones that were most useful for the original task. Y ou\\nwant to find the right number of layers to reuse.\\nThe more similar the tasks are, the more layers you want to reuse\\n(starting with the lower layers). For very similar tasks, you can try\\nkeeping all the hidden layers and just replace the output layer.\\nTry freezing all the reused layers first (i.e., make their weights non-trainable, so gradi\\nent descent wont modify them), then train your model and see how it performs.\\nThen try unfreezing one or two of the top hidden layers to let backpropagation tweak\\nthem and see if performance improves. The more training data you have, the more\\n340 | Chapter 11: Training Deep Neural Networks',\n",
       "  'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLets look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Lets\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (lets call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since its a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Lets find out!\\nFirst, you need to load model A, and create a new model based on the model A s lay\\ners. Lets reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model(\"my_model_A.h5\")\\nmodel_B_on_A = keras.models.Sequential(model_A.layers[:-1])\\nmodel_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\\nNote that model_A and model_B_on_A now share some layers. When you train\\nmodel_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\\nmodel_A before you reuse its layers. To do this, you must clone model A s architecture,\\nthen copy its weights (since clone_model() does not clone the weights):\\nmodel_A_clone = keras.models.clone_model(model_A)\\nmodel_A_clone.set_weights(model_A.get_weights())\\nNow we could just train model_B_on_A for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layers train\\nable attribute to False and compile the model:\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = False\\nReusing Pretrained Layers | 341',\n",
       "  'model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\\n                     metrics=[\"accuracy\"])\\nY ou must always compile your model after you freeze or unfreeze\\nlayers.\\nNext, we can train the model for a few epochs, then unfreeze the reused layers (which\\nrequires compiling the model again) and continue training to fine-tune the reused\\nlayers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\\nthe learning rate, once again to avoid damaging the reused weights:\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\\n                           validation_data=(X_valid_B, y_valid_B))\\nfor layer in model_B_on_A.layers[:-1]:\\n    layer.trainable = True\\noptimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-3\\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\\n                     metrics=[\"accuracy\"])\\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\\n                           validation_data=(X_valid_B, y_valid_B))\\nSo, whats the final verdict? Well this models test accuracy is 99.25%, which means\\nthat transfer learning reduced the error rate from 2.8% down to almost 0.7%! Thats a\\nfactor of 4!\\n>>> model_B_on_A.evaluate(X_test_B, y_test_B)\\n[0.06887910133600235, 0.9925]\\nAre you convinced? Well you shouldnt be: I cheated! :) I tried many configurations\\nuntil I found one that demonstrated a strong improvement. If you try to change the\\nclasses or the random seed, you will see that the improvement generally drops, or\\neven vanishes or reverses. What I did is called torturing the data until it confesses .\\nWhen a paper just looks too positive, you should be suspicious: perhaps the flashy\\nnew technique does not help much (in fact, it may even degrade performance), but\\nthe authors tried many variants and reported only the best results (which may be due\\nto shear luck), without mentioning how many failures they encountered on the way.\\nMost of the time, this is not malicious at all, but it is part of the reason why so many\\nresults in Science can never be reproduced.\\nSo why did I cheat? Well it turns out that transfer learning does not work very well\\nwith small dense networks: it works best with deep convolutional neural networks, so\\nwe will revisit transfer learning in Chapter 14, using the same techniques (and this\\ntime there will be no cheating, I promise!).\\n342 | Chapter 11: Training Deep Neural Networks',\n",
       "  'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you dont have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDont lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper\\nvised pretraining (see Figure 11-5). It is often rather cheap to gather unlabeled train\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com\\nReusing Pretrained Layers | 343',\n",
       "  'mon to train DNNs purely using supervised learning. However, unsupervised pre\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural networks lower layers will learn feature detectors that will likely be reusa\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individualclearly not enough to train a good classifier. Gather\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence What ___\\nyou saying? is probably are or were). If you can train a model to reach good per\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\nlabeled dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks',\n",
       "  '12 Some methods of speeding up the convergence of iteration methods,  B. Polyak (1964).\\nwe will present the most popular ones: Momentum optimization, Nesterov Acceler\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization, proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights  by directly subtracting the\\ngradient of the cost function J() with regards to the weights ( J()) multiplied by\\nthe learning rate . The equation is:     J(). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector m (multi\\nplied by the learning rate ), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4). In other words, the gradient is used for accel\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\n, simply called the momentum, which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m m  J \\n2 .   + m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate  multiplied by 11  (ignoring the sign). For example, if  = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4 that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val\\nFaster Optimizers | 345',\n",
       "  '13  A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2),  Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that dont use\\nBatch Normalization, the upper layers will often end up having inputs with very dif\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum hyperparameter, then lie back and profit!\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at  + m rather than at .\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m m  J  + m\\n2 .   + m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi\\nent at the original position, as you can see in Figure 11-6 (where 1 represents the\\ngradient of the cost function measured at the starting point , and 2 represents the\\n346 | Chapter 11: Training Deep Neural Networks',\n",
       "  '14  Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,  J. Duchi et al. (2011).\\ngradient at the point located at  + m). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More\\nover, note that when the momentum pushes the weights across a valley, 1 continues\\nto push further across the valley, while 2 pushes back toward the bottom of the val\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi\\nzation. To use it, simply set nesterov=True when creating the SGD optimizer:\\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s + J  J \\n2 .     J   s + \\nFaster Optimizers | 347',\n",
       "  'The first step accumulates the square of the gradients into the vector s (recall that the\\n symbol represents the element-wise multiplication). This vectorized form is equiv\\nalent to computing si  si + ( J() /  i)2 for each element si of the vector s; in other\\nwords, each si accumulates the squares of the partial derivative of the cost function\\nwith regards to parameter i. If the cost function is steep along the ith dimension, then\\nsi will get larger and larger at each iteration.\\nThe second step is almost identical to Gradient Descent, but with one big difference:\\nthe gradient vector is scaled down by a factor of + (the  symbol represents the\\nelement-wise division, and  is a smoothing term to avoid division by zero, typically\\nset to 10 10). This vectorized form is equivalent to computing\\ni i   J  / i/ si +  for all parameters i (simultaneously).\\nIn short, this algorithm decays the learning rate, but it does so faster for steep dimen\\nsions than for dimensions with gentler slopes. This is called an adaptive learning rate. \\nIt helps point the resulting updates more directly toward the global optimum (see\\nFigure 11-7). One additional benefit is that it requires much less tuning of the learn\\ning rate hyperparameter .\\nFigure 11-7. AdaGrad versus Gradient Descent\\nAdaGrad often performs well for simple quadratic problems, but unfortunately it\\noften stops too early when training neural networks. The learning rate gets scaled\\ndown so much that the algorithm ends up stopping entirely before reaching the\\nglobal optimum. So even though Keras has an Adagrad optimizer, you should not use\\nit to train deep neural networks (it may be efficient for simpler tasks such as Linear\\nRegression, though). However, understanding Adagrad is helpful to grasp the other\\nadaptive learning rate optimizers.\\n348 | Chapter 11: Training Deep Neural Networks',\n",
       "  '15 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite slide 29 in lecture 6\\nin their papers.\\n16  Adam: A Method for Stochastic Optimization,  D. Kingma, J. Ba (2015).\\n17 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment, while the variance is often called the second moment, hence the name of the algorithm.\\nRMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp algorithm15 fixes this by accumulating only the gradi\\nents from the most recent iterations (as opposed to all the gradients since the begin\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7).\\nEquation 11-7. RMSProp algorithm\\n1 . s s + 1   J  J \\n2 .     J   s + \\nThe decay rate  is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp optimizer:\\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam,16 which stands for adaptive moment estimation, combines the ideas of Momen\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8).17\\nFaster Optimizers | 349',\n",
       "  'Equation 11-8. Adam algorithm\\n1 . m 1m  1  1 J \\n2 . s 2s + 1  2 J  J \\n3 . m m\\n1  1\\nt\\n4 . s s\\n1  2\\nt\\n5 .   +  m  s + \\n t represents the iteration number (starting at 1).\\nIf you just look at steps 1, 2, and 5, you will notice Adams close similarity to both\\nMomentum optimization and RMSProp. The only difference is that step 1 computes\\nan exponentially decaying average rather than an exponentially decaying sum, but\\nthese are actually equivalent except for a constant factor (the decaying average is just\\n1  1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since\\nm and s are initialized at 0, they will be biased toward 0 at the beginning of training,\\nso these two steps will help boost m and s at the beginning of training.\\nThe momentum decay hyperparameter 1 is typically initialized to 0.9, while the scal\\ning decay hyperparameter 2 is often initialized to 0.999. As earlier, the smoothing\\nterm  is usually initialized to a tiny number such as 10 7. These are the default values\\nfor the Adam class (to be precise, epsilon defaults to None, which tells Keras to use\\nkeras.backend.epsilon(), which defaults to 10 7; you can change it using\\nkeras.backend.set_epsilon()).\\noptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\\nSince Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\\nrequires less tuning of the learning rate hyperparameter . Y ou can often use the\\ndefault value  = 0.001, making Adam even easier to use than Gradient Descent.\\nIf you are starting to feel overwhelmed by all these different techni\\nques, and wondering how to choose the right ones for your task,\\ndont worry: some practical guidelines are provided at the end of\\nthis chapter.\\nFinally, two variants of Adam are worth mentioning:\\n350 | Chapter 11: Training Deep Neural Networks',\n",
       "  '18 Incorporating Nesterov Momentum into Adam,  Timothy Dozat (2015).\\n19 The Marginal Value of Adaptive Gradient Methods in Machine Learning,  A. C. Wilson et al. (2017).\\n Adamax, introduced in the same paper as Adam: notice that in step 2 of Equation\\n11-8, Adam accumulates the squares of the gradients in s (with a greater weight\\nfor more recent weights). In step 5, if we ignore  and steps 3 and 4 (which are\\ntechnical details anyway), Adam just scales down the parameter updates by the\\nsquare root of s. In short, Adam scales down the parameter updates by the  2\\nnorm of the time-decayed gradients (recall that the  2 norm is the square root of\\nthe sum of squares). Adamax just replaces the  2 norm with the  norm (a fancy\\nway of saying the max). Specifically, it replaces step 2 in Equation 11-8  with\\n max 2, J  , it drops step 4, and in step 5 it scales down the gradient\\nupdates by a factor of s, which is just the max of the time-decayed gradients. In\\npractice, this can make Adamax more stable than Adam, but this really depends\\non the dataset, and in general Adam actually performs better. So its just one\\nmore optimizer you can try if you experience problems with Adam on some task.\\n Nadam optimization18 is more important: it is simply Adam optimization plus\\nthe Nesterov trick, so it will often converge slightly faster than Adam. In his\\nreport, Timothy Dozat compares many different optimizers on various tasks, and\\nfinds that Nadam generally outperforms Adam, but is sometimes outperformed\\nby RMSProp.\\nAdaptive optimization methods (including RMSProp, Adam and\\nNadam optimization) are often great, converging fast to a good sol\\nution. However, a 2017 paper19 by Ashia C. Wilson et al. showed\\nthat they can lead to solutions that generalize poorly on some data\\nsets. So when you are disappointed by your models performance,\\ntry using plain Nesterov Accelerated Gradient instead: your dataset\\nmay just be allergic to adaptive gradients. Also check out the latest\\nresearch, it is moving fast (e.g., AdaBound).\\nAll the optimization techniques discussed so far only rely on the first-order partial\\nderivatives (Jacobians). The optimization literature contains amazing algorithms\\nbased on the second-order partial derivatives  (the Hessians, which are the partial\\nderivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\\nto deep neural networks because there are n2 Hessians per output (where n is the\\nnumber of parameters), as opposed to just n Jacobians per output. Since DNNs typi\\ncally have tens of thousands of parameters, the second-order optimization algorithms\\nFaster Optimizers | 351',\n",
       "  '20 Primal-Dual Subgradient Methods for Convex Problems,  Yurii Nesterov (2005).\\n21  Ad Click Prediction: a View from the Trenches,  H. McMahan et al. (2013).\\noften dont even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the models performance.\\nA better option is to apply strong  1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging, often called Follow The Regularized Leader (FTRL), a techni\\nque proposed by Yurii Nesterov .20 When used with  1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8).\\n352 | Chapter 11: Training Deep Neural Networks',\n",
       "  'Figure 11-8. Learning curves for various learning rates \\nAs we discussed in Chapter 10, one approach is to start with a large learning rate, and\\ndivide it by 3 until the training algorithm stops diverging. Y ou will not be too far\\nfrom the optimal learning rate, which will learn quickly and converge to good solu\\ntion.\\nHowever, you can do better than a constant learning rate: if you start with a high\\nlearning rate and then reduce it once it stops making fast progress, you can reach a\\ngood solution faster than with the optimal constant learning rate. There are many dif\\nferent strategies to reduce the learning rate during training. These strategies are called\\nlearning schedules (we briefly introduced this concept in Chapter 4), the most com\\nmon of which are:\\nPower scheduling\\nSet the learning rate to a function of the iteration number t: (t) = 0 / (1 + t/k)c.\\nThe initial learning rate 0, the power c (typically set to 1) and the steps s are\\nhyperparameters. The learning rate drops at each step, and after s steps it is down\\nto 0 / 2. After s more steps, it is down to 0 / 3. Then down to 0 / 4, then 0 / 5,\\nand so on. As you can see, this schedule first drops quickly, then more and more\\nslowly. Of course, this requires tuning 0, s (and possibly c).\\nExponential scheduling\\nSet the learning rate to: (t) = 0 0.1t/s. The learning rate will gradually drop by a\\nfactor of 10 every s steps. While power scheduling reduces the learning rate more\\nand more slowly, exponential scheduling keeps slashing it by a factor of 10 every\\ns steps.\\nPiecewise constant scheduling\\nUse a constant learning rate for a number of epochs (e.g., 0 = 0.1 for 5 epochs),\\nthen a smaller learning rate for another number of epochs (e.g., 1 = 0.001 for 50\\nepochs), and so on. Although this solution can work very well, it requires fid\\nFaster Optimizers | 353',\n",
       "  '22  An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition,  A. Senior et al.\\n(2013).\\ndling around to figure out the right sequence of learning rates, and how long to\\nuse each of them.\\nPerformance scheduling\\nMeasure the validation error every N steps (just like for early stopping) and\\nreduce the learning rate by a factor of  when the error stops dropping.\\nA 2013 paper22 by Andrew Senior et al. compared the performance of some of the\\nmost popular learning schedules when training deep neural networks for speech rec\\nognition using Momentum optimization. The authors concluded that, in this setting,\\nboth performance scheduling and exponential scheduling performed well. They\\nfavored exponential scheduling because it was easy to tune and it converged slightly\\nfaster to the optimal solution (they also mentioned that it was easier to implement\\nthan performance scheduling, but in Keras both options are easy).\\nImplementing power scheduling in Keras is the easiest option: just set the decay\\nhyperparameter when creating an optimizer. The decay is the inverse of s (the num\\nber of steps it takes to divide the learning rate by one more unit), and Keras assumes\\nthat c is equal to 1. For example:\\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\\nExponential scheduling and piecewise scheduling are quite simple too. Y ou first need\\nto define a function that takes the current epoch and returns the learning rate. For\\nexample, lets implement exponential scheduling:\\ndef exponential_decay_fn(epoch):\\n    return 0.01 * 0.1**(epoch / 20)\\nIf you do not want to hard-code 0 and s, you can create a function that returns a\\nconfigured function:\\ndef exponential_decay(lr0, s):\\n    def exponential_decay_fn(epoch):\\n        return lr0 * 0.1**(epoch / s)\\n    return exponential_decay_fn\\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)\\nNext, just create a LearningRateScheduler callback, giving it the schedule function,\\nand pass this callback to the fit() method:\\nlr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\\nhistory = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\\n354 | Chapter 11: Training Deep Neural Networks',\n",
       "  'The LearningRateScheduler will update the optimizers learning_rate attribute at\\nthe beginning of each epoch. Updating the learning rate just once per epoch is usually\\nenough, but if you want it to be updated more often, for example at every step, you\\nneed to write your own callback (see the notebook for an example). This can make\\nsense if there are many steps per epoch.\\nThe schedule function can optionally take the current learning rate as a second argu\\nment. For example, the following schedule function just multiplies the previous\\nlearning rate by 0.1&1/20, which results in the same exponential decay (except the decay\\nnow starts at the beginning of epoch 0 instead of 1). This implementation relies on\\nthe optimizers initial learning rate (contrary to the previous implementation), so\\nmake sure to set it appropriately.\\ndef exponential_decay_fn(epoch, lr):\\n    return lr * 0.1**(1 / 20)\\nWhen you save a model, the optimizer and its learning rate get saved along with it.\\nThis means that with this new schedule function, you could just load a trained model\\nand continue training where it left off, no problem. However, things are not so simple\\nif your schedule function uses the epoch argument: indeed, the epoch does not get\\nsaved, and it gets reset to 0 every time you call the fit() method. This could lead to a\\nvery large learning rate when you continue training a model where it left off, which\\nwould likely damage your models weights. One solution is to manually set the fit()\\nmethods initial_epoch argument so the epoch starts at the right value.\\nFor piecewise constant scheduling, you can use a schedule function like the following\\none (as earlier, you can define a more general function if you want, see the notebook\\nfor an example), then create a LearningRateScheduler callback with this function\\nand pass it to the fit() method, just like we did for exponential scheduling:\\ndef piecewise_constant_fn(epoch):\\n    if epoch < 5:\\n        return 0.01\\n    elif epoch < 15:\\n        return 0.005\\n    else:\\n        return 0.001\\nFor performance scheduling, simply use the ReduceLROnPlateau callback. For exam\\nple, if you pass the following callback to the fit() method, it will multiply the learn\\ning rate by 0.5 whenever the best validation loss does not improve for 5 consecutive\\nepochs (other options are available, please check the documentation for more\\ndetails):\\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\\nLastly, tf.keras offers an alternative way to implement learning rate scheduling: just\\ndefine the learning rate using one of the schedules available in keras.optimiz\\nFaster Optimizers | 355',\n",
       "  'ers.schedules, then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\\noptimizer = keras.optimizers.SGD(learning_rate)\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\nJohn von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari\\nety of complex datasets. But this great flexibility also means that it is prone to overfit\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net\\nworks: 1 and 2 regularization, dropout and max-norm regularization.\\n1 and 2 Regularization\\nJust like you did in Chapter 4 for simple linear models, you can use 1 and 2 regulari\\nzation to constrain a neural networks connection weights (but typically not its bia\\nses). Here is how to apply  2 regularization to a Keras layers connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation=\"elu\",\\n                           kernel_initializer=\"he_normal\",\\n                           kernel_regularizer=keras.regularizers.l2(0.01))\\nThe l2() function returns a regularizer that will be called to compute the regulariza\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1() if you\\n356 | Chapter 11: Training Deep Neural Networks',\n",
       "  '23 Improving neural networks by preventing co-adaptation of feature detectors,  G. Hinton et al. (2012).\\n24 Dropout: A Simple Way to Prevent Neural Networks from Overfitting,  N. Srivastava et al. (2014).\\nwant 1 regularization, and if you want both  1 and 2 regularization, use keras.regu\\nlarizers.l1_l2() (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Pythons functools.partial() function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools import partial\\nRegularizedDense = partial(keras.layers.Dense,\\n                           activation=\"elu\",\\n                           kernel_initializer=\"he_normal\",\\n                           kernel_regularizer=keras.regularizers.l2(0.01))\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    RegularizedDense(300),\\n    RegularizedDense(100),\\n    RegularizedDense(10, activation=\"softmax\",\\n                     kernel_initializer=\"glorot_uniform\")\\n])\\nDropout\\nDropout is one of the most popular regularization techniques for deep neural net\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 12% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily dropped out,  meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9). The hyperparameter\\np is called the dropout rate, and it is typically set to 50%. After training, neurons dont\\nget dropped anymore. And thats all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting Through Regularization | 357',\n",
       "  'Figure 11-9. Dropout regularization\\nIt is quite surprising at first that this rather brutal technique works at all. Would a\\ncompany perform better if its employees were told to toss a coin every morning to\\ndecide whether or not to go to work? Well, who knows; perhaps it would! The com\\npany would obviously be forced to adapt its organization; it could not rely on any sin\\ngle person to fill in the coffee machine or perform any other critical tasks, so this\\nexpertise would have to be spread across several people. Employees would have to\\nlearn to cooperate with many of their coworkers, not just a handful of them. The\\ncompany would become much more resilient. If one person quit, it wouldnt make\\nmuch of a difference. Its unclear whether this idea would actually work for compa\\nnies, but it certainly does for neural networks. Neurons trained with dropout cannot\\nco-adapt with their neighboring neurons; they have to be as useful as possible on\\ntheir own. They also cannot rely excessively on just a few input neurons; they must\\npay attention to each of their input neurons. They end up being less sensitive to slight\\nchanges in the inputs. In the end you get a more robust network that generalizes bet\\nter.\\nAnother way to understand the power of dropout is to realize that a unique neural\\nnetwork is generated at each training step. Since each neuron can be either present or\\nabsent, there is a total of 2 N possible networks (where N is the total number of drop\\npable neurons). This is such a huge number that it is virtually impossible for the same\\nneural network to be sampled twice. Once you have run a 10,000 training steps, you\\nhave essentially trained 10,000 different neural networks (each with just one training\\ninstance). These neural networks are obviously not independent since they share\\nmany of their weights, but they are nevertheless all different. The resulting neural\\nnetwork can be seen as an averaging ensemble of all these smaller neural networks.\\nThere is one small but important technical detail. Suppose p = 50%, in which case\\nduring testing a neuron will be connected to twice as many input neurons as it was\\n(on average) during training. To compensate for this fact, we need to multiply each\\n358 | Chapter 11: Training Deep Neural Networks',\n",
       "  '25 This is specific to tf.keras, so you may prefer to use keras.backend.set_learning_phase(1) before calling\\nthe fit() method (and set it back to 0 right after).\\nneurons input connection weights by 0.5 after training. If we dont, each neuron will\\nget a total input signal roughly twice as large as what the network was trained on, and\\nit is unlikely to perform well. More generally, we need to multiply each input connec\\ntion weight by the keep probability (1  p) after training. Alternatively, we can divide\\neach neurons output by the keep probability during training (these alternatives are\\nnot perfectly equivalent, but they work equally well).\\nTo implement dropout using Keras, you can use the keras.layers.Dropout layer.\\nDuring training, it randomly drops some inputs (setting them to 0) and divides the\\nremaining inputs by the keep probability. After training, it does nothing at all, it just\\npasses the inputs to the next layer. For example, the following code applies dropout\\nregularization before every Dense layer, using a dropout rate of 0.2:\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28, 28]),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\\n    keras.layers.Dropout(rate=0.2),\\n    keras.layers.Dense(10, activation=\"softmax\")\\n])\\nSince dropout is only active during training, the training loss is\\npenalized compared to the validation loss, so comparing the two\\ncan be misleading. In particular, a model may be overfitting the\\ntraining set and yet have similar training and validation losses. So\\nmake sure to evaluate the training loss without dropout (e.g., after\\ntraining). Alternatively, you can call the fit() method inside a\\nwith keras.backend.learning_phase_scope(1)  block: this will\\nforce dropout to be active during both training and validation.25\\nIf you observe that the model is overfitting, you can increase the dropout rate. Con\\nversely, you should try decreasing the dropout rate if the model underfits the training\\nset. It can also help to increase the dropout rate for large layers, and reduce it for\\nsmall ones. Moreover, many state-of-the-art architectures only use dropout after the\\nlast hidden layer, so you may want to try this if full dropout is too strong.\\nDropout does tend to significantly slow down convergence, but it usually results in a\\nmuch better model when tuned properly. So, it is generally well worth the extra time\\nand effort.\\nAvoiding Overfitting Through Regularization | 359',\n",
       "  '26 Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,  Y . Gal and Z.\\nGhahramani (2016).\\n27 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process.\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout: this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica\\ntion.\\n Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n Moreover, MC Dropout also provides a much better measure of the models\\nuncertainty.\\n Finally, it is also amazingly simple to implement. If this all sounds like a one\\nweird trick advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout, boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope(1): # force training mode = dropout on\\n    y_probas = np.stack([model.predict(X_test_scaled)\\n                         for sample in range(100)])\\ny_proba = y_probas.mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1) context. This\\nturns dropout on within the with block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict() returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0), we get y_proba, an array of\\nshape [10000, 10], like we would get with a single prediction. Thats all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks',\n",
       "  'over multiple predictions with dropout on gives us a Monte Carlo estimate that is\\ngenerally more reliable than the result of a single prediction with dropout off. For\\nexample, lets look at the models prediction for the first instance in the test set, with\\ndropout off:\\n>>> np.round(model.predict(X_test_scaled[:1]), 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\\n      dtype=float32)\\nThe model seems almost certain that this image belongs to class 9 (ankle boot).\\nShould you trust it? Is there really so little room for doubt? Compare this with the\\npredictions made when dropout is activated:\\n>>> np.round(y_probas[:, :1], 2)\\narray([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\\n       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\\n       [...]\\nThis tells a very different story: apparently, when we activate dropout, the model is\\nnot sure anymore. It still seems to prefer class 9, but sometimes it hesitates with\\nclasses 5 (sandal) and 7 (sneaker), which makes sense given theyre all footwear. Once\\nwe average over the first dimension, we get the following MC dropout predictions:\\n>>> np.round(y_proba[:1], 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],\\n      dtype=float32)\\nThe model still thinks this image belongs to class 9, but only with a 62% confidence,\\nwhich seems much more reasonable than 99%. Plus its useful to know exactly which\\nother classes it thinks are likely. And you can also take a look at the standard devia\\ntion of the probability estimates:\\n>>> y_std = y_probas.std(axis=0)\\n>>> np.round(y_std[:1], 2)\\narray([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\\n      dtype=float32)\\nApparently theres quite a lot of variance in the probability estimates: if you were\\nbuilding a risk-sensitive system (e.g., a medical or financial system), you should prob\\nably treat such an uncertain prediction with extreme caution. Y ou definitely would\\nnot treat it like a 99% confident prediction. Moreover, the models accuracy got a\\nsmall boost from 86.8 to 86.9:\\n>>> accuracy = np.sum(y_pred == y_test) / len(y_test)\\n>>> accuracy\\n0.8694\\nAvoiding Overfitting Through Regularization | 361',\n",
       "  'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu\\nrate the predictions and their uncertainty estimates will be. How\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout layers with the following MCDropout\\nclass:\\nclass MCDropout(keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training=True)\\nWe just sublass the Dropout layer and override the call() method to force its train\\ning argument to True (see Chapter 12). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout instead. If you are creating a model from\\nscratch, its just a matter of using MCDropout rather than Dropout. But if you have a\\nmodel that was already trained using Dropout, you need to create a new model, iden\\ntical to the existing model except replacing the Dropout layers with MCDropout, then\\ncopy the existing models weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization: for each neuron, it constrains the weights w of the incom\\ning connections such that  *w* 2  _r_, where r is the max-norm hyperparameter\\nand   2 is the 2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing w2 after each training\\nstep and clipping w if needed (w w r\\n w 2\\n).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks',\n",
       "  'To implement max-norm regularization in Keras, just set every hidden layers ker\\nnel_constraint argument to a max_norm() constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\\n                   kernel_constraint=keras.constraints.max_norm(1.))\\nAfter each training iteration, the models fit() method will call the object returned\\nby max_norm(), passing it the layers weights and getting clipped weights in return,\\nwhich then replace the layers weights. As we will see in Chapter 12, you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint. Y ou can also constrain the bias terms by setting the bias_con\\nstraint argument.\\nThe max_norm() function has an axis argument that defaults to 0. A Dense layer usu\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neurons weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm() constraints axis argument appropriately (usually\\naxis=[0, 1, 2]).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder\\ning which ones you should use. The configuration in Table 11-2 will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDont forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2 may need to be tweaked:\\nSummary and Practical Guidelines | 363',\n",
       "  ' If your model self-normalizes:\\n If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord\\ningly (e.g., He init for ELU or ReLU).\\n If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or  2 reg\\nularization.\\n If you need a sparse model, you can use 1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with  1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlows lower-level API, as we will see in the next chapter.\\nExercises\\n1. Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2. Is it okay to initialize the bias terms to 0?\\n3. Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks',\n",
       "  '4. In which cases would you want to use each of the following activation functions:\\nSELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\n5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,\\n0.99999) when using an SGD optimizer?\\n6. Name three ways you can produce a sparse model.\\n7. Does dropout slow down training? Does it slow down inference (i.e., making\\npredictions on new instances)? What are about MC dropout?\\n8. Deep Learning.\\na. Build a DNN with five hidden layers of 100 neurons each, He initialization,\\nand the ELU activation function.\\nb. Using Adam optimization and early stopping, try training it on MNIST but\\nonly on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\\nnext exercise. Y ou will need a softmax output layer with five neurons, and as\\nalways make sure to save checkpoints at regular intervals and save the final\\nmodel so you can reuse it later.\\nc. Tune the hyperparameters using cross-validation and see what precision you\\ncan achieve.\\nd. Now try adding Batch Normalization and compare the learning curves: is it\\nconverging faster than before? Does it produce a better model?\\ne. Is the model overfitting the training set? Try adding dropout to every layer\\nand try again. Does it help?\\n9. Transfer learning.\\na. Create a new DNN that reuses all the pretrained hidden layers of the previous\\nmodel, freezes them, and replaces the softmax output layer with a new one.\\nb. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\\nhow long it takes. Despite this small number of examples, can you achieve\\nhigh precision?\\nc. Try caching the frozen layers, and train the model again: how much faster is it\\nnow?\\nd. Try again reusing just four hidden layers instead of five. Can you achieve a\\nhigher precision?\\ne. Now unfreeze the top two hidden layers and continue training: can you get\\nthe model to perform even better?\\n10. Pretraining on an auxiliary task.\\na. In this exercise you will build a DNN that compares two MNIST digit images\\nand predicts whether they represent the same digit or not. Then you will reuse\\nthe lower layers of this network to train an MNIST classifier using very little\\nExercises | 365',\n",
       "  'training data. Start by building two DNNs (lets call them DNN A and B), both\\nsimilar to the one you built earlier but without the output layer: each DNN\\nshould have five hidden layers of 100 neurons each, He initialization, and ELU\\nactivation. Next, add one more hidden layer with 10 units on top of both\\nDNNs. To do this, you should use a keras.layers.Concatenate layer to con\\ncatenate the outputs of both DNNs for each instance, then feed the result to\\nthe hidden layer. Finally, add an output layer with a single neuron using the\\nlogistic activation function.\\nb. Split the MNIST training set in two sets: split #1 should containing 55,000\\nimages, and split #2 should contain contain 5,000 images. Create a function\\nthat generates a training batch where each instance is a pair of MNIST images\\npicked from split #1. Half of the training instances should be pairs of images\\nthat belong to the same class, while the other half should be images from dif\\nferent classes. For each pair, the training label should be 0 if the images are\\nfrom the same class, or 1 if they are from different classes.\\nc. Train the DNN on this training set. For each image pair, you can simultane\\nously feed the first image to DNN A and the second image to DNN B. The\\nwhole network will gradually learn to tell whether two images belong to the\\nsame class or not.\\nd. Now create a new DNN by reusing and freezing the hidden layers of DNN A\\nand adding a softmax output layer on top with 10 neurons. Train this network\\non split #2 and see if you can achieve high performance despite having only\\n500 images per class.\\nSolutions to these exercises are available in ???.\\n366 | Chapter 11: Training Deep Neural Networks',\n",
       "  'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlows high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13). But now its time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API. This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten\\nsorFlows automatic graph generation feature. But first, lets take a quick tour of Ten\\nsorFlow.\\n367',\n",
       "  '1 TensorFlow also includes another Deep Learning API called the Estimators API, but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow is a powerful library for numerical computation, particu\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Googles large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Heres a summary:\\n Its core is very similar to NumPy, but with GPU support.\\n It also supports distributed computing (across multiple devices and servers).\\n It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu\\ntations for speed and memory usage: it works by extracting the computation\\ngraph from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlows Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  '2 If you ever need to (but you probably wont), you can write your own operations using the C++ API.\\n3 If you are a researcher, you may be eligible to use these TPUs for free, see https://tensorflow.org/tfrc/ for more\\ndetails.\\nFigure 12-1. TensorFlows Python API\\nWe will cover many of the packages and functions of the Tensor\\nFlow API, but its impossible to cover them all so you should really\\ntake some time to browse through the API: you will find that it is\\nquite rich and well documented.\\nAt the lowest level, each TensorFlow operation is implemented using highly efficient\\nC++ code2. Many operations (or ops for short) have multiple implementations, called\\nkernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or\\neven TPUs (Tensor Processing Units). As you may know, GPUs can dramatically speed\\nup computations by splitting computations into many smaller chunks and running\\nthem in parallel across many GPU threads. TPUs are even faster. Y ou can purchase\\nyour own GPU devices (for now, TensorFlow only supports Nvidia cards with CUDA\\nCompute Capability 3.5+), but TPUs are only available on Google Cloud Machine\\nLearning Engine (see ???).3\\nTensorFlows architecture is shown in Figure 12-2 : most of the time your code will\\nuse the high level APIs (especially tf.keras and tf.data), but when you need more flexi\\nbility you will use the lower level Python API, handling tensors directly. Note that\\nAPIs for other languages are also available. In any case, TensorFlows execution\\nA Quick Tour of TensorFlow | 369',\n",
       "  'engine will take care of running the operations efficiently, even across multiple devi\\nces and machines if you tell it to.\\nFigure 12-2. TensorFlows architecture\\nTensorFlow runs not only on Windows, Linux, and MacOS, but also on mobile devi\\nces (using TensorFlow Lite), including both iOS and Android (see ???). If you do not\\nwant to use the Python API, there are also C++, Java, Go and Swift APIs. There is\\neven a Javascript implementation called TensorFlow.js that makes it possible to run\\nyour models directly in your browser.\\nTheres more to TensorFlow than just the library. TensorFlow is at the center of an\\nextensive ecosystem of libraries. First, theres TensorBoard for visualization (see\\nChapter 10). Next, theres TensorFlow Extended (TFX), which is a set of libraries built\\nby Google to productionize TensorFlow projects: it includes tools for data validation,\\npreprocessing, model analysis and serving (with TF Serving, see ???). Google also\\nlaunched TensorFlow Hub, a way to easily download and reuse pretrained neural net\\nworks. Y ou can also get many neural network architectures, some of them pretrained,\\nin TensorFlows model garden . Check out the TensorFlow Resources , or https://\\ngithub.com/jtoy/awesome-tensorflow for more TensorFlow-based projects. Y ou will\\nfind hundreds of TensorFlow projects on GitHub, so it is often easy to find existing\\ncode for whatever you are trying to do.\\nMore and more ML papers are released along with their implemen\\ntation, and sometimes even with pretrained models. Check out\\nhttps://paperswithcode.com/ to easily find them.\\n370 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel\\nopers, and a large community contributing to improving it. To ask technical ques\\ntions, you should use http://stackoverflow.com/ and tag your question with tensorflow\\nand python. Y ou can file bugs and feature requests through GitHub. For general dis\\ncussions, join the Google group.\\nOkay, its time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlows API revolves around tensors, hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so lets see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant(). For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant(42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray, a tf.Tensor has a shape and a data type (dtype):\\n>>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371',\n",
       "  'array([[11., 12., 13.],\\n       [14., 15., 16.]], dtype=float32)>\\n>>> tf.square(t)\\n<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)>\\n>>> t @ tf.transpose(t)\\n<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\\narray([[14., 32.],\\n       [32., 77.]], dtype=float32)>\\nNote that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls\\nthe magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators\\n(like -, *, etc.) are also supported. The @ operator was added in Python 3.5, for matrix\\nmultiplication: it is equivalent to calling the tf.matmul() function.\\nY ou will find all the basic math operations you need (e.g., tf.add(), tf.multiply(),\\ntf.square(), tf.exp(), tf.sqrt()), and more generally most operations that you\\ncan find in NumPy (e.g., tf.reshape(), tf.squeeze(), tf.tile()), but sometimes\\nwith a different name (e.g., tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(),\\ntf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log()).\\nWhen the name differs, there is often a good reason for it: for example, in Tensor\\nFlow you must write tf.transpose(t), you cannot just write t.T like in NumPy. The\\nreason is that it does not do exactly the same thing: in TensorFlow, a new tensor is\\ncreated with its own copy of the transposed data, while in NumPy, t.T is just a trans\\nposed view on the same data. Similarly, the tf.reduce_sum() operation is named this\\nway because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that\\ndoes not guarantee the order in which the elements are added: because 32-bit floats\\nhave limited precision, this means that the result may change ever so slightly every\\ntime you call this operation. The same is true of tf.reduce_mean() (but of course\\ntf.reduce_max() is deterministic).\\n372 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  '4 A notable exception is tf.math.log() which is commonly used but there is no tf.log() alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add() are the same function. This allows TensorFlow\\nto have concise names for the most common operations 4, while\\npreserving well organized packages.\\nKeras Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend. It\\nincludes functions like square(), exp(), sqrt() and so on. In tf.keras, these func\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend, which is commonly named K for short:\\n>>> from tensorflow import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose(t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant(a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373',\n",
       "  \"Notice that NumPy uses 64-bit precision by default, while Tensor\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32.\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant(2.) + tf.constant(40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that its for a good cause! And of\\ncourse you can use tf.cast() when you really need to convert types:\\n>>> t2 = tf.constant(40., dtype=tf.float64)\\n>>> tf.constant(2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable:\\n>>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable acts much like a constant tensor: you can perform the same opera\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign() method (or assign_add() or\\nassign_sub() which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cells (or slices) assign()\\nmethod (direct item assignment will not work), or using the scatter_update() or\\nscatter_nd_update() methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\",\n",
       "  'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight() method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly\\n0s. The tf.sparse package contains operations for sparse tensors.\\n Tensor arrays (tf.TensorArray) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged package contains\\noperations for ragged tensors.\\n String tensors are regular tensors of type tf.string. These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"caf\"`), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\"). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32, where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can\\nprioritize some items ( PriorityQueue), queues that shuffle their items ( Random\\nShuffleQueue), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue). These classes are all in the tf.queue package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375',\n",
       "  'Customizing Models and Training Algorithms\\nLets start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro\\nduced in Chapter 10) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber class). But lets pretend its not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instances loss:\\ndef huber_fn(y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error = tf.abs(error) < 1\\n    squared_loss = tf.square(error) / 2\\n    linear_loss  = tf.abs(error) - 0.5\\n    return tf.where(is_small_error, squared_loss, linear_loss)\\nFor better performance, you should use a vectorized implementa\\ntion, as in this example. Moreover, if you want to benefit from Ten\\nsorFlows graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn, optimizer=\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd thats it! For each batch during training, Keras will call the huber_fn() function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\\n                                custom_objects={\"huber_fn\": huber_fn})\\nWith the current implementation, any error between -1 and 1 is considered small .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber(threshold=1.0):\\n    def huber_fn(y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error = tf.abs(error) < threshold\\n        squared_loss = tf.square(error) / 2\\n        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\\n        return tf.where(is_small_error, squared_loss, linear_loss)\\n    return huber_fn\\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\")\\nUnfortunately, when you save the model, the threshold will not be saved. This means\\nthat you will have to specify the threshold value when loading the model (note that\\nthe name to use is \"huber_fn\", which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\\n                                custom_objects={\"huber_fn\": create_huber(2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss class, and imple\\nment its get_config() method:\\nclass HuberLoss(keras.losses.Loss):\\n    def __init__(self, threshold=1.0, **kwargs):\\n        self.threshold = threshold\\n        super().__init__(**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error = tf.abs(error) < self.threshold\\n        squared_loss = tf.square(error) / 2\\n        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\\n        return tf.where(is_small_error, squared_loss, linear_loss)\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"threshold\": self.threshold}\\nCustomizing Models and Training Algorithms | 377',\n",
       "  '5 It would not be a good idea to use a weighted mean: if we did, then two instances with the same weight but in\\ndifferent batches would have a different impact on training, depending on the total weight of each batch.\\nThe Keras API only specifies how to use subclassing to define lay\\ners, models, callbacks, and regularizers. If you build other compo\\nnents (such as losses, metrics, initializers or constraints) using\\nsubclassing, they may not be portable to other Keras implementa\\ntions.\\nLets walk through this code:\\n The constructor accepts **kwargs and passes them to the parent constructor,\\nwhich handles standard hyperparameters: the name of the loss and the reduction\\nalgorithm to use to aggregate the individual instance losses. By default, it is\\n\"sum_over_batch_size\", which means that the loss will be the sum of the\\ninstance losses, possibly weighted by the sample weights, if any, and then divide\\nthe result by the batch size (not by the sum of weights, so this is not the weighted\\nmean).5. Other possible values are \"sum\" and None.\\n The call() method takes the labels and predictions, computes all the instance\\nlosses, and returns them.\\n The get_config() method returns a dictionary mapping each hyperparameter\\nname to its value. It first calls the parent classs get_config() method, then adds\\nthe new hyperparameters to this dictionary (note that the convenient {**x} syn\\ntax was added in Python 3.5).\\nY ou can then use any instance of this class when you compile the model:\\nmodel.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\\nWhen you save the model, the threshold will be saved along with it, and when you\\nload the model you just need to map the class name to the class itself:\\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\\n                                custom_objects={\"HuberLoss\": HuberLoss})\\nWhen you save a model, Keras calls the loss instances get_config() method and\\nsaves the config as JSON in the HDF5 file. When you load the model, it calls the\\nfrom_config() class method on the HuberLoss class: this method is implemented by\\nthe base class (Loss) and just creates an instance of the class, passing **config to the\\nconstructor.\\nThats it for losses! It was not too hard, was it? Well its just as simple for custom acti\\nvation functions, initializers, regularizers, and constraints. Lets look at these now.\\n378 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa\\ntion function (equivalent to keras.activations.softplus or tf.nn.softplus), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal), a cus\\ntom 1 regularizer (equivalent to keras.regularizers.l1(0.01)) and a custom con\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg() or tf.nn.relu):\\ndef my_softplus(z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer(shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer(weights):\\n    return tf.reduce_sum(tf.abs(0.01 * weights))\\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation=my_softplus,\\n                           kernel_initializer=my_glorot_initializer,\\n                           kernel_regularizer=my_l1_regularizer,\\n                           kernel_constraint=my_positive_weights)\\nThe activation function will be applied to the output of this Dense layer, and its result\\nwill be passed on to the next layer. The layers weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layers weights will be replaced by the con\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer, keras.constraints.Constraint, keras.initializers.Initial\\nizer or keras.layers.Layer (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for  1 regulariza\\nCustomizing Models and Training Algorithms | 379',\n",
       "  '6 However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).\\ntion, that saves its factor hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config() method, as they are not defined by the parent class):\\nclass MyL1Regularizer(keras.regularizers.Regularizer):\\n    def __init__(self, factor):\\n        self.factor = factor\\n    def __call__(self, weights):\\n        return tf.reduce_sum(tf.abs(self.factor * weights))\\n    def get_config(self):\\n        return {\"factor\": self.factor}\\nNote that you must implement the call() method for losses, layers (including activa\\ntion functions) and models, or the __call__() method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train a model, so they must be differentiable (at least where they are evalu\\nated) and their gradients should not be 0 everywhere. Plus, its okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric 6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\"):\\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifiers precision, for example. As we saw\\nin Chapter 3, precision is the number of true positives divided by the number of posi\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: thats 80% pre\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: thats 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod\\nels precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num\\n380 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'ber of false positives, and compute their ratio when requested. This is precisely what\\nthe keras.metrics.Precision class does:\\n>>> precision = keras.metrics.Precision()\\n>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\\n<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\\n>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\\n<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\\nIn this example, we created a Precision object, then we used it like a function, pass\\ning it the labels and predictions for the first batch, then for the second batch (note\\nthat we could also have passed sample weights). We used the same number of true\\nand false positives as in the example we just discussed. After the first batch, it returns\\nthe precision of 80%, then after the second batch it returns 50% (which is the overall\\nprecision so far, not the second batchs precision). This is called a streaming metric (or\\nstateful metric), as it is gradually updated, batch after batch.\\nAt any point, we can call the result() method to get the current value of the metric.\\nWe can also look at its variables (tracking the number of true and false positives)\\nusing the variables attribute, and reset these variables using the reset_states()\\nmethod:\\n>>> p.result()\\n<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\\n>>> p.variables\\n[<tf.Variable \\'true_positives:0\\' [...] numpy=array([4.], dtype=float32)>,\\n <tf.Variable \\'false_positives:0\\' [...] numpy=array([4.], dtype=float32)>]\\n>>> p.reset_states() # both variables get reset to 0.0\\nIf you need to create such a streaming metric, you can just create a subclass of the\\nkeras.metrics.Metric class. Here is a simple example that keeps track of the total\\nHuber loss and the number of instances seen so far. When asked for the result, it\\nreturns the ratio, which is simply the mean Huber loss:\\nclass HuberMetric(keras.metrics.Metric):\\n    def __init__(self, threshold=1.0, **kwargs):\\n        super().__init__(**kwargs) # handles base args (e.g., dtype)\\n        self.threshold = threshold\\n        self.huber_fn = create_huber(threshold)\\n        self.total = self.add_weight(\"total\", initializer=\"zeros\")\\n        self.count = self.add_weight(\"count\", initializer=\"zeros\")\\n    def update_state(self, y_true, y_pred, sample_weight=None):\\n        metric = self.huber_fn(y_true, y_pred)\\n        self.total.assign_add(tf.reduce_sum(metric))\\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\\n    def result(self):\\n        return self.total / self.count\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"threshold\": self.threshold}\\nCustomizing Models and Training Algorithms | 381',\n",
       "  '7 This class is for illustration purposes only. A simpler and better implementation would just subclass the\\nkeras.metrics.Mean class, see the notebook for an example.\\nLets walk through this code:7:\\n The constructor uses the add_weight() method to create the variables needed to\\nkeep track of the metrics state over multiple batches, in this case the sum of all\\nHuber losses (total) and the number of instances seen so far ( count). Y ou could\\njust create variables manually if you preferred. Keras tracks any tf.Variable that\\nis set as an attribute (and more generally, any trackable object, such as layers or\\nmodels).\\n The update_state() method is called when you use an instance of this class as a\\nfunction (as we did with the Precision object). It updates the variables given the\\nlabels and predictions for one batch (and sample weights, but in this case we just\\nignore them).\\n The result() method computes and returns the final result, in this case just the\\nmean Huber metric over all instances. When you use the metric as a function, the\\nupdate_state() method gets called first, then the result() method is called,\\nand its output is returned.\\n We also implement the get_config() method to ensure the threshold gets\\nsaved along with the model.\\n The default implementation of the reset_states() method just resets all vari\\nables to 0.0 (but you can override it if needed).\\nKeras will take care of variable persistence seamlessly, no action is\\nrequired.\\nWhen you define a metric using a simple function, Keras automatically calls it for\\neach batch, and it keeps track of the mean during each epoch, just like we did man\\nually. So the only benefit of our HuberMetric class is that the threshold will be saved.\\nBut of course, some metrics, like precision, cannot simply be averaged over batches:\\nin thoses cases, theres no other option than to implement a streaming metric.\\nNow that we have built a streaming metric, building a custom layer will seem like a\\nwalk in the park!\\n382 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Lets see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten or keras.lay\\ners.ReLU. If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda layer. For exam\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp, or activation=keras.activations.expo\\nnential, or simply activation=\"exponential\"). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer class. For exam\\nple, the following class implements a simplified version of the Dense layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__(self, units, activation=None, **kwargs):\\n        super().__init__(**kwargs)\\n        self.units = units\\n        self.activation = keras.activations.get(activation)\\n    def build(self, batch_input_shape):\\n        self.kernel = self.add_weight(\\n            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\\n            initializer=\"glorot_normal\")\\n        self.bias = self.add_weight(\\n            name=\"bias\", shape=[self.units], initializer=\"zeros\")\\n        super().build(batch_input_shape) # must be at the end\\n    def call(self, X):\\n        return self.activation(X @ self.kernel + self.bias)\\n    def compute_output_shape(self, batch_input_shape):\\n        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383',\n",
       "  '8 This function is specific to tf.keras. Y ou could use keras.activations.Activation instead.\\n9 The Keras API calls this argument input_shape, but since it also includes the batch dimension, I prefer to call\\nit batch_input_shape. Same for compute_output_shape().\\n    def get_config(self):\\n        base_config = super().get_config()\\n        return {**base_config, \"units\": self.units,\\n                \"activation\": keras.activations.serialize(self.activation)}\\nLets walk through this code:\\n The constructor takes all the hyperparameters as arguments (in this example just\\nunits and activation), and importantly it also takes a **kwargs argument. It\\ncalls the parent constructor, passing it the kwargs: this takes care of standard\\narguments such as input_shape, trainable, name, and so on. Then it saves the\\nhyperparameters as attributes, converting the activation argument to the\\nappropriate activation function using the keras.activations.get() function (it\\naccepts functions, standard strings like \"relu\" or \"selu\", or simply None)8.\\n The build() methods role is to create the layers variables, by calling the\\nadd_weight() method for each weight. The build() method is called the first\\ntime the layer is used. At that point, Keras will know the shape of this layers\\ninputs, and it will pass it to the build() method9, which is often necessary to cre\\nate some of the weights. For example, we need to know the number of neurons in\\nthe previous layer in order to create the connection weights matrix (i.e., the \"ker\\nnel\"): this corresponds to the size of the last dimension of the inputs. At the end\\nof the build() method (and only at the end), you must call the parents build()\\nmethod: this tells Keras that the layer is built (it just sets self.built = True).\\n The call() method actually performs the desired operations. In this case, we\\ncompute the matrix multiplication of the inputs X and the layers kernel, we add\\nthe bias vector, we apply the activation function to the result, and this gives us the\\noutput of the layer.\\n The compute_output_shape() method simply returns the shape of this layers\\noutputs. In this case, it is the same shape as the inputs, except the last dimension\\nis replaced with the number of neurons in the layer. Note that in tf.keras, shapes\\nare instances of the tf.TensorShape class, which you can convert to Python lists\\nusing as_list().\\n The get_config() method is just like earlier. Note that we save the activation\\nfunctions full configuration by calling keras.activations.serialize().\\nY ou can now use a MyDense layer just like any other layer!\\n384 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'Y ou can generally omit the compute_output_shape() method, as\\ntf.keras automatically infers the output shape, except when the\\nlayer is dynamic (as we will see shortly). In other Keras implemen\\ntations, this method is either required or by default it assumes the\\noutput shape is the same as the input shape.\\nTo create a layer with multiple inputs (e.g., Concatenate), the argument to the call()\\nmethod should be a tuple containing all the inputs, and similarly the argument to the\\ncompute_output_shape() method should be a tuple containing each inputs batch\\nshape. To create a layer with multiple outputs, the call() method should return the\\nlist of outputs, and the compute_output_shape() should return the list of batch out\\nput shapes (one per output). For example, the following toy layer takes two inputs\\nand returns three outputs:\\nclass MyMultiLayer(keras.layers.Layer):\\n    def call(self, X):\\n        X1, X2 = X\\n        return [X1 + X2, X1 * X2, X1 / X2]\\n    def compute_output_shape(self, batch_input_shape):\\n        b1, b2 = batch_input_shape\\n        return [b1, b1, b1] # should probably handle broadcasting rules\\nThis layer may now be used like any other layer, but of course only using the func\\ntional and subclassing APIs, not the sequential API (which only accepts layers with\\none input and one output).\\nIf your layer needs to have a different behavior during training and during testing\\n(e.g., if it uses Dropout or BatchNormalization layers), then you must add a train\\ning argument to the call() method and use this argument to decide what to do. For\\nexample, lets create a layer that adds Gaussian noise during training (for regulariza\\ntion), but does nothing during testing (Keras actually has a layer that does the same\\nthing: keras.layers.GaussianNoise):\\nclass MyGaussianNoise(keras.layers.Layer):\\n    def __init__(self, stddev, **kwargs):\\n        super().__init__(**kwargs)\\n        self.stddev = stddev\\n    def call(self, X, training=None):\\n        if training:\\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\\n            return X + noise\\n        else:\\n            return X\\n    def compute_output_shape(self, batch_input_shape):\\n        return batch_input_shape\\nCustomizing Models and Training Algorithms | 385',\n",
       "  '10 The name subclassing API usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.\\nWith that, you can now build any custom layer you need! Now lets create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10 when we discussed the sub\\nclassing API. 10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model class, create layers and variables in the constructor, and implement the\\ncall() method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3:\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14, a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, its just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain\\n386 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'ing loops and skip connections. To implement this model, it is best to first create a\\nResidualBlock layer, since we are going to create a couple identical blocks (and we\\nmight want to reuse it in another model):\\nclass ResidualBlock(keras.layers.Layer):\\n    def __init__(self, n_layers, n_neurons, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\\n                                          kernel_initializer=\"he_normal\")\\n                       for _ in range(n_layers)]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        return inputs + Z\\nThis layer is a bit special since it contains other layers. This is handled transparently\\nby Keras: it automatically detects that the hidden attribute contains trackable objects\\n(layers in this case), so their variables are automatically added to this layers list of\\nvariables. The rest of this class is self-explanatory. Next, lets use the subclassing API\\nto define the model itself:\\nclass ResidualRegressor(keras.models.Model):\\n    def __init__(self, output_dim, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\\n                                          kernel_initializer=\"he_normal\")\\n        self.block1 = ResidualBlock(2, 30)\\n        self.block2 = ResidualBlock(2, 30)\\n        self.out = keras.layers.Dense(output_dim)\\n    def call(self, inputs):\\n        Z = self.hidden1(inputs)\\n        for _ in range(1 + 3):\\n            Z = self.block1(Z)\\n        Z = self.block2(Z)\\n        return self.out(Z)\\nWe create the layers in the constructor, and use them in the call() method. This\\nmodel can then be used like any other model (compile it, fit it, evaluate it and use it to\\nmake predictions). If you also want to be able to save the model using the save()\\nmethod, and load it using the keras.models.load_model() function, you must\\nimplement the get_config() method (as we did earlier) in both the ResidualBlock\\nclass and the ResidualRegressor class. Alternatively, you can just save and load the\\nweights using the save_weights() and load_weights() methods.\\nThe Model class is actually a subclass of the Layer class, so models can be defined and\\nused exactly like layers. But a model also has some extra functionalities, including of\\ncourse its compile(), fit(), evaluate() and predict() methods (and a few var\\nCustomizing Models and Training Algorithms | 387',\n",
       "  'iants, such as train_on_batch() or fit_generator()), plus the get_layers()\\nmethod (which can return any of the models layers by name or by index), and the\\nsave() method (and support for keras.models.load_model() and keras.mod\\nels.clone_model()). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer class,\\nwhile the latter should subclass the Model class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these.  Almost any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss() method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay\\ners, except it also implements a reconstruction loss (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the models inputs,\\nwe need to create this Dense layer in the build() method to have access to the shape\\nof the inputs. In the call() method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the models list of losses by calling add_loss(). During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor(keras.models.Model):\\n    def __init__(self, output_dim, **kwargs):\\n        super().__init__(**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\\n                                          kernel_initializer=\"lecun_normal\")\\n388 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim)\\n    def build(self, batch_input_shape):\\n        n_inputs = batch_input_shape[-1]\\n        self.reconstruct = keras.layers.Dense(n_inputs)\\n        super().build(batch_input_shape)\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction = self.reconstruct(Z)\\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\\n        self.add_loss(0.05 * recon_loss)\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean() object in the constructor, then call it in the\\ncall() method, passing it the recon_loss, and finally add it to the model by calling\\nthe models add_metric() method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, lets consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func\\ntion with regards to w1 is 6 * w1 + 2 * w2. Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par\\nCustomizing Models and Training Algorithms | 389',\n",
       "  'tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\\nis (36, 10). But if this were a neural network, the function would be much more com\\nplex, typically with tens of thousands of parameters, and finding the partial deriva\\ntives analytically by hand would be an almost impossible task. One solution could be\\nto compute an approximation of each partial derivative by measuring how much the\\nfunctions output changes when you tweak the corresponding parameter:\\n>>> w1, w2 = 5, 3\\n>>> eps = 1e-6\\n>>> (f(w1 + eps, w2) - f(w1, w2)) / eps\\n36.000003007075065\\n>>> (f(w1, w2 + eps) - f(w1, w2)) / eps\\n10.000000003174137\\nLooks about right! This works rather well and it is trivial to implement, but it is just\\nan approximation, and importantly you need to call f() at least once per parameter\\n(not twice, since we could compute f(w1, w2) just once). This makes this approach\\nintractable for large neural networks. So instead we should use autodiff (see Chap\\nter 10 and ???). TensorFlow makes this pretty simple:\\nw1, w2 = tf.Variable(5.), tf.Variable(3.)\\nwith tf.GradientTape() as tape:\\n    z = f(w1, w2)\\ngradients = tape.gradient(z, [w1, w2])\\nWe first define two variables w1 and w2, then we create a tf.GradientTape context\\nthat will automatically record every operation that involves a variable, and finally we\\nask this tape to compute the gradients of the result z with regards to both variables\\n[w1, w2]. Lets take a look at the gradients that TensorFlow computed:\\n>>> gradients\\n[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\\n <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\\nPerfect! Not only is the result accurate (the precision is only limited by the floating\\npoint errors), but the gradient() method only goes through the recorded computa\\ntions once (in reverse order), no matter how many variables there are, so it is incredi\\nbly efficient. Its like magic!\\nOnly put the strict minimum inside the tf.GradientTape() block,\\nto save memory. Alternatively, you can pause recording by creating\\na with tape.stop_recording()  block inside the tf.Gradient\\nTape() block.\\nThe tape is automatically erased immediately after you call its gradient() method, so\\nyou will get an exception if you try to call gradient() twice:\\n390 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'with tf.GradientTape() as tape:\\n    z = f(w1, w2)\\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\\ndz_dw2 = tape.gradient(z, w2) # RuntimeError!\\nIf you need to call gradient() more than once, you must make the tape persistent,\\nand delete it when you are done with it to free resources:\\nwith tf.GradientTape(persistent=True) as tape:\\n    z = f(w1, w2)\\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\\ndz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!\\ndel tape\\nBy default, the tape will only track operations involving variables, so if you try to\\ncompute the gradient of z with regards to anything else than a variable, the result will\\nbe None:\\nc1, c2 = tf.constant(5.), tf.constant(3.)\\nwith tf.GradientTape() as tape:\\n    z = f(c1, c2)\\ngradients = tape.gradient(z, [c1, c2]) # returns [None, None]\\nHowever, you can force the tape to watch any tensors you like, to record every opera\\ntion that involves them. Y ou can then compute gradients with regards to these ten\\nsors, as if they were variables:\\nwith tf.GradientTape() as tape:\\n    tape.watch(c1)\\n    tape.watch(c2)\\n    z = f(c1, c2)\\ngradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]\\nThis can be useful in some cases, for example if you want to implement a regulariza\\ntion loss that penalizes activations that vary a lot when the inputs vary little: the loss\\nwill be based on the gradient of the activations with regards to the inputs. Since the\\ninputs are not variables, you would need to tell the tape to watch them.\\nIf you compute the gradient of a list of tensors (e.g., [z1, z2, z3]) with regards to\\nsome variables (e.g., [w1, w2]), TensorFlow actually efficiently computes the sum of\\nthe gradients of these tensors (i.e., gradient(z1, [w1, w2]) , plus gradient(z2,\\n[w1, w2]), plus gradient(z3, [w1, w2])). Due to the way reverse-mode autodiff\\nworks, it is not possible to compute the individual gradients ( z1, z2 and z3) without\\nactually calling gradient() multiple times (once for z1, once for z2 and once for z3),\\nwhich requires making the tape persistent (and deleting it afterwards).\\nCustomizing Models and Training Algorithms | 391',\n",
       "  'Moreover, it is actually possible to compute second order partial derivatives (the Hes\\nsians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to\\nrecord the operations that are performed when computing the first-order partial\\nderivatives (the Jacobians): this requires a second tape. Here is how it works:\\nwith tf.GradientTape(persistent=True) as hessian_tape:\\n    with tf.GradientTape() as jacobian_tape:\\n        z = f(w1, w2)\\n    jacobians = jacobian_tape.gradient(z, [w1, w2])\\nhessians = [hessian_tape.gradient(jacobian, [w1, w2])\\n            for jacobian in jacobians]\\ndel hessian_tape\\nThe inner tape is used to compute the Jacobians, as we did earlier. The outer tape is\\nused to compute the partial derivatives of each Jacobian. Since we need to call gradi\\nent() once for each Jacobian (or else we would get the sum of the partial derivatives\\nover all the Jabobians, as explained earlier), we need the outer tape to be persistent, so\\nwe delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but\\nnow we also have the Hessians:\\n>>> hessians # dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\\n[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\\n  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\\n [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\\nLets verify these Hessians. The first two are the partial derivatives of 6 * w1 + 2 * w2\\n(which is, as we saw earlier, the partial derivative of f with regards to w1), with\\nregards to w1 and w2. The result is correct: 6 for w1 and 2 for w2. The next two are the\\npartial derivatives of 2 * w1  (the partial derivative of f with regards to w2), with\\nregards to w1 and w2, which are 2 for w1 and 0 for w2. Note that TensorFlow returns\\nNone instead of 0 since w2 does not appear at all in 2 * w1. TensorFlow also returns\\nNone when you use an operation whose gradients are not defined (e.g., tf.argmax()).\\nIn some rare cases you may want to stop gradients from backpropagating through\\nsome part of your neural network. To do this, you must use the tf.stop_gradient()\\nfunction: it just returns its inputs during the forward pass (like tf.identity()), but\\nit does not let gradients through during backpropagation (it acts like a constant). For\\nexample:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\\nwith tf.GradientTape() as tape:\\n    z = f(w1, w2) # same result as without stop_gradient()\\ngradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\\n392 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'Finally, you may occasionally run into some numerical issues when computing gradi\\nents. For example, if you compute the gradients of the my_softplus() function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable([100.])\\n>>> with tf.GradientTape() as tape:\\n...     z = my_softplus(x)\\n...\\n>>> tape.gradient(z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus() function, by decorating it with\\n@tf.custom_gradient, and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this functions gradients):\\n@tf.custom_gradient\\ndef my_better_softplus(z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients(grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus() function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where() to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit() method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10 actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit() method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393',\n",
       "  'compiling the model), implementing this paper requires writing your own custom\\nloop.\\nY ou may also like to write your own custom training loops simply to feel more confi\\ndent that it does precisely what you intent it to do (perhaps you are unsure about\\nsome details of the fit() method). It can sometimes feel safer to make everything\\nexplicit. However, remember that writing a custom training loop will make your code\\nlonger, more error prone and harder to maintain.\\nUnless you really need the extra flexibility, you should prefer using\\nthe fit() method rather than implementing your own training\\nloop, especially if you work in a team.\\nFirst, lets build a simple model. No need to compile it, since we will handle the train\\ning loop manually:\\nl2_reg = keras.regularizers.l2(0.05)\\nmodel = keras.models.Sequential([\\n    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\\n                       kernel_regularizer=l2_reg),\\n    keras.layers.Dense(1, kernel_regularizer=l2_reg)\\n])\\nNext, lets create a tiny function that will randomly sample a batch of instances from\\nthe training set (in Chapter 13 we will discuss the Data API, which offers a much bet\\nter alternative):\\ndef random_batch(X, y, batch_size=32):\\n    idx = np.random.randint(len(X), size=batch_size)\\n    return X[idx], y[idx]\\nLets also define a function that will display the training status, including the number\\nof steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\\nwill use the Mean metric to compute it), and other metrics:\\ndef print_status_bar(iteration, total, loss, metrics=None):\\n    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\\n                         for m in [loss] + (metrics or [])])\\n    end = \"\" if iteration < total else \"\\\\n\"\\n    print(\"\\\\r{}/{} - \".format(iteration, total) + metrics,\\n          end=end)\\nThis code is self-explanatory, unless you are unfamiliar with Python string format\\nting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using\\n\\\\r (carriage return) along with end=\"\" ensures that the status bar always gets printed\\non the same line. In the notebook, the print_status_bar() function also includes a\\nprogress bar, but you could use the handy tqdm library instead.\\n394 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'With that, lets get down to business! First, we need to define some hyperparameters,\\nchoose the optimizer, the loss function and the metrics (just the MAE in this exam\\nple):\\nn_epochs = 5\\nbatch_size = 32\\nn_steps = len(X_train) // batch_size\\noptimizer = keras.optimizers.Nadam(lr=0.01)\\nloss_fn = keras.losses.mean_squared_error\\nmean_loss = keras.metrics.Mean()\\nmetrics = [keras.metrics.MeanAbsoluteError()]\\nAnd now we are ready to build the custom loop!\\nfor epoch in range(1, n_epochs + 1):\\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\\n    for step in range(1, n_steps + 1):\\n        X_batch, y_batch = random_batch(X_train_scaled, y_train)\\n        with tf.GradientTape() as tape:\\n            y_pred = model(X_batch, training=True)\\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss] + model.losses)\\n        gradients = tape.gradient(loss, model.trainable_variables)\\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n        mean_loss(loss)\\n        for metric in metrics:\\n            metric(y_batch, y_pred)\\n        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\\n    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\\n    for metric in [mean_loss] + metrics:\\n        metric.reset_states()\\nTheres a lot going on in this code, so lets walk through it:\\n We create two nested loops: one for the epochs, the other for the batches within\\nan epoch.\\n Then we sample a random batch from the training set.\\n Inside the tf.GradientTape() block, we make a prediction for one batch (using\\nthe model as a function), and we compute the loss: it is equal to the main loss\\nplus the other losses (in this model, there is one regularization loss per layer).\\nSince the mean_squared_error() function returns one loss per instance, we\\ncompute the mean over the batch using tf.reduce_mean() (if you wanted to\\napply different weights to each instance, this is where you would do it). The regu\\nlarization losses are already reduced to a single scalar each, so we just need to\\nsum them (using tf.add_n(), which sums multiple tensors of the same shape\\nand data type).\\nCustomizing Models and Training Algorithms | 395',\n",
       "  '11 The truth is we did not process every single instance in the training set because we sampled instances ran\\ndomly, so some were processed more than once while others were not processed at all. In practice thats fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12 Alternatively, check out K.learning_phase(), K.set_learning_phase() and K.learning_phase_scope().\\n13 With the exception of optimizers, as very few people ever customize these: see the notebook for an example.\\n Next, we ask the tape to compute the gradient of the loss with regards to each\\ntrainable variable (not all variables!), and we apply them to the optimizer to per\\nform a Gradient Descent step.\\n Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n At the end of each epoch, we display the status bar again to make it look com\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizers clipnorm or clipvalue hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients() method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint or\\nbias_constraint when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients():\\nfor variable in model.variables:\\n    if variable.constraint is not None:\\n        variable.assign(variable.constraint(variable))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization or Dropout). To handle these,\\nyou need to call the model with training=True and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so its your call.\\nNow that you know how to customize any part of your models 13 and training algo\\nrithms, lets see how you can use TensorFlows automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlows API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  'there, but not as central, and much (much!) simpler to use. To demonstrate this, lets\\nstart with a trivial function that just computes the cube of its input:\\ndef cube(x):\\n    return x ** 3\\nWe can obviously call this function with a Python value, such as an int or a float, or\\nwe can call it with a tensor:\\n>>> cube(2)\\n8\\n>>> cube(tf.constant(2.0))\\n<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\\nNow, lets use tf.function() to convert this Python function to a TensorFlow Func\\ntion:\\n>>> tf_cube = tf.function(cube)\\n>>> tf_cube\\n<tensorflow.python.eager.def_function.Function at 0x1546fc080>\\nThis TF Function can then be used exactly like the original Python function, and it\\nwill return the same result (but as tensors):\\n>>> tf_cube(2)\\n<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\\n>>> tf_cube(tf.constant(2.0))\\n<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\\nUnder the hood, tf.function() analyzed the computations performed by the cube()\\nfunction and generated an equivalent computation graph! As you can see, it was\\nrather painless (we will see how this works shortly). Alternatively, we could have used\\ntf.function as a decorator; this is actually more common:\\n@tf.function\\ndef tf_cube(x):\\n    return x ** 3\\nThe original Python function is still available via the TF Functions python_function\\nattribute, in case you ever need it:\\n>>> tf_cube.python_function(2)\\n8\\nTensorFlow optimizes the computation graph, pruning unused nodes, simplifying\\nexpressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\\ngraph is ready, the TF Function efficiently executes the operations in the graph, in the\\nappropriate order (and in parallel when it can). As a result, a TF Function will usually\\nrun much faster than the original Python function, especially if it performs complex\\nTensorFlow Functions and Graphs | 397',\n",
       "  '14 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube() actually runs much slower than cube().\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThats all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function(). So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the models compile() method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)), a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)), the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])), a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10) and tf_cube(20) will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunctions source code to capture all the control flow statements, such as for loops\\nand while loops, if statements, as well as break, continue and return statements.\\nThis first step is called autograph. The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state\\nments: it offers magic methods like __add__() or __mul__() to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  '+ and *, but there are no __while__() or __if__() magic methods. After analyzing\\nthe functions code, autograph outputs an upgraded version of that function in which\\nall the control flow statements are replaced by the appropriate TensorFlow opera\\ntions, such as tf.while_loop() for loops and tf.cond() for if statements. For\\nexample, in Figure 12-4 , autograph analyzes the source code of the sum_squares()\\nPython function, and it generates the tf__sum_squares() function. In this function,\\nthe for loop is replaced by the definition of the loop_body() function (containing\\nthe body of the original for loop), followed by a call to the for_stmt() function. This\\ncall will build the appropriate tf.while_loop() operation in the computation graph.\\nFigure 12-4. How TensorFlow generates graphs using autograph and tracing\\nNext, TensorFlow calls this upgraded function, but instead of passing the actual\\nargument, it passes a symbolic tensor, meaning a tensor without any actual value, only\\na name, a data type, and a shape. For example, if you call sum_squares(tf.con\\nstant(10)), then the tf__sum_squares() function will actually be called with a sym\\nbolic tensor of type int32 and shape []. The function will run in graph mode, meaning\\nthat each TensorFlow operation will just add a node in the graph to represent itself\\nand its output tensor(s) (as opposed to the regular mode, called eager execution, or\\neager mode). In graph mode, TF operations do not perform any actual computations.\\nThis should feel familiar if you know TensorFlow 1, as graph mode was the default\\nmode. In Figure 12-4, you can see the tf__sum_squares() function being called with\\na symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final\\ngraph generated during tracing. The ellipses represent operations, and the arrows\\nrepresent tensors (both the generated function and the graph are simplified).\\nTensorFlow Functions and Graphs | 399',\n",
       "  'To view the generated functions source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function). The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum() instead of\\nnp.sum(), and tf.sort() instead of the built-in sorted() function, and so on\\n(unless you really want the code to run only during tracing).\\n For example, if you define a TF function f(x) that just returns np.ran\\ndom.rand(), a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the\\nsame random number, but f(tf.constant([2., 3.])) will return a different\\none. If you replace np.random.rand() with tf.random.uniform([]), then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func\\ntion is traced.\\n Y ou can wrap arbitrary Python code in a tf.py_function() operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function.\\n If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari\\nables outside of the TF Function (e.g., in the build() method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow',\n",
       "  ' The source code of your Python function should be available to TensorFlow. If\\nthe source code is unavailable (for example, if you define your function in the\\nPython shell, which does not give access to the source code, or if you deploy only\\nthe compiled Python files *.pyc to production), then the graph generation pro\\ncess will fail or have limited functionality.\\n TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\\nmake sure you use for i in tf.range(10) rather than for i in range(10), or\\nelse the loop will not be captured in the graph. Instead, it will run during tracing.\\nThis may be what you want, if the for loop is meant to build the graph, for exam\\nple to create each layer in a neural network.\\n And as always, for performance reasons, you should prefer a vectorized imple\\nmentation whenever you can, rather than using loops.\\nIts time to sum up! In this chapter we started with a brief overview of TensorFlow,\\nthen we looked at TensorFlows low-level API, including tensors, operations, variables\\nand special data structures. We then used these tools to customize almost every com\\nponent in tf.keras. Finally, we looked at how TF Functions can boost performance,\\nhow graphs are generated using autograph and tracing, and what rules to follow when\\nyou write TF Functions (if you would like to open the black box a bit further, for\\nexample to explore the generated graphs, you will find further technical details\\nin ???).\\nIn the next chapter, we will look at how to efficiently load and preprocess data with\\nTensorFlow.\\nTensorFlow Functions and Graphs | 401',\n",
       "  'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API: you just create a data\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlows TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten\\nsions are available to read from all sorts of data sources, such as Googles BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API: it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403',\n",
       "  'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor\\nFlows ecosystem:\\n TF Transform ( tf.Transform) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve\\nnient dataset objects to manipulate them using the Data API.\\nSo lets get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset: as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity lets just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices():\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices(X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices() function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con\\ntains 10 items: tensors 0, 1, 2, , 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10).\\nY ou can simply iterate over a datasets items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans\\nformations like this (this chain is illustrated in Figure 13-1):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat() method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch() method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405',\n",
       "  'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ...), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: its as simple as setting the num_parallel_calls argument.\\nWhile the map() applies a transformation to each item, the apply() method applies a\\ntransformation to the dataset as a whole. For example, the following code unbatches\\nthe dataset, by applying the unbatch() function to the dataset (this function is cur\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter() method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle() method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '1 Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and shuffle them, then pick\\none randomly and put it to your right, keeping the other 2 in your hands. Take another card on your left,\\nshuffle the 3 cards in your hands and pick one of them randomly, and put it on your right. When you are\\ndone going through all the cards like this, you will have a deck of cards on your right: do you think it will be\\nperfectly shuffled?\\nit is important to make it large enough or else shuffling will not be very efficient. 1\\nHowever, obviously do not exceed the amount of RAM you have, and even if you\\nhave plenty of it, theres no need to go well beyond the datasets size. Y ou can provide\\na random seed if you want the same random order every time you run your program.\\n>>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\\n>>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\\ntf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\\ntf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\\ntf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\\ntf.Tensor([3 6], shape=(2,), dtype=int64)\\nIf you call repeat() on a shuffled dataset, by default it will generate\\na new order at every iteration. This is generally a good idea, but if\\nyou prefer to reuse the same order at each iteration (e.g., for tests\\nor debugging), you can set reshuffle_each_iteration=False.\\nFor a large dataset that does not fit in memory, this simple shuffling-buffer approach\\nmay not be sufficient, since the buffer will be small compared to the dataset. One sol\\nution is to shuffle the source data itself (for example, on Linux you can shuffle text\\nfiles using the shuf command). This will definitely improve shuffling a lot! However,\\neven if the source data is shuffled, you will usually want to shuffle it some more, or\\nelse the same order will be repeated at each epoch, and the model may end up being\\nbiased (e.g., due to some spurious patterns present by chance in the source datas\\norder). To shuffle the instances some more, a common approach is to split the source\\ndata into multiple files, then read them in a random order during training. However,\\ninstances located in the same file will still end up close to each other. To avoid this\\nyou can pick multiple files randomly, and read them simultaneously, interleaving\\ntheir lines. Then on top of that you can add a shuffling buffer using the shuffle()\\nmethod. If all this sounds like a lot of work, dont worry: the Data API actually makes\\nall this possible in just a few lines of code. Lets see how to do this.\\nThe Data API | 407',\n",
       "  \"Interleaving Lines From Multiple Files\\nFirst, lets suppose that you loaded the California housing dataset, you shuffled it\\n(unless it was already shuffled), you split it into a training set, a validation set and a\\ntest set, then you split each set into many CSV files that each look like this (each row\\ncontains 8 input features plus the target median house value):\\nMedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\\n3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\\n5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\\n3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\\n[...]\\nLets also suppose train_filepaths contains the list of file paths (and you also have\\nvalid_filepaths and test_filepaths):\\n>>> train_filepaths\\n['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\\nNow lets create a dataset containing only these file paths:\\nfilepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\\nBy default, the list_files() function returns a dataset that shuffles the file paths. In\\ngeneral this is a good thing, but you can set shuffle=False if you do not want that,\\nfor some reason.\\nNext, we can call the interleave() method to read from 5 files at a time and inter\\nleave their lines (skipping the first line of each file, which is the header row, using the\\nskip() method):\\nn_readers = 5\\ndataset = filepath_dataset.interleave(\\n    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\\n    cycle_length=n_readers)\\nThe interleave() method will create a dataset that will pull 5 file paths from the\\nfilepath_dataset, and for each one it will call the function we gave it (a lambda in\\nthis example) to create a new dataset, in this case a TextLineDataset. It will then\\ncycle through these 5 datasets, reading one line at a time from each until all datasets\\nare out of items. Then it will get the next 5 file paths from the filepath_dataset, and\\ninterleave them the same way, and so on until it runs out of file paths.\\nFor interleaving to work best, it is preferable to have files of identi\\ncal length, or else the end of the longest files will not be interleaved.\\n408 | Chapter 13: Loading and Preprocessing Data with TensorFlow\",\n",
       "  \"By default, interleave() does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Lets look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLets implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs = 8\\ndef preprocess(line):\\n  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv(line, record_defaults=defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLets walk through this code:\\n First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean and X_std are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n The preprocess() function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv() function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32 as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con\\nThe Data API | 409\",\n",
       "  \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n The decode_csv() function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack() on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLets test this preprocessing function:\\n>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, lets put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2):\\ndef csv_reader_dataset(filepaths, repeat=None, n_readers=5,\\n                       n_read_threads=None, shuffle_buffer_size=10000,\\n                       n_parse_threads=5, batch_size=32):\\n    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\\n    dataset = dataset.interleave(\\n        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\\n        cycle_length=n_readers, num_parallel_calls=n_read_threads)\\n    dataset = dataset.shuffle(shuffle_buffer_size)\\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\\n    dataset = dataset.batch(batch_size)\\n    return dataset.prefetch(1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\",\n",
       "  '2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1)),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1) at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead 2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3. If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls when calling interleave() and map()), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411',\n",
       "  'Figure 13-3. Speedup Training Thanks to Prefetching and Multithreading\\nIf you plan to purchase a GPU card, its processing power and its\\nmemory size are of course very important (in particular, a large\\nRAM is crucial for computer vision), but its memory bandwidth is\\njust as important as the processing power to get good performance:\\nthis is the number of gigabytes of data it can get in or out of its\\nRAM per second.\\nWith that, you can now build efficient input pipelines to load and preprocess data\\nfrom multiple text files. We have discussed the most common dataset methods, but\\nthere are a few more you may want to look at: concatenate(), zip(), window(),\\nreduce(), cache(), shard(), flat_map() and padded_batch(). There are also a cou\\nple more class methods: from_generator() and from_tensors(), which create a new\\ndataset from a Python generator or a list of tensors respectively. Please check the API\\ndocumentation for more details. Also note that there are experimental features avail\\nable in tf.data.experimental, many of which will most likely make it to the core\\nAPI in future releases (e.g., check out the CsvDataset class and the SqlDataset\\nclasses).\\n412 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '3 Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4 The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5 Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).\\nUsing the Dataset With tf.keras\\nNow we can use the csv_reader_dataset() function to create a dataset for the train\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set = csv_reader_dataset(train_filepaths, repeat=None)\\nvalid_set = csv_reader_dataset(valid_filepaths)\\ntest_set = csv_reader_dataset(test_filepaths)\\nAnd now we can simply build and train a Keras model using these datasets. 3 All we\\nneed to do is to call the fit() method with the datasets instead of X_train and\\ny_train, and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential([...])\\nmodel.compile([...])\\nmodel.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\\n          validation_data=valid_set,\\n          validation_steps=len(X_valid) // batch_size)\\nSimilarly, we can pass a dataset to the evaluate() and predict() methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate(test_set, steps=len(X_test) // batch_size)\\nmodel.predict(new_set, steps=len(X_new) // batch_size)\\nUnlike the other sets, the new_set will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set:\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer, loss_fn, n_epochs, [...]):\\n    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\\n    for X_batch, y_batch in train_set:\\n        with tf.GradientTape() as tape:\\nThe Data API | 413',\n",
       "  'y_pred = model(X_batch)\\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss] + model.losses)\\n        grads = tape.gradient(loss, model.trainable_variables)\\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So lets use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have to use TFRecords. As the saying goes, if it\\naint broke, dont fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlows preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter class:\\nwith tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\\n    f.write(b\"This is the first record\")\\n    f.write(b\"And this is the second record\")\\nAnd you can then use a tf.data.TFRecordDataset to read one or more TFRecord\\nfiles:\\nfilepaths = [\"my_data.tfrecord\"]\\ndataset = tf.data.TFRecordDataset(filepaths)\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads. Alternatively, you could\\nobtain the same result by using list_files() and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '6 Since protobuf objects are meant to be serialized and transmitted, they are called messages.\\nCompressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options argument:\\noptions = tf.io.TFRecordOptions(compression_type=\"GZIP\")\\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\\n                                  compression_type=\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs). This is a portable, extensi\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC, Googles remote proce\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\";\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person object6 may (optionally) have a name of type string, an id of type int32,\\nand zero or more email fields, each of type string. The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each records binary representation. Once you\\nhave a definition in a .proto file, you can compile it. This requires protoc, the proto\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc. All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlets look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2 import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415',\n",
       "  '7 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf.\\nname: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\")  # add an email address\\n>>> s = person.SerializeToString()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString(s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person class generated by protoc, we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString() method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString() method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString() and ParseFrom\\nString() are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function() operation, which would make the code slower and less\\nportable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\";\\nmessage BytesList { repeated bytes value = 1; }\\nmessage FloatList { repeated float value = 1 [packed = true]; }\\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '8 Why was Example even defined since it contains no more than a Features object? Well, TensorFlow may one\\nday decide to add more fields to it. As long as the new Example definition still contains the features field,\\nwith the same id, it will be backward compatible. This extensibility is one of the great features of protobufs.\\nmessage Feature {\\n    oneof kind {\\n        BytesList bytes_list = 1;\\n        FloatList float_list = 2;\\n        Int64List int64_list = 3;\\n    }\\n};\\nmessage Features { map<string, Feature> feature = 1; };\\nmessage Example { Features features = 1; };\\nThe definitions of BytesList, FloatList and Int64List are straightforward enough\\n([packed = true] is used for repeated numerical fields, for a more efficient encod\\ning). A Feature either contains a BytesList, a FloatList or an Int64List. A Fea\\ntures (with an s) contains a dictionary that maps a feature name to the\\ncorresponding feature value. And finally, an Example just contains a Features object.8\\nHere is how you could create a tf.train.Example representing the same person as\\nearlier, and write it to TFRecord file:\\nfrom tensorflow.train import BytesList, FloatList, Int64List\\nfrom tensorflow.train import Feature, Features, Example\\nperson_example = Example(\\n    features=Features(\\n        feature={\\n            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\\n            \"id\": Feature(int64_list=Int64List(value=[123])),\\n            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\\n                                                          b\"c@d.com\"]))\\n        }))\\nThe code is a bit verbose and repetitive, but its rather straightforward (and you could\\neasily wrap it inside a small helper function). Now that we have an Example protobuf,\\nwe can serialize it by calling its SerializeToString() method, then write the result\\ning data to a TFRecord file:\\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\\n    f.write(person_example.SerializeToString())\\nNormally you would write much more than just one example! Typically, you would\\ncreate a conversion script that reads from your current format (say, CSV files), creates\\nan Example protobuf for each instance, serializes them and saves them to several\\nTFRecord files, ideally shuffling them in the process. This requires a bit of work, so\\nonce again make sure it is really necessary (perhaps your pipeline works fine with\\nCSV files).\\nThe TFRecord Format | 417',\n",
       "  'Now that we have a nice TFRecord file containing a serialized Example, lets try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example using tf.io.parse_single_example().\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature descriptor indicating the features shape,\\ntype and default value, or a tf.io.VarLenFeature descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\" feature). For example:\\nfeature_description = {\\n    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\\n    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\\n    \"emails\": tf.io.VarLenFeature(tf.string),\\n}\\nfor serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\\n    parsed_example = tf.io.parse_single_example(serialized_example,\\n                                                feature_description)\\nThe fixed length features are parsed as regular tensors, but the variable length fea\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense(), but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example[\"emails\"].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg() to encode an image using the JPEG\\nformat, and put this binary data in a BytesList. Later, when your code reads the\\nTFRecord, it will start by parsing the Example, then you will need to call\\ntf.io.decode_jpeg() to parse the data and get the original image (or you can use\\ntf.io.decode_image(), which can decode any BMP , GIF , JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList by serializing the tensor using\\ntf.io.serialize_tensor(), then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor().\\nInstead of parsing examples one by one using tf.io.parse_single_example(), you\\nmay want to parse them batch by batch using tf.io.parse_example():\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\\nfor serialized_examples in dataset:\\n    parsed_examples = tf.io.parse_example(serialized_examples,\\n                                          feature_description)\\nAs you can see, the Example proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the documents author, title and publication date. TensorFlows\\nSequenceExample protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample Protobuf\\nHere is the definition of the SequenceExample protobuf:\\nmessage FeatureList { repeated Feature feature = 1; };\\nmessage FeatureLists { map<string, FeatureList> feature_list = 1; };\\nmessage SequenceExample {\\n    Features context = 1;\\n    FeatureLists feature_lists = 2;\\n};\\nA SequenceExample contains a Features object for the contextual data and a Fea\\ntureLists object which contains one or more named FeatureList objects (e.g., a\\nFeatureList named \"content\" and another named \"comments\"). Each FeatureList\\njust contains a list of Feature objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample, serializing it and parsing it is very similar to building, serializing\\nand parsing an Example, but you must use tf.io.parse_single_sequence_exam\\nple() to parse a single SequenceExample or tf.io.parse_sequence_example() to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the\\nfull code):\\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\\n    serialized_sequence_example, context_feature_descriptions,\\n    sequence_feature_descriptions)\\nparsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419',\n",
       "  'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the datasets map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learns ColumnTransformer class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLets go back to the variant of the California housing dataset that we used in Chap\\nter 2, since it includes a categorical feature and missing data. Here is a simple numeri\\ncal column named \"housing_median_age\":\\nhousing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, lets tweak the \"housing_median_age\" column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age = tf.feature_column.numeric_column(\\n    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlets create a bucketized column based on the median_income column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income = tf.feature_column.numeric_column(\"median_income\")\\nbucketized_income = tf.feature_column.bucketized_column(\\n    median_income, boundaries=[1.5, 3., 4.5, 6.])\\nIf the median_income feature is equal to, say, 3.2, then the bucketized_income feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal, meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age column.\\nCategorical Features\\nFor categorical features such as ocean_proximity, there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity() function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list():\\nocean_prox_vocab = [\\'<1H OCEAN\\', \\'INLAND\\', \\'ISLAND\\', \\'NEAR BAY\\', \\'NEAR OCEAN\\']\\nocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\\n    \"ocean_proximity\", ocean_prox_vocab)\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file() instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate\\ngories, or perhaps categories may be added or removed so frequently that using cate\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket(). If we had a \"city\" feature in the dataset,\\nwe could encode it like this:\\ncity_hash = tf.feature_column.categorical_column_with_hash_bucket(\\n    \"city\", hash_bucket_size=1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column. For example, suppose people are partic\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421',\n",
       "  '9 Since the housing_median_age feature was normalized, the boundaries are for normalized ages.\\ncreate a bucketized column for the housing_median_age feature9, and cross it with\\nthe ocean_proximity column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size, and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age = tf.feature_column.bucketized_column(\\n    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity = tf.feature_column.crossed_column(\\n    [bucketized_age, ocean_proximity], hash_bucket_size=100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion column. This will create a 2020 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude = tf.feature_column.numeric_column(\"latitude\")\\nlongitude = tf.feature_column.numeric_column(\"longitude\")\\nbucketized_latitude = tf.feature_column.bucketized_column(\\n    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\\nbucketized_longitude = tf.feature_column.bucketized_column(\\n    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\\nlocation = tf.feature_column.crossed_column(\\n    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings. For the first option, simply use the indicator_col\\numn() function:\\nocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\" category could\\nbe represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\\nOCEAN\" category may be represented by another random vector such as [0.631,\\n0.791] (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\" categorys embedding (see Figure 13-4).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa\\ntions of the categories. This is called representation learning (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423',\n",
       "  '10 Distributed Representations of Words and Phrases and their Compositionality , T. Mikolov et al. (2013).\\nFigure 13-4. Embeddings Will Gradually Improve During Training\\nWord Embeddings\\nNot only will embeddings generally be useful representations for the task at hand, but\\nquite often these same embeddings can be reused successfully for other tasks as well.\\nThe most common example of this is word embeddings (i.e., embeddings of individual\\nwords): when you are working on a natural language processing task, you are often\\nbetter off reusing pretrained word embeddings than training your own. The idea of\\nusing vectors to represent words dates back to the 1960s, and many sophisticated\\ntechniques have been used to generate useful vectors, including using neural net\\nworks, but things really took off in 2013, when Tom Mikolov and other Google\\nresearchers published a paper10 describing how to learn word embeddings using deep\\nneural networks, much faster than previous attempts. This allowed them to learn\\nembeddings on a very large corpus of text: they trained a deep neural network to pre\\ndict the words near any given word. This allowed them to obtain astounding word\\nembeddings. For example, synonyms had very close embeddings, and semantically\\nrelated words such as France, Spain, Italy, and so on, ended up clustered together. But\\nits not just about proximity: word embeddings were also organized along meaningful\\naxes in the embedding space. Here is a famous example: if you compute King  Man\\n+ Woman (adding and subtracting the embedding vectors of these words), then the\\nresult will be very close to the embedding of the word Queen (see Figure 13-5 ). In\\nother words, the word embeddings encode the concept of gender! Similarly, you can\\ncompute Madrid  Spain + France, and of course the result is close to Paris, which\\nseems to show that the notion of capital city was also encoded in the embeddings.\\n424 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'Figure 13-5. Word Embeddings\\nLets go back to the Features API. Here is how you could encode the ocean_proxim\\nity categories as 2D embeddings:\\nocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\\n                                                           dimension=2)\\nEach of the five ocean_proximity categories will now be represented as a 2D vector.\\nThese vectors are stored in an embedding matrix with one row per category, and one\\ncolumn per embedding dimension, so in this example it is a 52 matrix. When an\\nembedding column is given a category index as input (say, 3, which corresponds to\\nthe category \"NEAR BAY\"), it just performs a lookup in the embedding matrix and\\nreturns the corresponding row (say, [0.331, 0.190]). Unfortunately, the embedding\\nmatrix can be quite large, especially when you have a large vocabulary: if this is the\\ncase, the model can only learn good representations for the categories for which it has\\nsufficient training data. To reduce the size of the embedding matrix, you can of\\ncourse try lowering the dimension hyperparameter, but if you reduce this parameter\\ntoo much, the representations may not be as good. Another option is to reduce the\\nvocabulary size (e.g., if you are dealing with text, you can try dropping the rare words\\nfrom the vocabulary, and replace them all with a token like \"<unknown>\" or \"<UNK>\").\\nIf you are using hash buckets, you can also try reducing the hash_bucket_size (but\\nnot too much, or else you will get collisions).\\nThe Features API | 425',\n",
       "  'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLets suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec() function to generate feature descriptions (so you dont\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age, ....., median_house_value] # all features + target\\nfeature_descriptions = tf.feature_column.make_parse_example_spec(columns)\\nY ou dont always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2] when calling numerical_column().\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples(serialized_examples):\\n    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\\n    targets = examples.pop(\"median_house_value\") # separate the targets\\n    return examples, targets\\nNext, you can create a TFRecordDataset that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example protobufs with the appropri\\nate features):\\nbatch_size = 32\\ndataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target = columns[:-1]\\nmodel = keras.models.Sequential([\\n    keras.layers.DenseFeatures(feature_columns=columns_without_target),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\\nsteps_per_epoch = len(X_train) // batch_size\\nhistory = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=5)\\nThe DenseFeatures layer will take care of converting every input feature to a dense\\nrepresentation, and it will also apply any extra transformation we specified, such as\\nscaling the housing_median_age using the normalizer_fn function we provided. Y ou\\ncan take a closer look at what the DenseFeatures layer does by calling it directly:\\n>>> some_columns = [ocean_proximity_embed, bucketized_income]\\n>>> dense_features = keras.layers.DenseFeatures(some_columns)\\n>>> dense_features({\\n...     \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\\n...     \"median_income\": [[3.], [7.2], [1.]]\\n... })\\n...\\n<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\\narray([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\\n       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\\n       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\\nIn this example, we create a DenseFeatures layer with just two columns, and we call\\nit with some data, in the form of a dictionary of features. In this case, since the bucke\\ntized_income column relies on the median_income column, the dictionary must\\ninclude the \"median_income\" key, and similarly since the ocean_proximity_embed\\ncolumn is based on the ocean_proximity column, the dictionary must include the\\n\"ocean_proximity\" key. Columns are handled in alphabetical order, so first we look\\nat the bucketized income column (its name is the same as the median_income column\\nname, plus \"_bucketized\"). The incomes 3, 7.2 and 1 get mapped respectively to cat\\negory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat\\negory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded:\\ncategory 2 gets encoded as [0., 0., 1., 0., 0.]  and so on (note that bucketized\\ncolumns get one-hot encoded by default, no need to call indicator_column()). Now\\non to the ocean_proximity_embed column. The \"NEAR OCEAN\" and \"INLAND\" cate\\ngories just get mapped to their respective embeddings (which were initialized ran\\ndomly). The resulting tensor is the concatenation of the one-hot vectors and the\\nembeddings.\\nNow you can feed all kinds of features to a neural network, including numerical fea\\ntures, categorical features, and even text (by splitting the text into words, then using\\nword embedding)! However, performing all the preprocessing on the fly can slow\\ndown training. Lets see how this can be improved.\\nThe Features API | 427',\n",
       "  'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew will lead to bugs or degraded perfor\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures layer). Thats definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layers code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform as tft\\ndef preprocess(inputs):  # inputs is a batch of input features\\n    median_age = inputs[\"housing_median_age\"]\\n    ocean_proximity = inputs[\"ocean_proximity\"]\\n    standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\\n    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\\n    return {\\n        \"standardized_median_age\": standardized_age,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  '11 At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\": ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess() function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age feature, and the vocabulary\\nfor the ocean_proximity feature. The components that compute these statistics are\\ncalled analyzers.\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet 11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429',\n",
       "  'and one for testing, but this depends on the dataset you choose). For example, lets\\ndownload MNIST:\\nimport tensorflow_datasets as tfds\\ndataset = tfds.load(name=\"mnist\")\\nmnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\\nY ou can then apply any transformation you want (typically repeating, batching and\\nprefetching), and youre ready to train your model. Here is a simple example:\\nmnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\\nfor item in mnist_train:\\n    images = item[\"image\"]\\n    labels = item[\"label\"]\\n    [...]\\nIn general, load() returns a shuffled training set, so theres no need\\nto shuffle it some more.\\nNote that each item in the dataset is a dictionary containing both the features and the\\nlabels. But Keras expects each item to be a tuple containing 2 elements (again, the fea\\ntures and the labels). Y ou could transform the dataset using the map() method, like\\nthis:\\nmnist_train = mnist_train.repeat(5).batch(32)\\nmnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\\nmnist_train = mnist_train.prefetch(1)\\nOr you can just ask the load() function to do this for you by setting as_super\\nvised=True (obviously this works only for labeled datasets). Y ou can also specify the\\nbatch size if you want. Then the dataset can be passed directly to your tf.keras model:\\ndataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\\nmnist_train = dataset[\"train\"].repeat().prefetch(1)\\nmodel = keras.models.Sequential([...])\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\\nmodel.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)\\nThis was quite a technical chapter, and you may feel that it is a bit far from the\\nabstract beauty of neural networks, but the fact is deep learning often involves large\\namounts of data, and knowing how to load, parse and preprocess it efficiently is a\\ncrucial skill to have. In the next chapter, we will look at Convolutional Neural Net\\nworks, which are among the most successful neural net architectures for image pro\\ncessing, and many other applications.\\n430 | Chapter 13: Loading and Preprocessing Data with TensorFlow',\n",
       "  'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBMs Deep Blue supercomputer beat the chess world champion Garry Kas\\nparov back in 1996, it wasnt until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec\\nognize a cute puppy; its just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brains visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11 for training deep nets, CNNs have man\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431',\n",
       "  '1 Single Unit Activity in Striate Cortex of Unrestrained Cats,  D. Hubel and T. Wiesel (1958).\\n2 Receptive Fields of Single Neurones in the Cats Striate Cortex,  D. Hubel and T. Wiesel (1959).\\n3 Receptive Fields and Functional Architecture of Monkey Striate Cortex,  D. Hubel and T. Wiesel (1968).\\nat many other tasks, such as voice recognition or natural language processing (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys 3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1, in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '4 Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\\nby Shift in Position,  K. Fukushima (1980).\\n5 Gradient-Based Learning Applied to Document Recognition,  Y . LeCun et al. (1998).\\nFigure 14-1. Local receptive fields in the visual cortex\\nThese studies of the visual cortex inspired the neocognitron, introduced in 1980 ,4\\nwhich gradually evolved into what we now call convolutional neural networks . An\\nimportant milestone was a 1998 paper5 by Y ann LeCun, Lon Bottou, Y oshua Bengio,\\nand Patrick Haffner, which introduced the famous LeNet-5 architecture, widely used\\nto recognize handwritten check numbers. This architecture has some building blocks\\nthat you already know, such as fully connected layers and sigmoid activation func\\ntions, but it also introduces two new building blocks: convolutional layers and pooling\\nlayers. Lets look at them now.\\nWhy not simply use a regular deep neural network with fully con\\nnected layers for image recognition tasks? Unfortunately, although\\nthis works fine for small images (e.g., MNIST), it breaks down for\\nlarger images because of the huge number of parameters it\\nrequires. For example, a 100  100 image has 10,000 pixels, and if\\nthe first layer has just 1,000 neurons (which already severely\\nrestricts the amount of information transmitted to the next layer),\\nthis means a total of 10 million connections. And thats just the first\\nlayer. CNNs solve this problem using partially connected layers and\\nweight sharing.\\nThe Architecture of the Visual Cortex | 433',\n",
       "  '6 A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76 for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer:6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh  1, columns j to j + fw  1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3). In\\norder for a layer to have the same height and width as the previous layer, it is com\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'mon to add zeros around the inputs, as shown in the diagram. This is called zero pad\\nding.\\nFigure 14-3. Connections between layers and zero padding\\nIt is also possible to connect a large input layer to a much smaller layer by spacing out\\nthe receptive fields, as shown in Figure 14-4. The shift from one receptive field to the\\nnext is called the stride. In the diagram, a 5  7 input layer (plus zero padding) is con\\nnected to a 3  4 layer, using 3  3 receptive fields and a stride of 2 (in this example\\nthe stride is the same in both directions, but it does not have to be so). A neuron loca\\nted in row i, column j in the upper layer is connected to the outputs of the neurons in\\nthe previous layer located in rows i  sh to i  sh + fh  1, columns j  sw to j  sw + fw \\n1, where sh and sw are the vertical and horizontal strides.\\nConvolutional Layer | 435',\n",
       "  'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neurons weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5 shows two possible sets of weights, called filters (or convolu\\ntion kernels). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7  7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5 (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map, which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'Figure 14-5. Applying two different filters to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre\\nsented in 3D (see Figure 14-6). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ\\nent parameters. A neurons receptive field is the same as described earlier, but it\\nextends across all the previous layers feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437',\n",
       "  'one channel, but some images may have much morefor example, satellite images\\nthat capture extra light frequencies (such as infrared).\\nFigure 14-6. Convolution layers with multiple feature maps, and images with three color\\nchannels\\nSpecifically, a neuron located in row i, column j of the feature map k in a given convo\\nlutional layer l is connected to the outputs of the neurons in the previous layer l  1,\\nlocated in rows i  sh to i  sh + fh  1 and columns j  sw to j  sw + fw  1, across all\\nfeature maps (in layer l  1). Note that all neurons located in the same row i and col\\numn j but in different feature maps are connected to the outputs of the exact same\\nneurons in the previous layer.\\nEquation 14-1 summarizes the preceding explanations in one big mathematical equa\\ntion: it shows how to compute the output of a given neuron in a convolutional layer.\\n438 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi, j, k = bk + \\nu = 0\\nf h  1\\n\\nv = 0\\nf w  1\\n\\nk = 0\\nf n  1\\nxi, j, k . wu, v, k, k with\\ni = i  sh + u\\nj = j  sw + v\\n zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn is the number of feature maps\\nin the previous layer (layer l  1).\\n xi, j, k is the output of the neuron located in layer l  1, row i, column j, feature\\nmap k (or channel k if the previous layer is the input layer).\\n bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n wu, v, k ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neurons receptive field),\\nand feature map k.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels]. A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn, fn]. The bias terms of a convo\\nlutional layer are simply represented as a 1D tensor of shape [fn].\\nLets look at a simple example. The following code loads two sample images, using\\nScikit-Learns load_sample_images() (which loads two color images, one of a Chi\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7  7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d() function,\\nwhich is part of TensorFlows low-level Deep Learning API. In this example, we use\\nzero padding (padding=\"SAME\") and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5).\\nConvolutional Layer | 439',\n",
       "  'from sklearn.datasets import load_sample_image\\n# Load sample images\\nchina = load_sample_image(\"china.jpg\") / 255\\nflower = load_sample_image(\"flower.jpg\") / 255\\nimages = np.array([china, flower])\\nbatch_size, height, width, channels = images.shape\\n# Create 2 filters\\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\\nfilters[:, 3, :, 0] = 1  # vertical line\\nfilters[3, :, :, 1] = 1  # horizontal line\\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image\\'s 2nd feature map\\nplt.show()\\nMost of this code is self-explanatory, but the tf.nn.conv2d() line deserves a bit of\\nexplanation:\\n images is the input mini-batch (a 4D tensor, as explained earlier).\\n filters is the set of filters to apply (also a 4D tensor, as explained earlier).\\n strides is equal to 1, but it could also be a 1D array with 4 elements, where the\\ntwo central elements are the vertical and horizontal strides ( sh and sw). The first\\nand last elements must currently be equal to 1. They may one day be used to\\nspecify a batch stride (to skip some instances) and a channel stride (to skip some\\nof the previous layers feature maps or channels).\\n padding must be either \"VALID\" or \"SAME\":\\n If set to \"VALID\", the convolutional layer does not use zero padding, and may\\nignore some rows and columns at the bottom and right of the input image,\\ndepending on the stride, as shown in Figure 14-7 (for simplicity, only the hor\\nizontal dimension is shown here, but of course the same logic applies to the\\nvertical dimension).\\n If set to \"SAME\", the convolutional layer uses zero padding if necessary. In this\\ncase, the number of output neurons is equal to the number of input neurons\\ndivided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to\\n3). Then zeros are added as evenly as possible around the inputs.\\n440 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'Figure 14-7. Padding optionsinput width: 13, filter width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\\n                           padding=\"SAME\", activation=\"relu\")\\nThis code creates a Conv2D layer with 32 filters, each 3  3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara\\nmeter values, but this is very time-consuming. We will discuss common CNN archi\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5  5 filters, outputting 200 feature\\nmaps of size 150  100, with stride 1 and SAME padding. If the input is a 150  100\\nConvolutional Layer | 441',\n",
       "  '7 A fully connected layer with 150  100 neurons, each connected to all 150  100  3 inputs, would have 1502\\n 1002  3 = 675 million parameters!\\n8 In the international system of units (SI), 1 MB = 1,000 kB = 1,000  1,000 bytes = 1,000  1,000  8 bits.\\nRGB image (three channels), then the number of parameters is (5  5  3 + 1)  200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150  100 neu\\nrons, and each of these neurons needs to compute a weighted sum of its 5  5  3 =\\n75 inputs: thats a total of 225 million float multiplications. Not as bad as a fully con\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layers output will occupy\\n200  150  100  32 = 96 million bits (12 MB) of RAM. 8 And thats just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib\\nute the CNN across multiple devices.\\nNow lets look at the second common building block of CNNs: the pooling layer.\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8 shows a\\nmax pooling layer, which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '9 Other kernels we discussed so far had weights, but pooling kernels do not: they are just stateless sliding win\\ndows.\\nwe use a 2  2 _pooling kernel_ 9, with a stride of 2, and no padding. Only the max\\ninput value in each receptive field makes it to the next layer, while the other inputs\\nare dropped. For example, in the lower left receptive field in Figure 14-8, the input\\nvalues are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because\\nof the stride of 2, the output image has half the height and half the width of the input\\nimage (rounded down since we use no padding).\\nFigure 14-8. Max pooling layer (2  2 pooling kernel, stride 2, no padding)\\nA pooling layer typically works on every input channel independ\\nently, so the output depth is the same as the input depth.\\nOther than reducing computations, memory usage and the number of parameters, a\\nmax pooling layer also introduces some level of invariance to small translations, as\\nshown in Figure 14-9. Here we assume that the bright pixels have a lower value than\\ndark pixels, and we consider 3 images (A, B, C) going through a max pooling layer\\nwith a 2  2 kernel and stride 2. Images B and C are the same as image A, but shifted\\nby one and two pixels to the right. As you can see, the outputs of the max pooling\\nlayer for images A and B are identical. This is what translation invariance means.\\nHowever, for image C, the output is different: it is shifted by one pixel to the right\\n(but there is still 75% invariance). By inserting a max pooling layer every few layers in\\na CNN, it is possible to get some level of translation invariance at a larger scale.\\nMoreover, max pooling also offers a small amount of rotational invariance and a\\nslight scale invariance. Such invariance (even if it is limited) can be useful in cases\\nwhere the prediction should not depend on these details, such as in classification\\ntasks.\\nPooling Layer | 443',\n",
       "  'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2  2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman\\ntic segmentation: this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance, not invariance: a small change to the inputs should lead to a corre\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2  2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool = keras.layers.MaxPool2D(pool_size=2)\\nTo create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'mostly use max pooling layers now, as they generally perform better. This may seem\\nsurprising, since computing the mean generally loses less information than comput\\ning the max. But on the other hand, max pooling preserves only the strongest feature,\\ngetting rid of all the meaningless ones, so the next layers get a cleaner signal to work\\nwith. Moreover, max pooling offers stronger translation invariance than average\\npooling.\\nNote that max pooling and average pooling can be performed along the depth dimen\\nsion rather than the spatial dimensions, although this is not as common. This can\\nallow the CNN to learn to be invariant to various features. For example, it could learn\\nmultiple filters, each detecting a different rotation of the same pattern, such as hand-\\nwritten digits (see Figure 14-10), and the depth-wise max pooling layer would ensure\\nthat the output is the same regardless of the rotation. The CNN could similarly learn\\nto be invariant to anything else: thickness, brightness, skew, color, and so on.\\nFigure 14-10. Depth-wise max pooling can help the CNN learn any invariance\\nPooling Layer | 445',\n",
       "  'Keras does not include a depth-wise max pooling layer, but TensorFlows low-level\\nDeep Learning API does: just use the tf.nn.max_pool() function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool(images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda layer (or create a custom Keras layer):\\ndepth_pool = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling layer. It works very differently: all it does is compute the mean\\nof each entire feature map (its like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D class:\\nglobal_avg_pool = keras.layers.GlobalAvgPool2D()\\nIt is actually equivalent to this simple Lamba layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Lets\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\\nsoftmax layer that outputs estimated class probabilities).\\nFigure 14-11. Typical CNN architecture\\nA common mistake is to use convolution kernels that are too large.\\nFor example, instead of using a convolutional layer with a 5  5\\nkernel, it is generally preferable to stack two layers with 3  3 ker\\nnels: it will use less parameters and require less computations, and\\nit will usually perform better. One exception to this recommenda\\ntion is for the first convolutional layer: it can typically have a large\\nkernel (e.g., 5  5), usually with stride of 2 or more: this will reduce\\nthe spatial dimension of the image without losing too much infor\\nmation, and since the input image only has 3 channels in general, it\\nwill not be too costly.\\nHere is how you can implement a simple CNN to tackle the fashion MNIST dataset\\n(introduced in Chapter 10):\\nfrom functools import partial\\nDefaultConv2D = partial(keras.layers.Conv2D,\\n                        kernel_size=3, activation=\\'relu\\', padding=\"SAME\")\\nmodel = keras.models.Sequential([\\n    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    DefaultConv2D(filters=128),\\n    DefaultConv2D(filters=128),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    DefaultConv2D(filters=256),\\n    DefaultConv2D(filters=256),\\n    keras.layers.MaxPooling2D(pool_size=2),\\n    keras.layers.Flatten(),\\n    keras.layers.Dense(units=128, activation=\\'relu\\'),\\n    keras.layers.Dropout(0.5),\\n    keras.layers.Dense(units=64, activation=\\'relu\\'),\\n    keras.layers.Dropout(0.5),\\n    keras.layers.Dense(units=10, activation=\\'softmax\\'),\\n])\\nCNN Architectures | 447',\n",
       "  ' In this code, we start by using the partial() function to define a thin wrapper\\naround the Conv2D class, called DefaultConv2D: it simply avoids having to repeat\\nthe same hyperparameter values over and over again.\\n The first layer uses a large kernel size, but no stride because the input images are\\nnot very large. It also sets input_shape=[28, 28, 1], which means the images\\nare 28  28 pixels, with a single color channel (i.e., grayscale).\\n Next, we have a max pooling layer, which divides each spatial dimension by a fac\\ntor of two (since pool_size=2).\\n Then we repeat the same structure twice: two convolutional layers followed by a\\nmax pooling layer. For larger images, we could repeat this structure several times\\n(the number of repetitions is a hyperparameter you can tune).\\n Note that the number of filters grows as we climb up the CNN towards the out\\nput layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since\\nthe number of low level features is often fairly low (e.g., small circles, horizontal\\nlines, etc.), but there are many different ways to combine them into higher level\\nfeatures. It is a common practice to double the number of filters after each pool\\ning layer: since a pooling layer divides each spatial dimension by a factor of 2, we\\ncan afford doubling the number of feature maps in the next layer, without fear of\\nexploding the number of parameters, memory usage, or computational load.\\n Next is the fully connected network, composed of 2 hidden dense layers and a\\ndense output layer. Note that we must flatten its inputs, since a dense network\\nexpects a 1D array of features for each instance. We also add two dropout layers,\\nwith a dropout rate of 50% each, to reduce overfitting.\\nThis CNN reaches over 92% accuracy on the test set. Its not the state of the art, but it\\nis pretty good, and clearly much better than what we achieved with dense networks in\\nChapter 10.\\nOver the years, variants of this fundamental architecture have been developed, lead\\ning to amazing advances in the field. A good measure of this progress is the error rate\\nin competitions such as the ILSVRC ImageNet challenge . In this competition the\\ntop-5 error rate for image classification fell from over 26% to less than 2.3% in just six\\nyears. The top-five error rate is the number of test images for which the systems top 5\\npredictions did not include the correct answer. The images are large (256 pixels high)\\nand there are 1,000 classes, some of which are really subtle (try distinguishing 120\\ndog breeds). Looking at the evolution of the winning entries is a good way to under\\nstand how CNNs work.\\nWe will first look at the classical LeNet-5 architecture (1998), then three of the win\\nners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\\n(2015).\\n448 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '10 Gradient-Based Learning Applied to Document Recognition , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).\\nLeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1.\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected  10   RBF\\nF6 Fully Connected  84   tanh\\nC5 Convolution 120 1  1 5  5 1 tanh\\nS4 Avg Pooling 16 5  5 2  2 2 tanh\\nC3 Convolution 16 10  10 5  5 1 tanh\\nS2 Avg Pooling 6 14  14 2  2 2 tanh\\nC1 Convolution 6 28  28 5  5 1 tanh\\nIn Input 1 32  32   \\nThere are a few extra details to be noted:\\n MNIST images are 28  28 pixels, but they are zero-padded to 32  32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper 10 for\\ndetails.\\n The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli\\ndian distance between its input vector and its weight vector. Each output meas\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro\\nducing larger gradients and converging faster.\\nCNN Architectures | 449',\n",
       "  '11 ImageNet Classification with Deep Convolutional Neural Networks,  A. Krizhevsky et al. (2012).\\nY ann LeCuns website (LENET section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet CNN architecture 11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2 presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected  1,000    Softmax\\nF9 Fully Connected  4,096    ReLU\\nF8 Fully Connected  4,096    ReLU\\nC7 Convolution 256 13  13 3  3 1 SAME ReLU\\nC6 Convolution 384 13  13 3  3 1 SAME ReLU\\nC5 Convolution 384 13  13 3  3 1 SAME ReLU\\nS4 Max Pooling 256 13  13 3  3 2 VALID \\nC3 Convolution 256 27  27 5  5 1 SAME ReLU\\nS2 Max Pooling 96 27  27 3  3 2 VALID \\nC1 Convolution 96 55  55 11  11 4 VALID ReLU\\nIn Input 3 (RGB) 227  227    \\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation by ran\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'ideally, given an image from the augmented training set, a human should not be able\\nto tell whether it was augmented or not. Moreover, simply adding white noise will not\\nhelp; the modifications should be learnable (white noise is not).\\nFor example, you can slightly shift, rotate, and resize every picture in the training set\\nby various amounts and add the resulting pictures to the training set (see\\nFigure 14-12). This forces the model to be more tolerant to variations in the position,\\norientation, and size of the objects in the pictures. If you want the model to be more\\ntolerant to different lighting conditions, you can similarly generate many images with\\nvarious contrasts. In general, you can also flip the pictures horizontally (except for\\ntext, and other non-symmetrical objects). By combining these transformations you\\ncan greatly increase the size of your training set.\\nFigure 14-12. Generating new training instances from existing ones\\nAlexNet also uses a competitive normalization step immediately after the ReLU step\\nof layers C1 and C3, called local response normalization. The most strongly activated\\nneurons inhibit other neurons located at the same position in neighboring feature\\nmaps (such competitive activation has been observed in biological neurons). This\\nencourages different feature maps to specialize, pushing them apart and forcing them\\nCNN Architectures | 451',\n",
       "  '12 Going Deeper with Convolutions,  C. Szegedy et al. (2015).\\n13 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.\\nto explore a wider range of features, ultimately improving generalization. Equation\\n14-2 shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi = ai k +  \\nj = jlow\\njhigh\\naj\\n2\\n\\nwith\\njhigh = min i + r\\n2, f n  1\\njlow = max 0, i  r\\n2\\n bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n ai is the activation of that neuron after the ReLU step, but before normalization.\\n k, , , and r are hyperparameters. k is called the bias, and r is called the depth\\nradius.\\n fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2,  = 0.00002,  = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion() function (which you can wrap in a Lambda layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14). This was made possible by\\nsub-networks called inception modules,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'much more efficiently than previous architectures: GoogLeNet actually has 10 times\\nfewer parameters than AlexNet (roughly 6 million instead of 60 million).\\nFigure 14-13 shows the architecture of an inception module. The notation 3  3 +\\n1(S) means that the layer uses a 3  3 kernel, stride 1, and SAME padding. The input\\nsignal is first copied and fed to four different layers. All convolutional layers use the\\nReLU activation function. Note that the second set of convolutional layers uses differ\\nent kernel sizes (1  1, 3  3, and 5  5), allowing them to capture patterns at different\\nscales. Also note that every single layer uses a stride of 1 and SAME padding (even\\nthe max pooling layer), so their outputs all have the same height and width as their\\ninputs. This makes it possible to concatenate all the outputs along the depth dimen\\nsion in the final depth concat layer (i.e., stack the feature maps from all four top con\\nvolutional layers). This concatenation layer can be implemented in TensorFlow using\\nthe tf.concat() operation, with axis=3 (axis 3 is the depth).\\nFigure 14-13. Inception module\\nY ou may wonder why inception modules have convolutional layers with 1  1 ker\\nnels. Surely these layers cannot capture any features since they look at only one pixel\\nat a time? In fact, these layers serve three purposes:\\n First, although they cannot capture spatial patterns, they can capture patterns\\nalong the depth dimension.\\n Second, they are configured to output fewer feature maps than their inputs, so\\nthey serve as bottleneck layers, meaning they reduce dimensionality. This cuts the\\ncomputational cost and the number of parameters, speeding up training and\\nimproving generalization.\\n Lastly, each pair of convolutional layers ([1  1, 3  3] and [1  1, 5  5]) acts like\\na single, powerful convolutional layer, capable of capturing more complex pat\\nterns. Indeed, instead of sweeping a simple linear classifier across the image (as a\\nCNN Architectures | 453',\n",
       "  'single convolutional layer does), this pair of convolutional layers sweeps a two-\\nlayer neural network across the image.\\nIn short, you can think of the whole inception module as a convolutional layer on\\nsteroids, able to output feature maps that capture complex patterns at various scales.\\nThe number of convolutional kernels for each convolutional layer\\nis a hyperparameter. Unfortunately, this means that you have six\\nmore hyperparameters to tweak for every inception layer you add.\\nNow lets look at the architecture of the GoogLeNet CNN (see Figure 14-14 ). The\\nnumber of feature maps output by each convolutional layer and each pooling layer is\\nshown before the kernel size. The architecture is so deep that it has to be represented\\nin three columns, but GoogLeNet is actually one tall stack, including nine inception\\nmodules (the boxes with the spinning tops). The six numbers in the inception mod\\nules represent the number of feature maps output by each convolutional layer in the\\nmodule (in the same order as in Figure 14-13). Note that all the convolutional layers\\nuse the ReLU activation function.\\n454 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'Figure 14-14. GoogLeNet architecture\\nLets go through this network:\\n The first two layers divide the images height and width by 4 (so its area is divided\\nby 16), to reduce the computational load. The first layer uses a large kernel size,\\nso that much of the information is still preserved.\\n Then the local response normalization layer ensures that the previous layers learn\\na wide variety of features (as discussed earlier).\\n Two convolutional layers follow, where the first acts like a bottleneck layer. As\\nexplained earlier, you can think of this pair as a single smarter convolutional\\nlayer.\\n Again, a local response normalization layer ensures that the previous layers cap\\nture a wide variety of patterns.\\nCNN Architectures | 455',\n",
       "  '14 Very Deep Convolutional Networks for Large-Scale Image Recognition,  K. Simonyan and A. Zisserman\\n(2015).\\n Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224  224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7  7. More\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net\\nwork and limits the risk of overfitting.\\n The last layers are self-explanatory: dropout for regularization, then a fully con\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was VGGNet14, developed by K. Simon\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net\\nwork with 2 hidden layers and the output layer. It used only 3  3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '15 Deep Residual Learning for Image Recognition,  K. He (2015).\\nResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network (or ResNet), devel\\noped by Kaiming He et al., 15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections (also called\\nshortcut connections): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Lets see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x)  x rather than h(x). This is\\ncalled residual learning (see Figure 14-15).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net\\nwork just outputs values close to zero. If you add a skip connection, the resulting net\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units, where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457',\n",
       "  'Figure 14-16. Regular deep neural network (left) and deep residual network (right)\\nNow lets look at ResNets architecture (see Figure 14-17 ). It is actually surprisingly\\nsimple. It starts and ends exactly like GoogLeNet (except without a dropout layer),\\nand in between is just a very deep stack of simple residual units. Each residual unit is\\ncomposed of two convolutional layers (and no pooling layer!), with Batch Normaliza\\ntion (BN) and ReLU activation, using 3  3 kernels and preserving spatial dimensions\\n(stride 1, SAME padding).\\nFigure 14-17. ResNet architecture\\nNote that the number of feature maps is doubled every few residual units, at the same\\ntime as their height and width are halved (using a convolutional layer with stride 2).\\nWhen this happens the inputs cannot be added directly to the outputs of the residual\\nunit since they dont have the same shape (for example, this problem affects the skip\\n458 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '16 Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,  C. Szegedy et al.\\n(2016).\\n17 Xception: Deep Learning with Depthwise Separable Convolutions,  Franois Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17). To solve this problem,\\nthe inputs are passed through a 1  1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3  3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1  1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3  3 layer\\nwith 64 feature maps, and finally another 1  1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogles Inception-v416 architecture merged the ideas of GoogLe\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception) was proposed in 2016 by Franois Chollet (the\\nCNN Architectures | 459',\n",
       "  '18 This name can sometimes be ambiguous, since spatially separable convolutions are often called separable\\nconvolutions as well.\\nauthor of Keras), and it significantly outperformed Inception-v3 on a huge vision task\\n(350 million images and 17,000 classes). Just like Inception-v4, it also merges the\\nideas of GoogLeNet and ResNet, but it replaces the inception modules with a special\\ntype of layer called a depthwise separable convolution  (or separable convolution for\\nshort18). These layers had been used before in some CNN architectures, but they were\\nnot as central as in the Xception architecture. While a regular convolutional layer\\nuses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\\nchannel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\\nmakes the strong assumption that spatial patterns and cross-channel patterns can be\\nmodeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\\napplies a single spatial filter for each input feature map, then the second part looks\\nexclusively for cross-channel patternsit is just a regular convolutional layer with 1 \\n1 filters.\\nFigure 14-19. Depthwise Separable Convolutional Layer\\nSince separable convolutional layers only have one spatial filter per input channel,\\nyou should avoid using them after layers that have too few channels, such as the input\\nlayer (granted, thats what Figure 14-19 represents, but it is just for illustration pur\\nposes). For this reason, the Xception architecture starts with 2 regular convolutional\\nlayers, but then the rest of the architecture uses only separable convolutions (34 in\\n460 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '19 Crafting GBD-Net for Object Detection,  X. Zeng et al. (2016).\\n20 Squeeze-and-Excitation Networks,  Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1  1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni\\nversity of Hong Kong. They used an ensemble of many different techniques, includ\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver\\nsions of inception networks and ResNet are called SE-Inception and SE-ResNet respec\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block, to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20.\\nCNN Architectures | 461',\n",
       "  'Figure 14-20. SE-Inception Module (left) and SE-ResNet Unit (right)\\nA SE Block analyzes the output of the unit it is attached to, focusing exclusively on\\nthe depth dimension (it does not look for any spatial pattern), and it learns which fea\\ntures are usually most active together. It then uses this information to recalibrate the\\nfeature maps, as shown in Figure 14-21 . For example, a SE Block may learn that\\nmouths, noses and eyes usually appear together in pictures: if you see a mouth and a\\nnose, you should expect to see eyes as well. So if a SE Block sees a strong activation in\\nthe mouth and nose feature maps, but only mild activation in the eye feature map, it\\nwill boost the eye feature map (more accurately, it will reduce irrelevant feature\\nmaps). If the eyes were somewhat confused with something else, this feature map\\nrecalibration will help resolve the ambiguity.\\n462 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'Figure 14-21. An SE Block Performs Feature Map Recalibration\\nA SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense\\nlayer using the ReLU activation function, and a dense output layer using the sigmoid\\nactivation function (see Figure 14-22):\\nFigure 14-22. SE Block Architecture\\nCNN Architectures | 463',\n",
       "  'As earlier, the global average pooling layer computes the mean activation for each fea\\nture map: for example, if its input contains 256 feature maps, it will output 256 num\\nbers representing the overall level of response for each filter. The next layer is where\\nthe squeeze happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot\\ntleneck step forces the SE Block to learn a general representation of the feature com\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, lets implement a ResNet-34 from scratch using Keras. First, lets\\ncreate a ResidualUnit layer:\\nDefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\\n                        padding=\"SAME\", use_bias=False)\\nclass ResidualUnit(keras.layers.Layer):\\n    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\\n        super().__init__(**kwargs)\\n        self.activation = keras.activations.get(activation)\\n        self.main_layers = [\\n            DefaultConv2D(filters, strides=strides),\\n            keras.layers.BatchNormalization(),\\n            self.activation,\\n            DefaultConv2D(filters),\\n            keras.layers.BatchNormalization()]\\n        self.skip_layers = []\\n        if strides > 1:\\n            self.skip_layers = [\\n                DefaultConv2D(filters, kernel_size=1, strides=strides),\\n                keras.layers.BatchNormalization()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers:\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers:\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'skip_Z = layer(skip_Z)\\n        return self.activation(Z + skip_Z)\\nAs you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call() method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit class):\\nmodel = keras.models.Sequential()\\nmodel.add(DefaultConv2D(64, kernel_size=7, strides=2,\\n                        input_shape=[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Activation(\"relu\"))\\nmodel.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\\nprev_filters = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters else 2\\n    model.add(ResidualUnit(filters, strides=strides))\\n    prev_filters = filters\\nmodel.add(keras.layers.GlobalAvgPool2D())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\\nand finally we update prev_filters.\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you wont have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications package. For example:\\nmodel = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\\nUsing Pretrained Models From Keras | 465',\n",
       "  '21 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\\nWordNet ID.\\nThats all! This will create a ResNet-50 model and download weights pretrained on\\nthe ImageNet dataset. To use it, you first need to ensure that the images have the right\\nsize. A ResNet-50 model expects 224  224 images (other models may expect other\\nsizes, such as 299  299), so lets use TensorFlows tf.image.resize() function to\\nresize the images we loaded earlier:\\nimages_resized = tf.image.resize(images, [224, 224])\\nThe tf.image.resize() will not preserve the aspect ratio. If this is\\na problem, you can try cropping the images to the appropriate\\naspect ratio before resizing. Both operations can be done in one\\nshot with tf.image.crop_and_resize().\\nThe pretrained models assume that the images are preprocessed in a specific way. In\\nsome cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on.\\nEach model provides a preprocess_input() function that you can use to preprocess\\nyour images. These functions assume that the pixel values range from 0 to 255, so we\\nmust multiply them by 255 (since earlier we scaled them to the 01 range):\\ninputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\\nNow we can use the pretrained model to make predictions:\\nY_proba = model.predict(inputs)\\nAs usual, the output Y_proba is a matrix with one row per image and one column per\\nclass (in this case, there are 1,000 classes). If you want to display the top K predic\\ntions, including the class name and the estimated probability of each predicted class,\\nyou can use the decode_predictions() function. For each image, it returns an array\\ncontaining the top K predictions, where each prediction is represented as an array\\ncontaining the class identifier21, its name and the corresponding confidence score:\\ntop_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\\nfor image_index in range(len(images)):\\n    print(\"Image #{}\".format(image_index))\\n    for class_id, name, y_proba in top_K[image_index]:\\n        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\\n    print()\\nThe output looks like this:\\nImage #0\\n  n03877845 - palace       42.87%\\n  n02825657 - bell_cote    40.57%\\n  n03781244 - monastery    14.56%\\n466 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThats pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre\\ntrained model. Other vision models are available in keras.applications, including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis\\ncussed in Chapter 11. For example, lets train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, lets load the dataset using TensorFlow\\nDatasets (see Chapter 13):\\nimport tensorflow_datasets as tfds\\ndataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\\ndataset_size = info.splits[\"train\"].num_examples # 3670\\nclass_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes = info.features[\"label\"].num_classes # 5\\nNote that you can get information about the dataset by setting with_info=True. Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\" dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, lets take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train\\ning:\\ntest_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\\ntest_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\\nvalid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\\ntrain_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\\nPretrained Models for Transfer Learning | 467',\n",
       "  'Next we must preprocess the images. The CNN expects 224  224 images, so we need\\nto resize them. We also need to run the image through Xceptions prepro\\ncess_input() function:\\ndef preprocess(image, label):\\n    resized_image = tf.image.resize(image, [224, 224])\\n    final_image = keras.applications.xception.preprocess_input(resized_image)\\n    return final_image, label\\nLets apply this preprocessing function to all 3 datasets, and lets also shuffle & repeat\\nthe training set, and add batching & prefetching to all datasets:\\nbatch_size = 32\\ntrain_set = train_set.shuffle(1000).repeat()\\ntrain_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\\nvalid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\\ntest_set = test_set.map(preprocess).batch(batch_size).prefetch(1)\\nIf you want to perform some data augmentation, you can just change the preprocess\\ning function for the training set, adding some random transformations to the training\\nimages. For example, use tf.image.random_crop() to randomly crop the images, use\\ntf.image.random_flip_left_right() to randomly flip the images horizontally, and\\nso on (see the notebook for an example).\\nNext lets load an Xception model, pretrained on ImageNet. We exclude the top of the\\nnetwork (by setting include_top=False): this excludes the global average pooling\\nlayer and the dense output layer. We then add our own global average pooling layer,\\nbased on the output of the base model, followed by a dense output layer with 1 unit\\nper class, using the softmax activation function. Finally, we create the Keras Model:\\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\",\\n                                                  include_top=False)\\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\\noutput = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\\nmodel = keras.models.Model(inputs=base_model.input, outputs=output)\\nAs explained in Chapter 11, its usually a good idea to freeze the weights of the pre\\ntrained layers, at least at the beginning of training:\\nfor layer in base_model.layers:\\n    layer.trainable = False\\nSince our model uses the base models layers directly, rather than\\nthe base_model object itself, setting base_model.trainable=False\\nwould have no effect.\\nFinally, we can compile the model and start training:\\n468 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\\n              metrics=[\"accuracy\"])\\nhistory = model.fit(train_set,\\n                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\\n                    validation_data=valid_set,\\n                    validation_steps=int(0.15 * dataset_size / batch_size),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapters notebook in Colab, using a GPU run\\ntime (its free!). See the instructions at https://github.com/ageron/\\nhandson-ml2.\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (dont forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam\\naging the pretrained weights:\\nfor layer in base_model.layers:\\n    layer.trainable = True\\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But theres more to com\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Lets look at this now.\\nClassification and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10: to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the objects center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\",\\n                                                  include_top=False)\\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\\nclass_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\\nClassification and Localization | 469',\n",
       "  '22 Crowdsourcing in Computer Vision,  A. Kovashka et al. (2016).\\nloc_output = keras.layers.Dense(4)(avg)\\nmodel = keras.models.Model(inputs=base_model.input,\\n                           outputs=[class_output, loc_output])\\nmodel.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\\n              loss_weights=[0.8, 0.2], # depends on what you care most about\\n              optimizer=optimizer, metrics=[\"accuracy\"])\\nBut now we have a problem: the flowers dataset does not have bounding boxes\\naround the flowers. So we need to add them ourselves. This is often one of the hard\\nest and most costly part of a Machine Learning project: getting the labels. Its a good\\nidea to spend time looking for the right tools. To annotate images with bounding\\nboxes, you may want to use an open source image labeling tool like VGG Image\\nAnnotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like\\nLabelBox or Supervisely. Y ou may also want to consider crowdsourcing platforms\\nsuch as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\\nimages to annotate. However, it is quite a lot of work to setup a crowdsourcing plat\\nform, prepare the form to be sent to the workers, to supervise them and ensure the\\nquality of the bounding boxes they produce is good, so make sure it is worth the\\neffort: if there are just a few thousand images to label, and you dont plan to do this\\nfrequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very\\npractical paper22 about crowdsourcing in Computer Vision, I recommend you check\\nit out, even if you do not plan to use crowdsourcing.\\nSo lets suppose you obtained the bounding boxes for every image in the flowers data\\nset (for now we will assume there is a single bounding box per image), you then need\\nto create a dataset whose items will be batches of preprocessed images along with\\ntheir class labels and their bounding boxes. Each item should be a tuple of the form:\\n(images, (class_labels, bounding_boxes)) . Then you are ready to train your\\nmodel!\\nThe bounding boxes should be normalized so that the horizontal\\nand vertical coordinates, as well as the height and width all range\\nfrom 0 to 1. Also, it is common to predict the square root of the\\nheight and width rather than the height and width directly: this\\nway, a 10 pixel error for a large bounding box will not be penalized\\nas much as a 10 pixel error for a small bounding box.\\nThe MSE often works fairly well as a cost function to train the model, but it is not a\\ngreat metric to evaluate how well the model can predict bounding boxes. The most\\ncommon metric for this is the Intersection over Union (IoU): it is the area of overlap\\nbetween the predicted bounding box and the target bounding box, divided by the\\n470 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection. Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24. In this example, the image was chopped into a 6  8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3  3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3  3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ\\nent sizes. For example, once you are done with the 3  3 regions, you might want to\\nslide the CNN across all 4  4 regions as well.\\nObject Detection | 471',\n",
       "  'Figure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image\\nThis technique is fairly straightforward, but as you can see it will detect the same\\nobject multiple times, at slightly different positions. Some post-processing will then\\nbe needed to get rid of all the unnecessary bounding boxes. A common approach for\\nthis is called non-max suppression:\\n First, you need to add an extra objectness output to your CNN, to estimate the\\nprobability that a flower is indeed present in the image (alternatively, you could\\nadd a no-flower class, but this usually does not work as well). It must use the\\nsigmoid activation function and you can train it using the \"binary_crossen\\ntropy\" loss. Then just get rid of all the bounding boxes for which the objectness\\nscore is below some threshold: this will drop all the bounding boxes that dont\\nactually contain a flower.\\n Second, find the bounding box with the highest objectness score, and get rid of\\nall the other bounding boxes that overlap a lot with it (e.g., with an IoU greater\\nthan 60%). For example, in Figure 14-24, the bounding box with the max object\\nness score is the thick bounding box over the top-most rose (the objectness score\\nis represented by the thickness of the bounding boxes). The other bounding box\\nover that same rose overlaps a lot with the max bounding box, so we will get rid\\nof it.\\n472 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '23 Fully Convolutional Networks for Semantic Segmentation,  J. Long, E. Shelhamer, T. Darrell (2015).\\n24 There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network.\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, lets look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7  7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100  7  7 activa\\ntions from the convolutional layer (plus a bias term). Now lets see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7  7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1  1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ\\nence is that the dense layers output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473',\n",
       "  '25 This assumes we used only SAME padding in the network: indeed, V ALID padding would reduce the size of\\nthe feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round\\ning error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\\nfeature maps may end up being smaller.\\nFor example, suppose we already trained a CNN for flower classification and localiza\\ntion. It was trained on 224  224 images and it outputs 10 numbers: outputs 0 to 4 are\\nsent through the softmax activation function, and this gives the class probabilities\\n(one per class); output 5 is sent through the logistic activation function, and this gives\\nthe objectness score; outputs 6 to 9 do not use any activation function, and they rep\\nresent the bounding boxs center coordinates, and its height and width. We can now\\nconvert its dense layers to convolutional layers. In fact, we dont even need to retrain\\nit, we can just copy the weights from the dense layers to the convolutional layers!\\nAlternatively, we could have converted the CNN into an FCN before training.\\nNow suppose the last convolutional layer before the output layer (also called the bot\\ntleneck layer) outputs 7  7 feature maps when the network is fed a 224  224 image\\n(see the left side of Figure 14-25). If we feed the FCN a 448  448 image (see the right\\nside of Figure 14-25 ), the bottleneck layer will now output 14  14 feature maps. 25\\nSince the dense output layer was replaced by a convolutional layer using 10 filters of\\nsize 7  7, V ALID padding and stride 1, the output will be composed of 10 features\\nmaps, each of size 8  8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\\nthe whole image only once and it will output an 8  8 grid where each cell contains 10\\nnumbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates).\\nIts exactly like taking the original CNN and sliding it across the image using 8 steps\\nper row and 8 steps per column: to visualize this, imagine chopping the original\\nimage into a 14  14 grid, then sliding a 7  7 window across this grid: there will be 8\\n 8 = 64 possible locations for the window, hence 8  8 predictions. However, the\\nFCN approach is much more efficient, since the network only looks at the image\\nonce. In fact, You Only Look Once (YOLO) is the name of a very popular object detec\\ntion architecture!\\n474 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '26 Y ou Only Look Once: Unified, Real-Time Object Detection,  J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27 YOLO9000: Better, Faster, Stronger,  J. Redmon, A. Farhadi (2016).\\n28 YOLOv3: An Incremental Improvement,  J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left) and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper 26, and subsequently improved in 2016 27\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo).\\nYOLOv3s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475',\n",
       "  ' First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\\nbounding box comes with an objectness score. It also outputs 20 class probabili\\nties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\\n20 classes. Thats a total of 45 numbers per grid cell (5 * 4 bounding box coordi\\nnates, plus 5 objectness scores, plus 20 class probabilities).\\n Second, instead of predicting the absolute coordinates of the bounding box cen\\nters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where\\n(0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each\\ngrid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in\\nthat cell (but the bounding box itself generally extends well beyond the grid cell).\\nYOLOv3 applies the logistic activation function to the bounding box coordinates\\nto ensure they remain in the 0 to 1 range.\\n Third, before training the neural net, YOLOv3 finds 5 representative bounding\\nbox dimensions, called anchor boxes  (or bounding box priors ): it does this by\\napplying the K-Means algorithm (see ???) to the height and width of the training\\nset bounding boxes. For example, if the training images contain many pedes\\ntrians, then one of the anchor boxes will likely have the dimensions of a typical\\npedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\\nactually predicts how much to rescale each of the anchor boxes. For example,\\nsuppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\\npredicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\\none of the grid cells), this will result in a predicted bounding box of size 150  45\\npixels. To be more precise, for each grid cell and each anchor box, the network\\npredicts the log of the vertical and horizontal rescaling factors. Having these pri\\nors makes the network more likely to predict bounding boxes of the appropriate\\ndimensions, and it also speeds up training since it will more quickly learn what\\nreasonable bounding boxes look like.\\n Fourth, the network is trained using images of different scales: every few batches\\nduring training, the network randomly chooses a new image dimension (from\\n330  330 to 608  608 pixels). This allows the network to learn to detect objects\\nat different scales. Moreover, it makes it possible to use YOLOv3 at different\\nscales: the smaller scale will be less accurate but faster than the larger scale, so\\nyou can choose the right tradeoff for your use case.\\nThere are a few more innovations you might be interested in, such as the use of skip\\nconnections to recover some of the spatial resolution that is lost in the CNN (we will\\ndiscuss this shortly when we look at semantic segmentation). Moreover, in the 2016\\npaper, the authors introduce the YOLO9000 model that uses hierarchical classifica\\ntion: the model predicts a probability for each node in a visual hierarchy called Word\\nTree. This makes it possible for the network to predict with high confidence that an\\nimage represents, say, a dog, even though it is unsure what specific type of dog it is.\\n476 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'So I encourage you to go ahead and read all three papers: they are quite pleasant to\\nread, and it is an excellent example of how Deep Learning systems can be incremen\\ntally improved.\\nMean Average Precision (mAP)\\nA very common metric used in object detection tasks is the mean Average Precision\\n(mAP). Mean Average sounds a bit redundant, doesnt it? To understand this met\\nric, lets go back to two classification metrics we discussed in Chapter 3: precision and\\nrecall. Remember the tradeoff: the higher the recall, the lower the precision. Y ou can\\nvisualize this in a Precision/Recall curve (see Figure 3-5 ). To summarize this curve\\ninto a single number, we could compute its Area Under the Curve (AUC). But note\\nthat the Precision/Recall curve may contain a few sections where precision actually\\ngoes up when recall increases, especially at low recall values (you can see this at the\\ntop left of Figure 3-5). This is one of the motivations for the mAP metric.\\nSuppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20%\\nrecall: theres really no tradeoff here: it simply makes more sense to use the classifier\\nat 20% recall rather than at 10% recall, as you will get both higher recall and higher\\nprecision. So instead of looking at the precision at 10% recall, we should really be\\nlooking at the maximum precision that the classifier can offer with at least 10% recall.\\nIt would be 96%, not 90%. So one way to get a fair idea of the models performance is\\nto compute the maximum precision you can get with at least 0% recall, then 10%\\nrecall, 20%, and so on up to 100%, and then calculate the mean of these maximum\\nprecisions. This is called the Average Precision (AP) metric. Now when there are more\\nthan 2 classes, we can compute the AP for each class, and then compute the mean AP\\n(mAP). Thats it!\\nHowever, in an object detection systems, there is an additional level of complexity:\\nwhat if the system detected the correct class, but at the wrong location (i.e., the\\nbounding box is completely off)? Surely we should not count this as a positive predic\\ntion. So one approach is to define an IOU threshold: for example, we may consider\\nthat a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted\\nclass is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\\nor sometimes just AP 50). In some competitions (such as the Pascal VOC challenge),\\nthis is what is done. In others (such as the COCO competition), the mAP is computed\\nfor different IOU thresholds (0.50, 0.55, 0.60, , 0.95), and the final metric is the\\nmean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Y es, thats a mean\\nmean average.\\nSeveral YOLO implementations built using TensorFlow are available on github, some\\nwith pretrained weights. At the time of writing, they are based on TensorFlow 1, but\\nby the time you read this, TF 2 implementations will certainly be available. Moreover,\\nother object detection models are available in the TensorFlow Models project, many\\nObject Detection | 477',\n",
       "  '29 SSD: Single Shot MultiBox Detector,  Wei Liu et al. (2015).\\n30 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,  Shaoqing Ren et al.\\n(2015).\\nwith pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN.30, which are both quite popu\\nlar. SSD is also a single shot detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met\\nrics, but there is quite a lot of variability in the testing environments, and the technol\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per\\nhaps you might want to be a bit more precise. Lets see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation, each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note\\nthat different objects of the same class are not distinguished. For example, all the bicy\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that theres a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '31 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati\\ncians call a deconvolution, so this name should be avoided.\\nFigure 14-26. Semantic segmentation\\nJust like for object detection, there are many different approaches to tackle this prob\\nlem, some quite complex. However, a fairly simple solution was proposed in the 2015\\npaper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained\\nCNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to\\nthe input image overall (i.e., if you add up all the strides greater than 1), meaning the\\nlast layer outputs feature maps that are 32 times smaller than the input image. This is\\nclearly too coarse, so they add a single upsampling layer that multiplies the resolution\\nby 32. There are several solutions available for upsampling (increasing the size of an\\nimage), such as bilinear interpolation, but it only works reasonably well up to 4 or\\n8. Instead, they used a transposed convolutional layer :31 it is equivalent to first\\nstretching the image by inserting empty rows and columns (full of zeros), then per\\nforming a regular convolution (see Figure 14-27). Alternatively, some people prefer to\\nthink of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in\\nFigure 14-27). The transposed convolutional layer can be initialized to perform some\\nthing close to linear interpolation, but since it is a trainable layer, it will learn to do\\nbetter during training.\\nSemantic Segmentation | 479',\n",
       "  'Figure 14-27. Upsampling Using a Transpose Convolutional Layer\\nIn a transposed convolution layer, the stride defines how much the\\ninput will be stretched, not the size of the filter steps, so the larger\\nthe stride, the larger the output (unlike for convolutional layers or\\npooling layers).\\nTensorFlow Convolution Operations\\nTensorFlow also offers a few other kinds of convolutional layers:\\n keras.layers.Conv1D creates a convolutional layer for 1D inputs, such as time\\nseries or text (sequences of letters or words), as we will see in ???.\\n keras.layers.Conv3D creates a convolutional layer for 3D inputs, such as 3D\\nPET scan.\\n Setting the dilation_rate hyperparameter of any convolutional layer to a value\\nof 2 or more creates an -trous convolutional layer ( trous is French for with\\nholes). This is equivalent to using a regular convolutional layer with a filter dila\\nted by inserting rows and columns of zeros (i.e., holes). For example, a 1  3 filter\\nequal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\\nfilter [[1, 0, 0, 0, 2, 0, 0, 0, 3]] . This allows the convolutional layer to\\n480 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  'have a larger receptive field at no computational price and using no extra param\\neters.\\n tf.nn.depthwise_conv2d() can be used to create a depthwise convolutional layer\\n(but you need to create the variables yourself). It applies every filter to every\\nindividual input channel independently. Thus, if there are fn filters and fn input\\nchannels, then this will output fn  fn feature maps.\\nThis solution is okay, but still too imprecise. To do better, the authors added skip con\\nnections from lower layers: for example, they upsampled the output image by a factor\\nof 2 (instead of 32), and they added the output of a lower layer that had this double\\nresolution. Then they upsampled the result by a factor of 16, leading to a total upsam\\npling factor of 32 (see Figure 14-28 ). This recovered some of the spatial resolution\\nthat was lost in earlier pooling layers. In their best architecture, they used a second\\nsimilar skip connection to recover even finer details from an even lower layer: in\\nshort, the output of the original CNN goes through the following extra steps: upscale\\n2, add the output of a lower layer (of the appropriate scale), upscale 2, add the out\\nput of an even lower layer, and finally upscale 8. It is even possible to scale up\\nbeyond the size of the original image: this can be used to increase the resolution of an\\nimage, which is a technique called super-resolution.\\nFigure 14-28. Skip layers recover some spatial resolution from lower layers\\nOnce again, many github repositories provide TensorFlow implementations of\\nsemantic segmentation (TensorFlow 1 for now), and you will even find a pretrained\\ninstance segmentation model in the TensorFlow Models project. Instance segmenta\\ntion is similar to semantic segmentation, but instead of merging all objects of the\\nsame class into one big lump, each object is distinguished from the others (e.g., it\\nidentifies each individual bicycle). At the present, they provide multiple implementa\\ntions of the Mask R-CNN  architecture, which was proposed in a 2017 paper : it\\nextends the Faster R-CNN model by additionally producing a pixel-mask for each\\nbounding box. So not only do you get a bounding box around each object, with a set\\nof estimated class probabilities, you also get a pixel mask that locates pixels in the\\nbounding box that belong to the object.\\nSemantic Segmentation | 481',\n",
       "  '32 Matrix Capsules with EM Routing,  G. Hinton, S. Sabour, N. Frosst (2018).\\nAs you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning (which\\nattempts to make the network more resistant to images designed to fool it), explaina\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration (which we will come back to in ???), single-shot learning (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hintons capsule networks32 (I pre\\nsented them in a couple videos, with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1. What are the advantages of a CNN over a fully connected DNN for image classi\\nfication?\\n2. Consider a CNN composed of three convolutional layers, each with 3  3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200  300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3. If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4. Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5. When would you want to add a local response normalization layer?\\n6. Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7. What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8. What is the main technical difficulty of semantic segmentation?\\n9. Build your own CNN from scratch and try to achieve the highest possible accu\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks',\n",
       "  '10. Use transfer learning for large image classification.\\na. Create a training set containing at least 100 images per class. For example, you\\ncould classify your own pictures based on the location (beach, mountain, city,\\netc.), or alternatively you can just use an existing dataset (e.g., from Tensor\\nFlow Datasets).\\nb. Split it into a training set, a validation set and a test set.\\nc. Build the input pipeline, including the appropriate preprocessing operations,\\nand optionally add data augmentation.\\nd. Fine-tune a pretrained model on this dataset.\\n11. Go through TensorFlows DeepDream tutorial. It is a fun way to familiarize your\\nself with various ways of visualizing the patterns learned by a CNN, and to gener\\nate art using Deep Learning.\\nSolutions to these exercises are available in ???.\\nExercises | 483',\n",
       "  'About the Author\\nAurlien Gron is a Machine Learning consultant. A former Googler, he led the Y ou\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSocit Gnrale), defense (Canadas DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft\\nware engineering, and his parachute didnt open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten\\nsorFlow is the fire salamander ( Salamandra salamandra), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibians common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamanders skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamanders numbers have been reduced by destruction of their forest habi\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on OReilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Woods Illustrated Natural History. The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maags Ubuntu Mono.'],\n",
       " 'data': None,\n",
       " 'uris': None,\n",
       " 'included': ['embeddings', 'documents', 'metadatas']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and check the content of the collection\n",
    "collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      " Whether or not they are trained with human supervision (supervised, unsuper\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      " Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      " Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Lets look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning, the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels (Figure 1-5).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 33, 'page_label': '8', 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "results. To reduce this risk, you need to monitor your system closely and promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop in performance. Y ou may also want to monitor the input data and react to\n",
      "abnormal data (e.g., using an anomaly detection algorithm).\n",
      "Instance-Based Versus Model-Based Learning\n",
      "One more way to categorize Machine Learning systems is by how they generalize.\n",
      "Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has never seen before. Having a good performance measure on the training data is\n",
      "good, but insufficient; the true goal is to perform well on new instances.\n",
      "There are two main approaches to generalization: instance-based learning and\n",
      "model-based learning.\n",
      "Instance-based learning\n",
      "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by usersnot the worst solution, but certainly not the\n",
      "best.\n",
      "Instead of just flagging emails that are identical to known spam emails, your spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity between two emails. A (very basic) simi\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com\n",
      "mon with a known spam email.\n",
      "This is called instance-based learning: the system learns the examples by heart, then\n",
      "generalizes to new cases by comparing them to the learned examples (or a subset of\n",
      "them), using a similarity measure. For example, in Figure 1-15  the new instance\n",
      "would be classified as a triangle because the majority of the most similar instances\n",
      "belong to that class.\n",
      "18 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 43, 'page_label': '18', 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "Figure 1-12. Reinforcement Learning\n",
      "For example, many robots implement Reinforcement Learning algorithms to learn\n",
      "how to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie\n",
      "at the game of Go. It learned its winning policy by analyzing millions of games, and\n",
      "then playing many games against itself. Note that learning was turned off during the\n",
      "games against the champion; AlphaGo was just applying the policy it had learned.\n",
      "Batch and Online Learning\n",
      "Another criterion used to classify Machine Learning systems is whether or not the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "Batch learning\n",
      "In batch learning, the system is incapable of learning incrementally: it must be trained\n",
      "using all the available data. This will generally take a lot of time and computing\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline learning.\n",
      "If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one.\n",
      "Fortunately, the whole process of training, evaluating, and launching a Machine\n",
      "Learning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\n",
      "Types of Machine Learning Systems | 15\n",
      "{'page': 40, 'page_label': '15', 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\n",
    "    search_query,\n",
    "    k=3\n",
    ")\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
