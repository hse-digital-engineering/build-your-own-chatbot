{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load embedding model into ollama:\n",
    "    1. Attach ollama container shell\n",
    "    2. Run command in container `ollama pull pull bge-m3`\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Ollama Embeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama/)\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 1024\n",
      "[-0.050017744, 0.021129876, -0.057069883, -0.0055166543, -0.0062899087, -0.02841217, 0.045161195, 0.042706154, -0.00025403852, 0.013057328]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "query_vector = embedding_model.embed_query(text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: ``PyPDFLoader``\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "loader = PyPDFLoader(pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "pages_chunked = loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter())\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "pages_chunked_cleaned = [clean_text(chunk.page_content) for chunk in pages_chunked]\n",
    "\n",
    "print(pages_chunked[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "collection = client.get_or_create_collection(\"ai_model_book\")\n",
    "\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"ai_model_book\",\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a5cc1801-ab92-4590-ae4a-edfa96466a2f',\n",
       " '3a21ae2e-78fe-4fb4-b0eb-a7b26f2f90fa',\n",
       " '84a6801f-b0dd-4069-b286-fed2d577f477',\n",
       " '576247b4-240c-469a-baab-d48c1a491e03',\n",
       " '23928335-1806-4f00-83a6-e07e5d872b45',\n",
       " 'f317bac0-94b0-480c-8736-5e23d825083c',\n",
       " 'c94e8651-2fb6-4d9f-ba01-2f5d6a0e7be5',\n",
       " '05998f8b-d70a-422c-a209-7c810e14610b',\n",
       " '525b05fb-90eb-4d9c-82cc-774e9e140050',\n",
       " 'c04e9b28-20a8-4f67-95d8-a6569d591901',\n",
       " 'd04d32af-b13a-4ba5-9925-5f13b5a0e882',\n",
       " '51af3226-99c7-4105-89a9-8884a239983a',\n",
       " '892a433a-1930-4777-8bd3-0a398addd88b',\n",
       " '1b466481-7c35-4ae0-8007-5882496bc58b',\n",
       " '656f53e4-80bc-4559-8f66-b3bbfe6d2017',\n",
       " 'd5ee9d93-1c0b-4566-a3d1-8c6ab9b6a3ec',\n",
       " 'f8fbc816-6122-4923-9b63-d2d34179f097',\n",
       " 'ca52d975-7015-4e57-b678-7d70a7dd7bbc',\n",
       " '98c5a999-be9d-4d3b-a610-d969741505d1',\n",
       " 'b7529d5a-533a-40ac-b5da-ae7cf249e7b2',\n",
       " 'e9fc90f6-2d4e-45b5-ac8d-f0af513580ed',\n",
       " 'e8ca3566-dac5-4a6c-a04f-068d0cd05096',\n",
       " 'ac19eeb0-b04e-4aa2-9c82-813a645eb878',\n",
       " '60bfaa36-ac45-48ee-b721-5fa39dc7527a',\n",
       " 'f0b84105-8892-4389-b1a5-4f5379285136',\n",
       " '4a101d3c-3777-4e75-81bd-4a9d47a02d8a',\n",
       " '6a4c7c23-a047-4ff7-b764-61b06bb94602',\n",
       " 'd8a34546-af8f-4e69-a750-aafe0e6de23b',\n",
       " 'f8da1623-7550-4cd5-b687-26c633e594be',\n",
       " '5bc916af-6cf4-4a6e-9c72-c12739325cff',\n",
       " 'f26ce747-57c4-4170-975f-40efaa83f48f',\n",
       " '7c3cde47-f66b-47b5-adca-0c0363ecc273',\n",
       " '62dd5c02-6a74-4bd5-b398-7b6d8ed208c1',\n",
       " '7bb4bea4-9521-46b1-b6c5-7cb28d046289',\n",
       " 'd5677f68-e947-495d-925c-b3eb82343f26',\n",
       " '4ccae628-ee2c-4208-adb7-7c5c4b2cff76',\n",
       " 'bfe2928b-f046-4231-991e-73f8b120904a',\n",
       " 'ca365a26-da0b-4a53-8455-541e70bbd3cb',\n",
       " '5ba5c37a-81ff-45c3-89a6-426d7f0919dc',\n",
       " '8bd89227-4466-4828-822b-785066a567aa',\n",
       " '728fca61-3036-4876-a60c-092e324ff3d9',\n",
       " '31794947-07ec-42f7-affe-7931c30b2a17',\n",
       " '6ac046c2-a888-45d2-b166-3d9b5e49a4c3',\n",
       " '05c07a1f-c24f-4f7a-a849-5f589df4b505',\n",
       " '2650e025-1176-47c2-86b6-bfef9dcd70b3',\n",
       " '0c3e1784-6ddd-4168-be6f-2a5cbed9abae',\n",
       " 'd291bf00-6ce4-4578-aebe-259a7717e620',\n",
       " '05ffb05b-7d40-43ca-8309-474e3fb842af',\n",
       " '03a2112f-6cec-428f-864a-9abd8b9bfa7d',\n",
       " 'dea0756e-962b-4fff-975e-7da0db809c49']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked[:50]))]\n",
    "\n",
    "vector_db_from_client.add_documents(documents=pages_chunked[:50], ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "•Whether or not they are trained with human supervision (supervised, unsuper‐\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "•Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "•Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Let’s look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning , the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels  (Figure 1-5 ).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 33, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "results. To reduce this risk, you need to monitor your system closely and promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop in performance. Y ou may also want to monitor the input data and react to\n",
      "abnormal data (e.g., using an anomaly detection algorithm).\n",
      "Instance-Based Versus Model-Based Learning\n",
      "One more way to categorize Machine Learning systems is by how they generalize .\n",
      "Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has never seen before. Having a good performance measure on the training data is\n",
      "good, but insufficient; the true goal is to perform well on new instances.\n",
      "There are two main approaches to generalization: instance-based learning and\n",
      "model-based learning.\n",
      "Instance-based learning\n",
      "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by users—not the worst solution, but certainly not the\n",
      "best.\n",
      "Instead of just flagging emails that are identical to known spam emails, your spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity  between two emails. A (very basic) simi‐\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com‐\n",
      "mon with a known spam email.\n",
      "This is called instance-based learning : the system learns the examples by heart, then\n",
      "generalizes to new cases by comparing them to the learned examples (or a subset of\n",
      "them), using a similarity measure. For example, in Figure 1-15  the new instance\n",
      "would be classified as a triangle because the majority of the most similar instances\n",
      "belong to that class.\n",
      "18 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 43, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "PART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "{'page': 26, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\n",
    "    search_query,\n",
    "    k=3\n",
    ")\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
