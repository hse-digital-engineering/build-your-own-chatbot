{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that can create new, original content such as images, videos, music, and text based on patterns learned from large datasets, allowing it to generate novel outputs that are often indistinguishable from human creations. This technology enables machines to learn and replicate the characteristics of human creativity, enabling applications in areas like art, design, writing, and even music composition."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that can create new, original content such as images, music, text, or videos based on patterns and structures learned from existing data, allowing it to generate unique outputs that may be indistinguishable from human-created work. This technology uses complex algorithms and machine learning techniques to learn from large datasets and produce novel, creative content that can range from artistic expressions to practical applications in fields like design, marketing, and entertainment.\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "response = ollama.chat(model=model, messages=[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Describe Generative AI in two sentences.\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Artificial Intelligence (AI) refers to a subset of machine learning algorithms that can create new, original content based on patterns and structures learned from existing data. This type of AI is capable of generating text, images, music, videos, and even entire websites, making it a powerful tool for various applications such as art, design, marketing, and more. Generative AI models use complex neural networks to identify relationships between different data points and then generate new content that is similar in style and structure. The technology has become increasingly sophisticated over the years, allowing for the creation of highly realistic and innovative content. However, generative AI also raises concerns about ownership, authenticity, and the potential impact on human creativity and jobs."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe Generative AI in 5 sentences.\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Positive\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'I hate the chair.'\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Negative\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: 'I hate the chair!'\n",
      "Sentiment: Negative\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'Who would use this product?'\n",
      "Sentiment:\n",
      "\n",
      "\n",
      "Model Output:\n",
      "Review: Who would use this product?\n",
      "\n",
      "Sentiment: Neutral\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompts\n",
    "zero_shot_prompt = \"Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'I hate the chair.'\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: 'I hate the chair!'\n",
    "Sentiment: Negative\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'Who would use this product?'\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "The paragraph discusses Generative AI, which focuses on creating new content from patterns found within existing data. This technology has diverse applications across several domains including text, images, and music. As it becomes more prevalent, it's seen as a significant tool for enhancing creative industries.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Summary:\n",
      "\n",
      "The paragraph discusses Generative AI, which involves using algorithms to create new forms of art or content from existing data patterns, focusing specifically on text, images, and music creation. This technology shows promise for various applications across the creative industry, making it a vital field in modern artificial intelligence research.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Refined prompt\n",
    "refined_prompt = \"Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "\n",
      "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
      "\n",
      "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
      "\n",
      "Example 1:\n",
      "Question: What be supervised learnin'?\n",
      "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
      "\n",
      "Example 2:\n",
      "Question: What be overfittin'?\n",
      "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
      "\n",
      "Example 3:\n",
      "Question: What be a neural network?\n",
      "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
      "\n",
      "Explain this:\n",
      "Question: What be unsupervised learnin'?\n",
      "Answer:\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Ahoy matey! Unsupervised learnin' be like a pirate's quest for hidden treasures, where ye only have a map and no clue how to find the treasure. But ye know that when ye sail into new waters, ye'll still see land and plenty of treasure ahead, but ye also know yer map will always guide ye to safety.\n",
      "\n",
      "So in summary, Unsupervised learnin' be like a pirate's quest for hidden treasures where ye only have a map, yet ye'll always find land and wealth. It's like the sailor who knows how to sail, even without a chart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "structured_prompt = \"\"\"\n",
    "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
    "\n",
    "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
    "\n",
    "Example 1:\n",
    "Question: What be supervised learnin'?\n",
    "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
    "\n",
    "Example 2:\n",
    "Question: What be overfittin'?\n",
    "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
    "\n",
    "Example 3:\n",
    "Question: What be a neural network?\n",
    "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
    "\n",
    "Explain this:\n",
    "Question: What be unsupervised learnin'?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
