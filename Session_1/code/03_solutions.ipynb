{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) -> [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)\n",
    "2. User Ollama python library -> [How To](https://pypi.org/project/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a two-sentence description of Generative AI:\n",
      "\n",
      "Generative AI refers to a subset of artificial intelligence that uses algorithms and statistical models to generate new, original content such as images, videos, music, text, or even 3D models that mimic the style of existing works or simulate real-world phenomena. This type of AI can create unique outputs without being explicitly programmed to do so, often by learning from large datasets and using complex mathematical techniques like deep learning and neural networks."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that can generate new, original content such as text, images, audio, or videos based on patterns and structures learned from existing data. This technology uses machine learning algorithms to create novel outputs that are often indistinguishable from those created by humans, with applications in fields like art, music, writing, and even product design.\n"
     ]
    }
   ],
   "source": [
    "# Simple API Call via ollama\n",
    "\n",
    "response = ollama.chat(model=model, messages=[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Describe Generative AI in two sentences.\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a description of Generative AI:\n",
      "\n",
      "Generative AI refers to a type of artificial intelligence that can create new, original content such as images, videos, music, or text based on patterns and structures it has learned from existing data. These models use complex algorithms and machine learning techniques to generate novel outputs that are often indistinguishable from those created by humans. Generative AI has various applications, including art, design, entertainment, and even tasks like language translation and text summarization. Some notable examples of generative AI include the development of AI-generated music and art, such as portraits and landscapes, which have sparked both wonder and controversy in recent years. Overall, Generative AI represents a significant advancement in the field of artificial intelligence and has the potential to transform various industries and aspects of society."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe Generative AI in 5 sentences.\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Sentiment: **Positive**\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'I hate the chair.'\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Based on the review \"I hate the chair.\", I would classify the sentiment as:\n",
      "\n",
      "Sentiment: Negative\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: 'I hate the chair!'\n",
      "Sentiment: Negative\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'Who would use this product?'\n",
      "Sentiment:\n",
      "\n",
      "\n",
      "Model Output:\n",
      "The sentiment of the given review is: Neutral\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompts\n",
    "zero_shot_prompt = \"Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'I hate the chair.'\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: 'I hate the chair!'\n",
    "Sentiment: Negative\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'Who would use this product?'\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Here is a summary of the paragraph:\n",
      "\n",
      "Generative AI creates new content (such as text, images, or music) by learning patterns from existing data.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Here is a concise and informative summary of the paragraph:\n",
      "\n",
      "**Key Points:** \n",
      "\n",
      "* Generative AI creates new content using patterns from existing data\n",
      "* Applications include text, image, and music generation\n",
      "* Increasingly used in creative industries\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Refined prompt\n",
    "refined_prompt = \"Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "\n",
      "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
      "\n",
      "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
      "\n",
      "Example 1:\n",
      "Question: What be supervised learnin'?\n",
      "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
      "\n",
      "Example 2:\n",
      "Question: What be overfittin'?\n",
      "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
      "\n",
      "Example 3:\n",
      "Question: What be a neural network?\n",
      "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
      "\n",
      "Explain this:\n",
      "Question: What be unsupervised learnin'?\n",
      "Answer:\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Arrr, ye landlubber! Unsolved learnin' be when ye have a great big ol' sea o' data, but no map in sight, savvy? Ye don't know what treasure ye're lookin' for, just that it's out there somewhere. Yer model has to swim through the vast ocean o' information, findin' patterns and relationships without a guide.\n",
      "\n",
      "Imagine bein' on a ship with a trusty first mate who can sniff out hidden coves and secret islands, even if ye don't know where they are yerself! That's what unsupervised learnin' be like. The model finds clusters o' similar data, detects anomalies, and uncovers hidden structures in the sea o' information.\n",
      "\n",
      "It be a mighty fine way to discover new insights and understand the underlying patterns in yer data, even if ye don't know exactly what ye're lookin' for, matey!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "structured_prompt = \"\"\"\n",
    "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
    "\n",
    "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
    "\n",
    "Example 1:\n",
    "Question: What be supervised learnin'?\n",
    "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
    "\n",
    "Example 2:\n",
    "Question: What be overfittin'?\n",
    "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
    "\n",
    "Example 3:\n",
    "Question: What be a neural network?\n",
    "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
    "\n",
    "Explain this:\n",
    "Question: What be unsupervised learnin'?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
