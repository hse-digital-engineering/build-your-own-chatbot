{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a type of artificial intelligence that uses complex algorithms and machine learning techniques to generate new, original content, such as images, videos, music, and text, by training on large datasets and producing novel outputs. Unlike traditional AI, generative AI systems can learn from patterns and relationships in the data they are trained on, allowing them to produce high-quality, realistic output that is often indistinguishable from human-generated content."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt as json\n",
    "body_json = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, json=body_json)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a class of machine learning algorithms that can create new, original content, such as text, images, and music, by generating variations of existing patterns and structures, often using large datasets and complex neural network architectures. Unlike traditional AI, which primarily focuses on pattern recognition and classification, generative AI seeks to create novel outputs that are often surprising, coherent, and contextually relevant, enabling it to generate new ideas, styles, and solutions in a wide range of domains.\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "response = ollama.chat(model=model, messages=[\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Describe Generative AI in two sentences.\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a type of artificial intelligence that can create original content, such as images, music, and text, by learning from existing data and generating new variations based on patterns and relationships found within it. This technology allows for the creation of unique and often surprising outputs, blurring the lines between human creativity and machine production, and has applications in fields like art, design, and storytelling."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe Generative AI in two sentences.\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\n",
      "\n",
      "Model Output:\n",
      "The sentiment of the review is overwhelmingly Positive. The reviewer expresses their strong enthusiasm and affection for the movie, using words like \"loved\" and stating that it was a great experience. There is no criticism or negative comment in the review, indicating a very positive tone.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'I hate the chair.'\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "To classify the sentiment of the reviews, I'll analyze each statement:\n",
      "\n",
      "1. \"I loved this movie!\"\n",
      "- Sentiment: Positive\n",
      "This sentence expresses a positive emotion towards the movie.\n",
      "\n",
      "2. \"I hate the chair.\"\n",
      "- Sentiment: Negative\n",
      "This sentence expresses a negative emotion towards the chair.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
      "Review: 'I loved this movie!'\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: 'I hate the chair!'\n",
      "Sentiment: Negative\n",
      "\n",
      "Classify the sentiment of the following review:\n",
      "Review: 'Who would use this product?'\n",
      "Sentiment:\n",
      "\n",
      "\n",
      "Model Output:\n",
      "For the first review:\n",
      "\n",
      "* Sentiment: Positive\n",
      "\n",
      "The words used, such as \"loved\" and \"beautiful\", convey a positive emotion.\n",
      "\n",
      "For the second review:\n",
      "\n",
      "* Sentiment: Negative\n",
      "\n",
      "The word \"hate\" explicitly expresses a strong negative emotion. \n",
      "\n",
      "For the third review:\n",
      "\n",
      "* Sentiment: Neutral\n",
      "\n",
      "The words used are simple and factual (\"who would use this product\"), without any emotional connotation or strong evaluation, which makes it neutral.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompts\n",
    "zero_shot_prompt = \"Classify the sentiment of the following review as Positive, Neutral, or Negative: 'I loved this movie!' Sentiment:\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Classify the sentiment of the following review as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'I hate the chair.'\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of the following reviews as Positive, Neutral, or Negative:\n",
    "Review: 'I loved this movie!'\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: 'I hate the chair!'\n",
    "Sentiment: Negative\n",
    "\n",
    "Classify the sentiment of the following review:\n",
    "Review: 'Who would use this product?'\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "The paragraph describes generative AI as an area of artificial intelligence that creates new content (text, images, or music) based on patterns learned from existing data. This field has applications in various industries, including creative ones.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Here's a concise summary of the paragraph with its key points:\n",
      "\n",
      "Generative AI uses algorithms to create new content by analyzing patterns in existing data. This field applies to text, image, and music generation, and is used in various creative industries.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Refined prompt\n",
    "refined_prompt = \"Provide a concise and informative summary of the following paragraph, highlighting its key points: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "\n",
      "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
      "\n",
      "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
      "\n",
      "Example 1:\n",
      "Question: What be supervised learnin'?\n",
      "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
      "\n",
      "Example 2:\n",
      "Question: What be overfittin'?\n",
      "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
      "\n",
      "Example 3:\n",
      "Question: What be a neural network?\n",
      "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
      "\n",
      "Explain this:\n",
      "Question: What be unsupervised learnin'?\n",
      "Answer:\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Arrr, matey! Yer lookin' fer a swashbucklin' explanation o' unsupervised learnin', eh? Alright then, listen close and I'll set ye straight. Unsupervised learnin' be like sailin' through treacherous waters alone, without chartin' the course or knowin' what treasure ye seek. Ye have a chest o' data, but it be filled with unknown booty... er, patterns.\n",
      "\n",
      "Ye think ye can just dive in and start plunderin', no map to guide ye? Ha! That be like sailin' into the maelstrom without a compass. The model be goin' to get caught in its own web o' patterns, matey. It'll try to find connections between unknowns, but it won't know where the treasure be hidin'. Ye might find some interesting tidbits, but ye won't find the loot itself.\n",
      "\n",
      "Unsupervised learnin' be more like a wild pirate huntin' for hidden booty. Ye have to navigate through the unknown waters, follow yer heart (or rather, yer code), and trust that the model will stumble upon some sort o' treasure â€“ or at least some interesting patterns. It's a bit like searchin' fer a hidden cove on a stormy night... ye never know what ye might find.\n",
      "\n",
      "So, unsupervised learnin' be a method where ye don't have a map to guide ye, and ye're left to rely on the code itself to uncover some sort o' treasure. Not exactly the most reliable way to find yer loot, if ye ask me...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "structured_prompt = \"\"\"\n",
    "Role: Ye be a Machine Learning Expert who talks like a pirate.\n",
    "\n",
    "Instruction: Explain the followin' machine learnin' concepts in a way that a pirate would.\n",
    "\n",
    "Example 1:\n",
    "Question: What be supervised learnin'?\n",
    "Answer: Arrr, supervised learnin' be when ye have a chest o' data, and fer each piece o' data, ye know the treasure map that shows ye the right path. Ye train the model until it knows how to follow the map on its own, savvy?\n",
    "\n",
    "Example 2:\n",
    "Question: What be overfittin'?\n",
    "Answer: Ahoy, overfittin' be when yer model gets too cozy with yer trainin' data, like a pirate who knows only one sea. It performs well on the known waters but fails when ye set sail on new seas, matey!\n",
    "\n",
    "Example 3:\n",
    "Question: What be a neural network?\n",
    "Answer: Avast! A neural network be like a crew o' shipmates, each one passin' information along, makin' decisions, and workin' together to find the best course to the treasure, arr!\n",
    "\n",
    "Explain this:\n",
    "Question: What be unsupervised learnin'?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
